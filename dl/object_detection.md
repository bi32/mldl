# YOLOç³»åˆ—ç›®æ ‡æ£€æµ‹å®Œå…¨æŒ‡å— ğŸ¯

YOLOï¼ˆYou Only Look Onceï¼‰å°†ç›®æ ‡æ£€æµ‹ä½œä¸ºå›å½’é—®é¢˜ï¼Œä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯å¾—åˆ°æ‰€æœ‰æ£€æµ‹ç»“æœã€‚ä»YOLOv1åˆ°YOLOv8ï¼Œè®©æˆ‘ä»¬æ¢ç´¢è¿™ä¸ªä¼ å¥‡ç³»åˆ—çš„æ¼”è¿›ã€‚

## 1. YOLOv5 - å·¥ç¨‹åŒ–å…¸èŒƒ ğŸ”§

### å®‰è£…å’Œä½¿ç”¨

```python
# å…‹éš†YOLOv5ä»“åº“
# git clone https://github.com/ultralytics/yolov5
# cd yolov5
# pip install -r requirements.txt

import torch
import cv2
import numpy as np
from pathlib import Path

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# æ¨ç†
def detect_objects(image_path):
    """ä½¿ç”¨YOLOv5æ£€æµ‹ç›®æ ‡"""
    # è¯»å–å›¾åƒ
    img = cv2.imread(image_path)
    
    # æ¨ç†
    results = model(img)
    
    # è§£æç»“æœ
    detections = results.pandas().xyxy[0]  # è·å–æ£€æµ‹æ¡†
    
    # å¯è§†åŒ–
    for _, row in detections.iterrows():
        x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), \
                         int(row['xmax']), int(row['ymax'])
        conf = row['confidence']
        cls = row['name']
        
        # ç”»æ¡†
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # æ ‡ç­¾
        label = f'{cls} {conf:.2f}'
        cv2.putText(img, label, (x1, y1-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    
    return img, detections

# è®­ç»ƒè‡ªå®šä¹‰æ•°æ®é›†
def train_custom_yolov5():
    """è®­ç»ƒYOLOv5"""
    import yaml
    
    # åˆ›å»ºæ•°æ®é…ç½®æ–‡ä»¶
    data_config = {
        'path': './datasets/custom',  # æ•°æ®é›†è·¯å¾„
        'train': 'images/train',
        'val': 'images/val',
        'nc': 10,  # ç±»åˆ«æ•°
        'names': ['class1', 'class2', '...']  # ç±»åˆ«åç§°
    }
    
    with open('custom_data.yaml', 'w') as f:
        yaml.dump(data_config, f)
    
    # è®­ç»ƒå‘½ä»¤
    train_cmd = """
    python train.py \\
        --img 640 \\
        --batch 16 \\
        --epochs 100 \\
        --data custom_data.yaml \\
        --weights yolov5s.pt \\
        --cache
    """
    
    print(f"è®­ç»ƒå‘½ä»¤:\n{train_cmd}")
```

## 2. YOLOv8 - æœ€æ–°SOTA ğŸš€

```python
# pip install ultralytics

from ultralytics import YOLO
import cv2
import numpy as np

# YOLOv8ä½¿ç”¨ç¤ºä¾‹
class YOLOv8Detector:
    def __init__(self, model_path='yolov8n.pt'):
        """åˆå§‹åŒ–YOLOv8æ£€æµ‹å™¨"""
        self.model = YOLO(model_path)
        
    def detect(self, image, conf_threshold=0.25):
        """æ£€æµ‹ç›®æ ‡"""
        results = self.model(image, conf=conf_threshold)
        return results
    
    def track(self, video_path):
        """ç›®æ ‡è·Ÿè¸ª"""
        cap = cv2.VideoCapture(video_path)
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # è·Ÿè¸ª
            results = self.model.track(frame, persist=True)
            
            # å¯è§†åŒ–
            annotated_frame = results[0].plot()
            cv2.imshow('YOLOv8 Tracking', annotated_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
    
    def train(self, data_yaml, epochs=100):
        """è®­ç»ƒæ¨¡å‹"""
        results = self.model.train(
            data=data_yaml,
            epochs=epochs,
            imgsz=640,
            batch=16,
            device='cuda'
        )
        return results
    
    def export(self, format='onnx'):
        """å¯¼å‡ºæ¨¡å‹"""
        self.model.export(format=format)

# ä½¿ç”¨ç¤ºä¾‹
detector = YOLOv8Detector('yolov8n.pt')

# æ£€æµ‹å›¾åƒ
image = cv2.imread('test.jpg')
results = detector.detect(image)

# è·å–æ£€æµ‹æ¡†
for r in results:
    boxes = r.boxes  # æ£€æµ‹æ¡†
    masks = r.masks  # åˆ†å‰²æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
    probs = r.probs  # åˆ†ç±»æ¦‚ç‡
    
    # è§£ææ£€æµ‹æ¡†
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0].tolist()
        conf = box.conf[0].item()
        cls = int(box.cls[0].item())
        
        print(f"æ£€æµ‹åˆ°: ç±»åˆ«{cls}, ç½®ä¿¡åº¦{conf:.2f}, "
              f"ä½ç½®({x1:.0f},{y1:.0f},{x2:.0f},{y2:.0f})")

# å®ä¾‹åˆ†å‰²ï¼ˆYOLOv8-Segï¼‰
seg_model = YOLO('yolov8n-seg.pt')
results = seg_model(image)

# å§¿æ€ä¼°è®¡ï¼ˆYOLOv8-Poseï¼‰
pose_model = YOLO('yolov8n-pose.pt')
results = pose_model(image)
```

## 3. è‡ªå®šä¹‰YOLOå®ç°ï¼ˆæ•™å­¦ç‰ˆï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class YOLOv3(nn.Module):
    """ç®€åŒ–ç‰ˆYOLOv3å®ç°"""
    def __init__(self, num_classes=80):
        super(YOLOv3, self).__init__()
        self.num_classes = num_classes
        
        # Darknet-53éª¨å¹²ç½‘ç»œï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.backbone = self._make_darknet53()
        
        # FPN neck
        self.neck = self._make_fpn()
        
        # Detection heads
        self.heads = nn.ModuleList([
            self._make_detection_head(512, num_classes),  # å¤§ç›®æ ‡
            self._make_detection_head(256, num_classes),  # ä¸­ç›®æ ‡
            self._make_detection_head(128, num_classes),  # å°ç›®æ ‡
        ])
        
    def _make_darknet53(self):
        """æ„å»ºDarknet-53éª¨å¹²"""
        return nn.Sequential(
            # çœç•¥å…·ä½“å®ç°ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ
            ConvBlock(3, 32, 3, 1),
            ConvBlock(32, 64, 3, 2),
            ResidualBlock(64, 32, 64, num_blocks=1),
            ConvBlock(64, 128, 3, 2),
            ResidualBlock(128, 64, 128, num_blocks=2),
            ConvBlock(128, 256, 3, 2),
            ResidualBlock(256, 128, 256, num_blocks=8),
            ConvBlock(256, 512, 3, 2),
            ResidualBlock(512, 256, 512, num_blocks=8),
            ConvBlock(512, 1024, 3, 2),
            ResidualBlock(1024, 512, 1024, num_blocks=4),
        )
    
    def _make_fpn(self):
        """æ„å»ºFPN"""
        return nn.ModuleList([
            nn.Conv2d(1024, 512, 1),
            nn.Conv2d(512, 256, 1),
            nn.Conv2d(256, 128, 1),
        ])
    
    def _make_detection_head(self, in_channels, num_classes):
        """æ„å»ºæ£€æµ‹å¤´"""
        return nn.Sequential(
            ConvBlock(in_channels, in_channels * 2, 3, 1),
            nn.Conv2d(in_channels * 2, 3 * (5 + num_classes), 1)
            # 3ä¸ªanchor Ã— (4ä¸ªåæ ‡ + 1ä¸ªç½®ä¿¡åº¦ + num_classesä¸ªç±»åˆ«)
        )
    
    def forward(self, x):
        # éª¨å¹²ç½‘ç»œ
        features = self.backbone(x)
        
        # FPNå’Œæ£€æµ‹
        outputs = []
        for i, head in enumerate(self.heads):
            feat = self.neck[i](features[-(i+1)])
            output = head(feat)
            outputs.append(output)
        
        return outputs

class ConvBlock(nn.Module):
    """å·ç§¯å—ï¼šConv + BN + LeakyReLU"""
    def __init__(self, in_channels, out_channels, kernel_size, stride):
        super(ConvBlock, self).__init__()
        padding = kernel_size // 2
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, 
                              stride, padding, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.activation = nn.LeakyReLU(0.1)
    
    def forward(self, x):
        return self.activation(self.bn(self.conv(x)))

class ResidualBlock(nn.Module):
    """æ®‹å·®å—"""
    def __init__(self, in_channels, hidden_channels, out_channels, num_blocks):
        super(ResidualBlock, self).__init__()
        self.blocks = nn.Sequential(
            *[self._make_layer(in_channels if i == 0 else out_channels,
                              hidden_channels, out_channels)
              for i in range(num_blocks)]
        )
    
    def _make_layer(self, in_channels, hidden_channels, out_channels):
        return nn.Sequential(
            ConvBlock(in_channels, hidden_channels, 1, 1),
            ConvBlock(hidden_channels, out_channels, 3, 1)
        )
    
    def forward(self, x):
        return x + self.blocks(x)

# YOLOæŸå¤±å‡½æ•°
class YOLOLoss(nn.Module):
    def __init__(self, num_classes=80, anchors=None):
        super(YOLOLoss, self).__init__()
        self.num_classes = num_classes
        self.anchors = anchors or [(10,13), (16,30), (33,23)]
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCEWithLogitsLoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
    def forward(self, predictions, targets):
        """
        è®¡ç®—YOLOæŸå¤±
        predictions: æ¨¡å‹è¾“å‡º
        targets: çœŸå®æ ‡ç­¾ [batch, max_objects, 5] (x,y,w,h,class)
        """
        device = predictions.device
        batch_size = predictions.shape[0]
        
        # è§£æé¢„æµ‹
        # predictions shape: [batch, 3*(5+num_classes), H, W]
        predictions = predictions.view(batch_size, 3, 5 + self.num_classes,
                                      predictions.shape[2], predictions.shape[3])
        predictions = predictions.permute(0, 1, 3, 4, 2)  # [batch, 3, H, W, 5+classes]
        
        # æå–é¢„æµ‹å€¼
        pred_xy = torch.sigmoid(predictions[..., :2])    # ä¸­å¿ƒåæ ‡
        pred_wh = torch.exp(predictions[..., 2:4])       # å®½é«˜
        pred_conf = torch.sigmoid(predictions[..., 4:5]) # ç½®ä¿¡åº¦
        pred_cls = predictions[..., 5:]                  # ç±»åˆ«
        
        # è®¡ç®—æŸå¤±
        loss_xy = self.mse_loss(pred_xy, targets[..., :2])
        loss_wh = self.mse_loss(pred_wh, targets[..., 2:4])
        loss_conf = self.bce_loss(pred_conf, targets[..., 4:5])
        loss_cls = self.ce_loss(pred_cls.reshape(-1, self.num_classes),
                               targets[..., 5].long().reshape(-1))
        
        # æ€»æŸå¤±
        total_loss = loss_xy + loss_wh + loss_conf + loss_cls
        
        return total_loss, {
            'xy': loss_xy.item(),
            'wh': loss_wh.item(),
            'conf': loss_conf.item(),
            'cls': loss_cls.item()
        }
```

## 4. æ•°æ®å‡†å¤‡å’Œå¢å¼º

```python
import albumentations as A
from albumentations.pytorch import ToTensorV2

class YOLODataset(torch.utils.data.Dataset):
    """YOLOæ•°æ®é›†"""
    def __init__(self, images_dir, labels_dir, img_size=640, augment=True):
        self.images_dir = Path(images_dir)
        self.labels_dir = Path(labels_dir)
        self.img_size = img_size
        self.augment = augment
        
        self.images = list(self.images_dir.glob('*.jpg'))
        
        # æ•°æ®å¢å¼º
        self.transform = A.Compose([
            A.RandomResizedCrop(img_size, img_size, scale=(0.5, 1.0)),
            A.HorizontalFlip(p=0.5),
            A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
            A.GaussNoise(p=0.2),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        # è¯»å–å›¾åƒ
        img_path = self.images[idx]
        image = cv2.imread(str(img_path))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # è¯»å–æ ‡ç­¾
        label_path = self.labels_dir / f"{img_path.stem}.txt"
        bboxes = []
        class_labels = []
        
        if label_path.exists():
            with open(label_path, 'r') as f:
                for line in f:
                    cls, x, y, w, h = map(float, line.strip().split())
                    bboxes.append([x, y, w, h])
                    class_labels.append(int(cls))
        
        # æ•°æ®å¢å¼º
        if self.augment and len(bboxes) > 0:
            transformed = self.transform(
                image=image,
                bboxes=bboxes,
                class_labels=class_labels
            )
            image = transformed['image']
            bboxes = transformed['bboxes']
            class_labels = transformed['class_labels']
        else:
            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        
        # è½¬æ¢æ ‡ç­¾æ ¼å¼
        targets = torch.zeros((len(bboxes), 6))
        for i, (box, cls) in enumerate(zip(bboxes, class_labels)):
            targets[i] = torch.tensor([0, cls, *box])  # [batch_idx, class, x, y, w, h]
        
        return image, targets

# Mosaicæ•°æ®å¢å¼º
def mosaic_augmentation(images, labels, img_size=640):
    """Mosaicæ•°æ®å¢å¼ºï¼šå°†4å¼ å›¾ç‰‡æ‹¼æ¥æˆä¸€å¼ """
    mosaic_img = np.zeros((img_size, img_size, 3), dtype=np.float32)
    mosaic_labels = []
    
    # éšæœºé€‰æ‹©åˆ†å‰²ç‚¹
    xc = np.random.randint(img_size * 0.25, img_size * 0.75)
    yc = np.random.randint(img_size * 0.25, img_size * 0.75)
    
    for i, (img, label) in enumerate(zip(images, labels)):
        h, w = img.shape[:2]
        
        # ç¡®å®šæ”¾ç½®ä½ç½®
        if i == 0:  # å·¦ä¸Š
            x1, y1, x2, y2 = 0, 0, xc, yc
            sx1, sy1, sx2, sy2 = w - xc, h - yc, w, h
        elif i == 1:  # å³ä¸Š
            x1, y1, x2, y2 = xc, 0, img_size, yc
            sx1, sy1, sx2, sy2 = 0, h - yc, w - xc, h
        elif i == 2:  # å·¦ä¸‹
            x1, y1, x2, y2 = 0, yc, xc, img_size
            sx1, sy1, sx2, sy2 = w - xc, 0, w, h - yc
        else:  # å³ä¸‹
            x1, y1, x2, y2 = xc, yc, img_size, img_size
            sx1, sy1, sx2, sy2 = 0, 0, w - xc, h - yc
        
        # æ”¾ç½®å›¾ç‰‡ç‰‡æ®µ
        mosaic_img[y1:y2, x1:x2] = img[sy1:sy2, sx1:sx2]
        
        # è°ƒæ•´æ ‡ç­¾
        for box in label:
            cls, x, y, w, h = box
            # è½¬æ¢åæ ‡å¹¶è£å‰ª
            # ... åæ ‡è½¬æ¢é€»è¾‘
            mosaic_labels.append([cls, x, y, w, h])
    
    return mosaic_img, mosaic_labels
```

## 5. æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

```python
def calculate_map(predictions, ground_truths, iou_threshold=0.5):
    """è®¡ç®—mAPï¼ˆmean Average Precisionï¼‰"""
    from collections import defaultdict
    
    # æŒ‰ç±»åˆ«ç»„ç»‡é¢„æµ‹å’ŒçœŸå®æ¡†
    detections = defaultdict(list)
    gt_boxes = defaultdict(list)
    
    for pred in predictions:
        detections[pred['class']].append({
            'bbox': pred['bbox'],
            'score': pred['score']
        })
    
    for gt in ground_truths:
        gt_boxes[gt['class']].append(gt['bbox'])
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„AP
    aps = []
    for cls in gt_boxes.keys():
        # æ’åºé¢„æµ‹æ¡†
        dets = sorted(detections[cls], key=lambda x: x['score'], reverse=True)
        gts = gt_boxes[cls]
        
        # è®¡ç®—precisionå’Œrecall
        tp = np.zeros(len(dets))
        fp = np.zeros(len(dets))
        
        for i, det in enumerate(dets):
            max_iou = 0
            max_idx = -1
            
            for j, gt in enumerate(gts):
                iou = calculate_iou(det['bbox'], gt)
                if iou > max_iou:
                    max_iou = iou
                    max_idx = j
            
            if max_iou >= iou_threshold:
                tp[i] = 1
                gts.pop(max_idx)  # ç§»é™¤å·²åŒ¹é…çš„çœŸå®æ¡†
            else:
                fp[i] = 1
        
        # è®¡ç®—AP
        tp_cumsum = np.cumsum(tp)
        fp_cumsum = np.cumsum(fp)
        
        recall = tp_cumsum / len(gt_boxes[cls])
        precision = tp_cumsum / (tp_cumsum + fp_cumsum)
        
        # è®¡ç®—AP (11ç‚¹æ’å€¼)
        ap = 0
        for r in np.linspace(0, 1, 11):
            if np.any(recall >= r):
                ap += np.max(precision[recall >= r])
        ap /= 11
        
        aps.append(ap)
    
    return np.mean(aps)

def calculate_iou(box1, box2):
    """è®¡ç®—IoU"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0

# å¯è§†åŒ–æ£€æµ‹ç»“æœ
def visualize_detections(image, detections, class_names):
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    
    fig, ax = plt.subplots(1, figsize=(12, 8))
    ax.imshow(image)
    
    colors = plt.cm.hsv(np.linspace(0, 1, len(class_names)))
    
    for det in detections:
        x1, y1, x2, y2 = det['bbox']
        cls_id = det['class']
        score = det['score']
        
        # ç”»æ¡†
        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,
                                linewidth=2, edgecolor=colors[cls_id],
                                facecolor='none')
        ax.add_patch(rect)
        
        # æ ‡ç­¾
        label = f'{class_names[cls_id]}: {score:.2f}'
        ax.text(x1, y1-5, label, color='white',
               bbox=dict(facecolor=colors[cls_id], alpha=0.5))
    
    ax.axis('off')
    plt.title('YOLO Detection Results')
    plt.tight_layout()
    plt.show()
```

## 6. éƒ¨ç½²ä¼˜åŒ–

```python
# ONNXå¯¼å‡º
def export_to_onnx(model, img_size=640):
    """å¯¼å‡ºYOLOæ¨¡å‹åˆ°ONNX"""
    model.eval()
    dummy_input = torch.randn(1, 3, img_size, img_size)
    
    torch.onnx.export(
        model,
        dummy_input,
        "yolo.onnx",
        opset_version=11,
        input_names=['images'],
        output_names=['output'],
        dynamic_axes={
            'images': {0: 'batch'},
            'output': {0: 'batch'}
        }
    )
    print("æ¨¡å‹å·²å¯¼å‡ºåˆ°yolo.onnx")

# TensorRTåŠ é€Ÿ
def tensorrt_inference():
    """ä½¿ç”¨TensorRTåŠ é€Ÿæ¨ç†"""
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    
    # æ„å»ºTensorRTå¼•æ“
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network()
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # è§£æONNX
    with open("yolo.onnx", 'rb') as f:
        parser.parse(f.read())
    
    # æ„å»ºå¼•æ“
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB
    engine = builder.build_engine(network, config)
    
    return engine

# é‡åŒ–
def quantize_model(model):
    """INT8é‡åŒ–"""
    import torch.quantization as quant
    
    # å‡†å¤‡é‡åŒ–
    model.qconfig = quant.get_default_qconfig('fbgemm')
    quant.prepare(model, inplace=True)
    
    # æ ¡å‡†ï¼ˆéœ€è¦ä»£è¡¨æ€§æ•°æ®ï¼‰
    # ... è¿è¡Œä¸€äº›æ¨ç†
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    quant.convert(model, inplace=True)
    
    return model
```

## æœ€ä½³å®è·µå»ºè®®

### 1. æ¨¡å‹é€‰æ‹©
- **é€Ÿåº¦ä¼˜å…ˆ**: YOLOv5n/YOLOv8n
- **ç²¾åº¦ä¼˜å…ˆ**: YOLOv5x/YOLOv8x
- **å¹³è¡¡é€‰æ‹©**: YOLOv5s/YOLOv8s

### 2. è®­ç»ƒæŠ€å·§
- ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
- Mosaicå’ŒMixUpæ•°æ®å¢å¼º
- å¤šå°ºåº¦è®­ç»ƒ
- æ ‡ç­¾å¹³æ»‘

### 3. éƒ¨ç½²ä¼˜åŒ–
- ONNX/TensorRTéƒ¨ç½²
- INT8é‡åŒ–
- æ‰¹å¤„ç†æ¨ç†
- NMSä¼˜åŒ–

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [PyTorchéƒ¨ç½²](deployment.md) - æ¨¡å‹éƒ¨ç½²è¯¦è§£
- [NLPæ¨¡å‹](nlp_models.md) - BERTã€GPTç­‰
- [LLMéƒ¨ç½²](llm_deployment.md) - å¤§æ¨¡å‹éƒ¨ç½²