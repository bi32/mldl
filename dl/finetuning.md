# æ¨¡å‹å¾®è°ƒå®Œå…¨æŒ‡å—ï¼šä»PEFTåˆ°QLoRA ğŸ¯

æŒæ¡ç°ä»£æ¨¡å‹å¾®è°ƒæŠ€æœ¯ï¼Œç”¨æœ€å°‘çš„èµ„æºå®ç°æœ€å¥½çš„æ•ˆæœã€‚

## 1. å¾®è°ƒåŸºç¡€æ¦‚å¿µ ğŸ“š

```python
import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset

class BasicFineTuning:
    """åŸºç¡€å¾®è°ƒç¤ºä¾‹"""
    
    def __init__(self, model_name="bert-base-uncased"):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
    def prepare_dataset(self, texts, labels=None):
        """å‡†å¤‡æ•°æ®é›†"""
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                padding="max_length",
                truncation=True,
                max_length=512
            )
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = load_dataset("text", data_files={"train": texts})
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        return tokenized_dataset
    
    def full_finetune(self, train_dataset, eval_dataset=None):
        """å…¨é‡å¾®è°ƒ"""
        training_args = TrainingArguments(
            output_dir="./results",
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir="./logs",
            logging_steps=10,
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=500 if eval_dataset else None,
            save_strategy="epoch",
            load_best_model_at_end=True if eval_dataset else False,
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        return trainer
    
    def selective_finetune(self, layers_to_freeze=None):
        """é€‰æ‹©æ€§å¾®è°ƒï¼šå†»ç»“éƒ¨åˆ†å±‚"""
        if layers_to_freeze is None:
            # å†»ç»“å‰6å±‚
            layers_to_freeze = list(range(6))
        
        # å†»ç»“æŒ‡å®šå±‚
        for i, layer in enumerate(self.model.transformer.h):
            if i in layers_to_freeze:
                for param in layer.parameters():
                    param.requires_grad = False
        
        # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
        
        return self.model

# å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
def get_lr_scheduler(optimizer, num_training_steps, warmup_steps=0):
    """è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
    
    # çº¿æ€§è¡°å‡
    linear_scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=num_training_steps
    )
    
    # ä½™å¼¦è¡°å‡
    cosine_scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=num_training_steps
    )
    
    return cosine_scheduler
```

## 2. LoRAï¼šä½ç§©é€‚åº” ğŸ¨

```python
from peft import LoraConfig, get_peft_model, TaskType, PeftModel

class LoRAFineTuning:
    """LoRAå¾®è°ƒå®ç°"""
    
    def __init__(self, model_name="meta-llama/Llama-2-7b-hf"):
        self.base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
    def apply_lora(self, r=8, lora_alpha=32, lora_dropout=0.1, 
                   target_modules=None):
        """åº”ç”¨LoRAé…ç½®"""
        if target_modules is None:
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        
        # LoRAé…ç½®
        peft_config = LoraConfig(
            r=r,  # ä½ç§©çŸ©é˜µçš„ç§©
            lora_alpha=lora_alpha,  # LoRAç¼©æ”¾å‚æ•°
            target_modules=target_modules,  # è¦åº”ç”¨LoRAçš„æ¨¡å—
            lora_dropout=lora_dropout,  # LoRA dropout
            bias="none",  # åç½®å¤„ç†æ–¹å¼
            task_type=TaskType.CAUSAL_LM,  # ä»»åŠ¡ç±»å‹
        )
        
        # è·å–PEFTæ¨¡å‹
        self.model = get_peft_model(self.base_model, peft_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        self.model.print_trainable_parameters()
        
        return self.model
    
    def train(self, dataset, output_dir="./lora_model"):
        """è®­ç»ƒLoRAæ¨¡å‹"""
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            warmup_steps=100,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=10,
            save_strategy="epoch",
            evaluation_strategy="no",
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            tokenizer=self.tokenizer,
        )
        
        trainer.train()
        
        # ä¿å­˜LoRAæƒé‡
        self.model.save_pretrained(output_dir)
        
    def merge_and_save(self, output_dir="./merged_model"):
        """åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹"""
        # åˆå¹¶æƒé‡
        merged_model = self.model.merge_and_unload()
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        merged_model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        return merged_model
    
    def load_lora_model(self, lora_path, base_model_name):
        """åŠ è½½LoRAæ¨¡å‹"""
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(base_model_name)
        
        # åŠ è½½LoRAæƒé‡
        model = PeftModel.from_pretrained(base_model, lora_path)
        
        return model

# å¤šLoRAåˆå¹¶
class MultiLoRAMerger:
    """å¤šä¸ªLoRAæ¨¡å‹åˆå¹¶"""
    
    def __init__(self, base_model):
        self.base_model = base_model
        
    def weighted_merge(self, lora_models, weights):
        """åŠ æƒåˆå¹¶å¤šä¸ªLoRA"""
        assert len(lora_models) == len(weights)
        assert abs(sum(weights) - 1.0) < 1e-6
        
        # åˆå§‹åŒ–åˆå¹¶å‚æ•°
        merged_params = {}
        
        for lora_model, weight in zip(lora_models, weights):
            for name, param in lora_model.named_parameters():
                if "lora" in name:
                    if name not in merged_params:
                        merged_params[name] = weight * param.data
                    else:
                        merged_params[name] += weight * param.data
        
        # åº”ç”¨åˆå¹¶çš„å‚æ•°
        for name, param in merged_params.items():
            self.base_model.state_dict()[name].copy_(param)
        
        return self.base_model
    
    def task_arithmetic_merge(self, lora_models, lambda_param=0.5):
        """ä»»åŠ¡ç®—æœ¯åˆå¹¶ï¼šç”¨äºå¤šä»»åŠ¡"""
        # è·å–ä»»åŠ¡å‘é‡
        task_vectors = []
        
        for lora_model in lora_models:
            task_vector = {}
            for name, param in lora_model.named_parameters():
                if "lora" in name:
                    # è®¡ç®—ä¸åŸºç¡€æ¨¡å‹çš„å·®å¼‚
                    base_param = self.base_model.state_dict()[name.replace("lora_", "")]
                    task_vector[name] = param.data - base_param
            task_vectors.append(task_vector)
        
        # åˆå¹¶ä»»åŠ¡å‘é‡
        merged_vector = {}
        for name in task_vectors[0].keys():
            merged_vector[name] = sum(tv[name] for tv in task_vectors) / len(task_vectors)
        
        # åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹
        for name, delta in merged_vector.items():
            base_name = name.replace("lora_", "")
            self.base_model.state_dict()[base_name] += lambda_param * delta
        
        return self.base_model
```

## 3. QLoRAï¼šé‡åŒ–LoRA ğŸ“¦

```python
from transformers import BitsAndBytesConfig
import bitsandbytes as bnb

class QLoRAFineTuning:
    """QLoRAï¼š4bité‡åŒ– + LoRA"""
    
    def __init__(self, model_name="meta-llama/Llama-2-7b-hf"):
        self.model_name = model_name
        
    def load_quantized_model(self):
        """åŠ è½½4bité‡åŒ–æ¨¡å‹"""
        # é‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–
            bnb_4bit_quant_type="nf4",  # NormalFloat4é‡åŒ–
            bnb_4bit_compute_dtype=torch.bfloat16  # è®¡ç®—æ•°æ®ç±»å‹
        )
        
        # åŠ è½½é‡åŒ–æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # å‡†å¤‡æ¨¡å‹ç”¨äºk-bitè®­ç»ƒ
        model = self.prepare_model_for_kbit_training(model)
        
        return model
    
    def prepare_model_for_kbit_training(self, model):
        """å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ"""
        model.gradient_checkpointing_enable()
        
        # å°†æŸäº›å±‚è½¬ä¸ºfp32ä»¥æé«˜ç¨³å®šæ€§
        for param in model.parameters():
            param.requires_grad = False  # å†»ç»“æ‰€æœ‰å‚æ•°
            
        # å¯ç”¨è¾“å…¥æ¢¯åº¦
        if hasattr(model, 'enable_input_require_grads'):
            model.enable_input_require_grads()
        else:
            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)
            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
        
        return model
    
    def apply_qlora(self, model, r=4, lora_alpha=16):
        """åº”ç”¨QLoRAé…ç½®"""
        # æ‰¾åˆ°æ‰€æœ‰Linearå±‚
        target_modules = self.find_all_linear_names(model)
        
        # LoRAé…ç½®
        peft_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=0.05,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, peft_config)
        
        return model
    
    def find_all_linear_names(self, model):
        """æ‰¾åˆ°æ‰€æœ‰Linearå±‚çš„åç§°"""
        cls = bnb.nn.Linear4bit
        lora_module_names = set()
        
        for name, module in model.named_modules():
            if isinstance(module, cls):
                names = name.split('.')
                lora_module_names.add(names[-1])
        
        # ç§»é™¤ä¸€äº›ä¸åº”è¯¥é€‚é…çš„å±‚
        if 'lm_head' in lora_module_names:
            lora_module_names.remove('lm_head')
        
        return list(lora_module_names)
    
    def train_qlora(self, model, dataset, output_dir="./qlora_model"):
        """è®­ç»ƒQLoRAæ¨¡å‹"""
        from transformers import TrainingArguments
        from trl import SFTTrainer
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=2,
            gradient_accumulation_steps=8,
            gradient_checkpointing=True,
            optim="paged_adamw_32bit",  # åˆ†é¡µä¼˜åŒ–å™¨
            learning_rate=2e-4,
            warmup_ratio=0.03,
            logging_steps=10,
            save_strategy="epoch",
            evaluation_strategy="no",
            fp16=False,
            bf16=True,
            max_grad_norm=0.3,
            warmup_steps=5,
            group_by_length=True,
            lr_scheduler_type="constant",
        )
        
        trainer = SFTTrainer(
            model=model,
            train_dataset=dataset,
            tokenizer=self.tokenizer,
            args=training_args,
            max_seq_length=512,
            packing=False,
        )
        
        trainer.train()
        
        return trainer

# å†…å­˜ä¼˜åŒ–æŠ€æœ¯
class MemoryEfficientTraining:
    """å†…å­˜é«˜æ•ˆè®­ç»ƒæŠ€æœ¯"""
    
    @staticmethod
    def gradient_checkpointing(model):
        """æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šç”¨è®¡ç®—æ¢å†…å­˜"""
        model.gradient_checkpointing_enable()
        return model
    
    @staticmethod
    def mixed_precision_training():
        """æ··åˆç²¾åº¦è®­ç»ƒ"""
        from torch.cuda.amp import autocast, GradScaler
        
        scaler = GradScaler()
        
        def train_step(model, inputs, optimizer):
            with autocast():
                outputs = model(**inputs)
                loss = outputs.loss
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            return loss
        
        return train_step
    
    @staticmethod
    def gradient_accumulation(batch_size=32, micro_batch_size=4):
        """æ¢¯åº¦ç´¯ç§¯"""
        accumulation_steps = batch_size // micro_batch_size
        
        def train_with_accumulation(model, dataloader, optimizer):
            model.train()
            optimizer.zero_grad()
            
            for i, batch in enumerate(dataloader):
                outputs = model(**batch)
                loss = outputs.loss / accumulation_steps
                loss.backward()
                
                if (i + 1) % accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
        
        return train_with_accumulation
```

## 4. Adapterå¾®è°ƒ ğŸ”Œ

```python
class AdapterFineTuning:
    """Adapterå¾®è°ƒå®ç°"""
    
    def __init__(self, model, adapter_size=64):
        self.model = model
        self.adapter_size = adapter_size
        self.adapters = {}
        
    def add_adapter_layer(self, name, input_size, adapter_size=None):
        """æ·»åŠ Adapterå±‚"""
        if adapter_size is None:
            adapter_size = self.adapter_size
        
        adapter = nn.Sequential(
            nn.Linear(input_size, adapter_size),
            nn.ReLU(),
            nn.Linear(adapter_size, input_size)
        )
        
        # åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„
        nn.init.zeros_(adapter[0].weight)
        nn.init.zeros_(adapter[0].bias)
        nn.init.zeros_(adapter[2].weight)
        nn.init.zeros_(adapter[2].bias)
        
        self.adapters[name] = adapter
        return adapter
    
    def forward_with_adapter(self, x, layer, adapter):
        """å¸¦Adapterçš„å‰å‘ä¼ æ’­"""
        # åŸå§‹å±‚è¾“å‡º
        output = layer(x)
        
        # Adapterè·¯å¾„
        adapter_output = adapter(x)
        
        # æ®‹å·®è¿æ¥
        return output + adapter_output
    
    def freeze_base_model(self):
        """å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°"""
        for param in self.model.parameters():
            param.requires_grad = False
        
        # åªè®­ç»ƒadapter
        for adapter in self.adapters.values():
            for param in adapter.parameters():
                param.requires_grad = True

# Prefix Tuning
class PrefixTuning:
    """å‰ç¼€å¾®è°ƒ"""
    
    def __init__(self, model, prefix_length=10, hidden_size=768):
        self.model = model
        self.prefix_length = prefix_length
        self.hidden_size = hidden_size
        
        # å‰ç¼€åµŒå…¥
        self.prefix_embeddings = nn.Embedding(prefix_length, hidden_size)
        
        # å‰ç¼€æŠ•å½±
        self.prefix_projection = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.Tanh(),
            nn.Linear(hidden_size * 2, hidden_size * model.config.num_hidden_layers * 2)
        )
        
    def get_prefix(self, batch_size):
        """è·å–å‰ç¼€"""
        prefix_tokens = torch.arange(self.prefix_length).unsqueeze(0).expand(batch_size, -1)
        prefix_emb = self.prefix_embeddings(prefix_tokens)
        
        # æŠ•å½±åˆ°æ‰€æœ‰å±‚
        past_key_values = self.prefix_projection(prefix_emb)
        
        # é‡å¡‘ä¸ºæ­£ç¡®çš„å½¢çŠ¶
        past_key_values = past_key_values.view(
            batch_size, 
            self.prefix_length, 
            self.model.config.num_hidden_layers * 2,
            self.model.config.num_attention_heads,
            self.hidden_size // self.model.config.num_attention_heads
        )
        
        # åˆ†å‰²ä¸ºkeyå’Œvalue
        past_key_values = torch.split(past_key_values, 2, dim=2)
        
        return past_key_values
    
    def forward(self, input_ids, **kwargs):
        """å‰å‘ä¼ æ’­"""
        batch_size = input_ids.shape[0]
        
        # è·å–å‰ç¼€
        past_key_values = self.get_prefix(batch_size)
        
        # è°ƒç”¨æ¨¡å‹
        outputs = self.model(
            input_ids=input_ids,
            past_key_values=past_key_values,
            **kwargs
        )
        
        return outputs

# Prompt Tuning
class PromptTuning:
    """æç¤ºå¾®è°ƒ"""
    
    def __init__(self, model, num_virtual_tokens=20):
        self.model = model
        self.num_virtual_tokens = num_virtual_tokens
        
        # è½¯æç¤ºåµŒå…¥
        self.soft_prompt = nn.Parameter(
            torch.randn(num_virtual_tokens, model.config.hidden_size)
        )
        
    def forward(self, input_ids, attention_mask=None):
        """æ·»åŠ è½¯æç¤ºçš„å‰å‘ä¼ æ’­"""
        batch_size = input_ids.shape[0]
        
        # è·å–è¾“å…¥åµŒå…¥
        inputs_embeds = self.model.get_input_embeddings()(input_ids)
        
        # æ‰©å±•è½¯æç¤ºåˆ°æ‰¹æ¬¡å¤§å°
        soft_prompt_expanded = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)
        
        # è¿æ¥è½¯æç¤ºå’Œè¾“å…¥åµŒå…¥
        inputs_embeds = torch.cat([soft_prompt_expanded, inputs_embeds], dim=1)
        
        # æ›´æ–°attention mask
        if attention_mask is not None:
            prompt_mask = torch.ones(batch_size, self.num_virtual_tokens).to(attention_mask.device)
            attention_mask = torch.cat([prompt_mask, attention_mask], dim=1)
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask
        )
        
        return outputs
```

## 5. æŒ‡ä»¤å¾®è°ƒ ğŸ“–

```python
from datasets import Dataset
import json

class InstructionTuning:
    """æŒ‡ä»¤å¾®è°ƒ"""
    
    def __init__(self, model_name="meta-llama/Llama-2-7b-hf"):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # è®¾ç½®pad token
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
    def format_instruction(self, instruction, input_text="", output=""):
        """æ ¼å¼åŒ–æŒ‡ä»¤"""
        if input_text:
            prompt = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}"""
        else:
            prompt = f"""### Instruction:
{instruction}

### Response:
{output}"""
        
        return prompt
    
    def prepare_dataset(self, data_path):
        """å‡†å¤‡æŒ‡ä»¤æ•°æ®é›†"""
        with open(data_path, 'r') as f:
            data = json.load(f)
        
        formatted_data = []
        for item in data:
            formatted_text = self.format_instruction(
                item.get("instruction", ""),
                item.get("input", ""),
                item.get("output", "")
            )
            formatted_data.append({"text": formatted_text})
        
        dataset = Dataset.from_list(formatted_data)
        
        # åˆ†è¯
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=512
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        return tokenized_dataset
    
    def train_with_sft(self, dataset, output_dir="./sft_model"):
        """ä½¿ç”¨SFTè®­ç»ƒå™¨è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ"""
        from trl import SFTTrainer
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=2e-5,
            warmup_steps=100,
            logging_steps=10,
            save_strategy="epoch",
            fp16=True,
        )
        
        trainer = SFTTrainer(
            model=self.model,
            train_dataset=dataset,
            tokenizer=self.tokenizer,
            args=training_args,
            max_seq_length=512,
            dataset_text_field="text",
        )
        
        trainer.train()
        
        return trainer

# å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒ
class MultiTaskInstructionTuning:
    """å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒ"""
    
    def __init__(self, model):
        self.model = model
        self.task_prompts = {
            "translation": "Translate the following text from {src_lang} to {tgt_lang}:",
            "summarization": "Summarize the following text:",
            "qa": "Answer the following question based on the context:",
            "classification": "Classify the following text into one of these categories: {categories}",
        }
    
    def create_multitask_dataset(self, datasets_dict):
        """åˆ›å»ºå¤šä»»åŠ¡æ•°æ®é›†"""
        all_examples = []
        
        for task_name, dataset in datasets_dict.items():
            task_prompt = self.task_prompts.get(task_name, "")
            
            for example in dataset:
                formatted_example = {
                    "task": task_name,
                    "instruction": task_prompt.format(**example.get("metadata", {})),
                    "input": example.get("input", ""),
                    "output": example.get("output", "")
                }
                all_examples.append(formatted_example)
        
        # æ‰“ä¹±æ•°æ®
        import random
        random.shuffle(all_examples)
        
        return Dataset.from_list(all_examples)
    
    def balanced_sampling(self, datasets_dict, samples_per_task=1000):
        """å¹³è¡¡é‡‡æ ·å¤šä»»åŠ¡æ•°æ®"""
        balanced_data = []
        
        for task_name, dataset in datasets_dict.items():
            # é‡‡æ ·å›ºå®šæ•°é‡
            task_samples = dataset.shuffle().select(range(min(samples_per_task, len(dataset))))
            balanced_data.extend(task_samples)
        
        return balanced_data
```

## 6. RLHFä¸DPO ğŸ®

```python
from trl import PPOTrainer, PPOConfig, DPOTrainer

class RLHFTraining:
    """åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ """
    
    def __init__(self, model, reward_model):
        self.model = model
        self.reward_model = reward_model
        
    def setup_ppo(self):
        """è®¾ç½®PPOè®­ç»ƒ"""
        ppo_config = PPOConfig(
            model_name="gpt2",
            learning_rate=1.41e-5,
            log_with="wandb",
            batch_size=128,
            mini_batch_size=4,
            gradient_accumulation_steps=1,
            optimize_cuda_cache=True,
        )
        
        ppo_trainer = PPOTrainer(
            config=ppo_config,
            model=self.model,
            tokenizer=self.tokenizer,
            dataset=self.dataset,
        )
        
        return ppo_trainer
    
    def train_with_ppo(self, ppo_trainer, prompts):
        """PPOè®­ç»ƒå¾ªç¯"""
        for epoch in range(10):
            for batch in prompts:
                # ç”Ÿæˆå“åº”
                query_tensors = batch["input_ids"]
                response_tensors = ppo_trainer.generate(query_tensors)
                
                # è®¡ç®—å¥–åŠ±
                rewards = self.compute_rewards(query_tensors, response_tensors)
                
                # PPOæ­¥éª¤
                stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
                
                # è®°å½•ç»Ÿè®¡
                ppo_trainer.log_stats(stats, batch, rewards)
    
    def compute_rewards(self, queries, responses):
        """è®¡ç®—å¥–åŠ±"""
        # ä½¿ç”¨å¥–åŠ±æ¨¡å‹
        rewards = []
        for query, response in zip(queries, responses):
            reward = self.reward_model(query, response)
            rewards.append(reward)
        
        return torch.tensor(rewards)

class DPOTraining:
    """ç›´æ¥åå¥½ä¼˜åŒ–"""
    
    def __init__(self, model_name="gpt2"):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
    def prepare_preference_dataset(self, data):
        """å‡†å¤‡åå¥½æ•°æ®é›†"""
        # æ•°æ®æ ¼å¼: {"prompt": str, "chosen": str, "rejected": str}
        formatted_data = []
        
        for item in data:
            formatted_item = {
                "prompt": item["prompt"],
                "chosen": item["prompt"] + item["chosen"],
                "rejected": item["prompt"] + item["rejected"]
            }
            formatted_data.append(formatted_item)
        
        return Dataset.from_list(formatted_data)
    
    def train_dpo(self, dataset, output_dir="./dpo_model"):
        """DPOè®­ç»ƒ"""
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=5e-6,
            warmup_steps=100,
            logging_steps=10,
            save_strategy="epoch",
            fp16=True,
        )
        
        dpo_trainer = DPOTrainer(
            model=self.model,
            args=training_args,
            beta=0.1,  # DPOæ¸©åº¦å‚æ•°
            train_dataset=dataset,
            tokenizer=self.tokenizer,
        )
        
        dpo_trainer.train()
        
        return dpo_trainer
```

## 7. é«˜æ•ˆå¾®è°ƒæŠ€å·§ ğŸ’¡

```python
class EfficientFineTuningTricks:
    """é«˜æ•ˆå¾®è°ƒæŠ€å·§é›†åˆ"""
    
    @staticmethod
    def layer_wise_lr_decay(model, base_lr=1e-4, decay_rate=0.9):
        """å±‚çº§å­¦ä¹ ç‡è¡°å‡"""
        parameters = []
        
        # ä»åå‘å‰éå†å±‚
        num_layers = len(list(model.children()))
        for i, (name, param) in enumerate(model.named_parameters()):
            layer_id = num_layers - 1 - i // 2  # ç®€åŒ–çš„å±‚IDè®¡ç®—
            lr = base_lr * (decay_rate ** layer_id)
            
            parameters.append({
                "params": param,
                "lr": lr
            })
        
        return parameters
    
    @staticmethod
    def smart_initialization(model, init_method="xavier"):
        """æ™ºèƒ½åˆå§‹åŒ–æ–°å¢å±‚"""
        for name, param in model.named_parameters():
            if "lora" in name or "adapter" in name:
                if init_method == "xavier":
                    nn.init.xavier_uniform_(param)
                elif init_method == "kaiming":
                    nn.init.kaiming_uniform_(param, a=math.sqrt(5))
                elif init_method == "normal":
                    nn.init.normal_(param, mean=0.0, std=0.02)
    
    @staticmethod
    def progressive_unfreezing(model, current_epoch, total_epochs):
        """æ¸è¿›å¼è§£å†»"""
        num_layers = len(list(model.children()))
        layers_to_unfreeze = int(num_layers * (current_epoch / total_epochs))
        
        # å†»ç»“æ‰€æœ‰å±‚
        for param in model.parameters():
            param.requires_grad = False
        
        # è§£å†»åé¢çš„å±‚
        for i, (name, param) in enumerate(model.named_parameters()):
            if i >= num_layers - layers_to_unfreeze:
                param.requires_grad = True
    
    @staticmethod
    def mixout_regularization(model, mixout_prob=0.1):
        """Mixoutæ­£åˆ™åŒ–"""
        class MixoutWrapper(nn.Module):
            def __init__(self, module, p):
                super().__init__()
                self.module = module
                self.p = p
                self.initial_weights = module.weight.data.clone()
            
            def forward(self, x):
                if self.training:
                    # éšæœºæ··åˆåˆå§‹æƒé‡å’Œå½“å‰æƒé‡
                    mask = torch.bernoulli(torch.ones_like(self.module.weight) * (1 - self.p))
                    mixed_weight = mask * self.module.weight + (1 - mask) * self.initial_weights
                    return F.linear(x, mixed_weight, self.module.bias)
                else:
                    return self.module(x)
        
        # åŒ…è£…æ‰€æœ‰Linearå±‚
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                wrapped = MixoutWrapper(module, mixout_prob)
                setattr(model, name, wrapped)
        
        return model

# æ•°æ®é«˜æ•ˆå¾®è°ƒ
class DataEfficientFineTuning:
    """æ•°æ®é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•"""
    
    @staticmethod
    def few_shot_learning(model, support_set, query_set, k_shot=5):
        """å°‘æ ·æœ¬å­¦ä¹ """
        # æ„å»ºæç¤º
        prompt = "Here are some examples:\n\n"
        
        for i in range(k_shot):
            example = support_set[i]
            prompt += f"Input: {example['input']}\nOutput: {example['output']}\n\n"
        
        prompt += f"Now, given the input: {query_set['input']}\nOutput:"
        
        # ç”Ÿæˆè¾“å‡º
        output = model.generate(prompt)
        
        return output
    
    @staticmethod
    def active_learning_selection(model, unlabeled_data, n_samples=100):
        """ä¸»åŠ¨å­¦ä¹ æ ·æœ¬é€‰æ‹©"""
        uncertainties = []
        
        for data in unlabeled_data:
            # è®¡ç®—æ¨¡å‹ä¸ç¡®å®šæ€§
            with torch.no_grad():
                outputs = model(data['input'])
                # ä½¿ç”¨ç†µä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
                probs = torch.softmax(outputs.logits, dim=-1)
                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1)
                uncertainties.append(entropy.mean().item())
        
        # é€‰æ‹©ä¸ç¡®å®šæ€§æœ€é«˜çš„æ ·æœ¬
        selected_indices = np.argsort(uncertainties)[-n_samples:]
        
        return [unlabeled_data[i] for i in selected_indices]
    
    @staticmethod
    def curriculum_learning(dataset, difficulty_scores):
        """è¯¾ç¨‹å­¦ä¹ ï¼šä»æ˜“åˆ°éš¾"""
        # æŒ‰éš¾åº¦æ’åº
        sorted_indices = np.argsort(difficulty_scores)
        
        # åˆ†é˜¶æ®µ
        easy_samples = sorted_indices[:len(sorted_indices)//3]
        medium_samples = sorted_indices[len(sorted_indices)//3:2*len(sorted_indices)//3]
        hard_samples = sorted_indices[2*len(sorted_indices)//3:]
        
        # æ„å»ºè¯¾ç¨‹
        curriculum = {
            "phase1": [dataset[i] for i in easy_samples],
            "phase2": [dataset[i] for i in medium_samples],
            "phase3": [dataset[i] for i in hard_samples]
        }
        
        return curriculum
```

## 8. è¯„ä¼°ä¸éƒ¨ç½² ğŸ“ˆ

```python
class FineTuningEvaluation:
    """å¾®è°ƒæ¨¡å‹è¯„ä¼°"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def evaluate_perplexity(self, test_dataset):
        """è®¡ç®—å›°æƒ‘åº¦"""
        total_loss = 0
        total_tokens = 0
        
        self.model.eval()
        with torch.no_grad():
            for batch in test_dataset:
                outputs = self.model(**batch)
                total_loss += outputs.loss.item() * batch["input_ids"].size(0)
                total_tokens += batch["input_ids"].numel()
        
        perplexity = torch.exp(torch.tensor(total_loss / total_tokens))
        return perplexity.item()
    
    def evaluate_generation_quality(self, prompts, max_length=100):
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        from nltk.translate.bleu_score import sentence_bleu
        from rouge import Rouge
        
        rouge = Rouge()
        bleu_scores = []
        rouge_scores = []
        
        for prompt in prompts:
            # ç”Ÿæˆ
            generated = self.model.generate(
                prompt["input"],
                max_length=max_length,
                temperature=0.7,
                do_sample=True
            )
            
            # è®¡ç®—BLEU
            reference = prompt["target"].split()
            hypothesis = self.tokenizer.decode(generated[0]).split()
            bleu = sentence_bleu([reference], hypothesis)
            bleu_scores.append(bleu)
            
            # è®¡ç®—ROUGE
            rouge_score = rouge.get_scores(
                self.tokenizer.decode(generated[0]),
                prompt["target"]
            )[0]
            rouge_scores.append(rouge_score)
        
        return {
            "bleu": np.mean(bleu_scores),
            "rouge-1": np.mean([s["rouge-1"]["f"] for s in rouge_scores]),
            "rouge-l": np.mean([s["rouge-l"]["f"] for s in rouge_scores])
        }
    
    def a_b_testing(self, model_a, model_b, test_prompts, num_evaluators=5):
        """A/Bæµ‹è¯•"""
        preferences = {"model_a": 0, "model_b": 0, "tie": 0}
        
        for prompt in test_prompts:
            # ä¸¤ä¸ªæ¨¡å‹ç”Ÿæˆ
            output_a = model_a.generate(prompt)
            output_b = model_b.generate(prompt)
            
            # äººå·¥è¯„ä¼°ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºè‡ªåŠ¨è¯„ä¼°ï¼‰
            score_a = len(output_a)  # ç®€åŒ–ï¼šç”¨é•¿åº¦ä»£æ›¿è´¨é‡
            score_b = len(output_b)
            
            if score_a > score_b:
                preferences["model_a"] += 1
            elif score_b > score_a:
                preferences["model_b"] += 1
            else:
                preferences["tie"] += 1
        
        return preferences

# éƒ¨ç½²ä¼˜åŒ–
class DeploymentOptimization:
    """éƒ¨ç½²ä¼˜åŒ–"""
    
    @staticmethod
    def merge_lora_weights(base_model, lora_model):
        """åˆå¹¶LoRAæƒé‡ç”¨äºéƒ¨ç½²"""
        merged_model = lora_model.merge_and_unload()
        return merged_model
    
    @staticmethod
    def quantize_for_deployment(model):
        """éƒ¨ç½²å‰é‡åŒ–"""
        from transformers import AutoModelForCausalLM
        import torch.quantization as quantization
        
        # åŠ¨æ€é‡åŒ–
        quantized_model = quantization.quantize_dynamic(
            model,
            {nn.Linear},
            dtype=torch.qint8
        )
        
        return quantized_model
    
    @staticmethod
    def optimize_for_inference(model):
        """æ¨ç†ä¼˜åŒ–"""
        model.eval()
        
        # ç¦ç”¨dropout
        for module in model.modules():
            if isinstance(module, nn.Dropout):
                module.p = 0
        
        # èåˆæ“ä½œ
        if hasattr(torch.quantization, 'fuse_modules'):
            torch.quantization.fuse_modules(
                model,
                [['conv', 'bn', 'relu']],
                inplace=True
            )
        
        return model
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“

```python
def finetuning_best_practices():
    """å¾®è°ƒæœ€ä½³å®è·µ"""
    
    practices = {
        "é€‰æ‹©å¾®è°ƒæ–¹æ³•": {
            "å…¨é‡å¾®è°ƒ": "å°æ¨¡å‹(<1B)ï¼Œå……è¶³è®¡ç®—èµ„æº",
            "LoRA": "å¤§æ¨¡å‹(>1B)ï¼Œä¸­ç­‰èµ„æº",
            "QLoRA": "è¶…å¤§æ¨¡å‹(>7B)ï¼Œæœ‰é™èµ„æº",
            "Prefix/Prompt": "æå°‘å‚æ•°è°ƒæ•´éœ€æ±‚"
        },
        
        "è¶…å‚æ•°å»ºè®®": {
            "å­¦ä¹ ç‡": "æ¯”é¢„è®­ç»ƒå°10-100å€",
            "æ‰¹æ¬¡å¤§å°": "å°½å¯èƒ½å¤§ï¼ˆä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼‰",
            "è®­ç»ƒè½®æ•°": "3-5è½®ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰",
            "Warmup": "æ€»æ­¥æ•°çš„5-10%"
        },
        
        "æ•°æ®å‡†å¤‡": {
            "è´¨é‡>æ•°é‡": "é«˜è´¨é‡çš„å°‘é‡æ•°æ®èƒœè¿‡å¤§é‡ä½è´¨é‡æ•°æ®",
            "æ ¼å¼ç»Ÿä¸€": "ä¿æŒæŒ‡ä»¤æ ¼å¼ä¸€è‡´",
            "å¤šæ ·æ€§": "è¦†ç›–ç›®æ ‡ä»»åŠ¡çš„å„ç§æƒ…å†µ",
            "æ¸…æ´—": "å»é™¤é‡å¤å’Œä½è´¨é‡æ ·æœ¬"
        },
        
        "è®­ç»ƒæŠ€å·§": {
            "æ—©åœ": "ç›‘æ§éªŒè¯é›†æ€§èƒ½",
            "æ£€æŸ¥ç‚¹": "å®šæœŸä¿å­˜ï¼Œæ”¯æŒæ¢å¤",
            "æ··åˆç²¾åº¦": "FP16/BF16åŠ é€Ÿè®­ç»ƒ",
            "æ¢¯åº¦è£å‰ª": "é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸"
        },
        
        "è¯„ä¼°ç­–ç•¥": {
            "å¤šç»´åº¦": "è‡ªåŠ¨æŒ‡æ ‡+äººå·¥è¯„ä¼°",
            "é¢†åŸŸç›¸å…³": "ä½¿ç”¨é¢†åŸŸç‰¹å®šçš„è¯„ä¼°é›†",
            "å¯¹æ¯”åŸºçº¿": "ä¸åŸå§‹æ¨¡å‹å’Œå…¶ä»–æ–¹æ³•å¯¹æ¯”",
            "åœ¨çº¿æµ‹è¯•": "å°æµé‡A/Bæµ‹è¯•"
        }
    }
    
    return practices

# å¸¸è§é—®é¢˜è§£å†³
troubleshooting = """
é—®é¢˜1ï¼šæ˜¾å­˜ä¸è¶³
- ä½¿ç”¨QLoRAæˆ–æ›´å°çš„rå€¼
- å‡å°æ‰¹æ¬¡å¤§å°
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

é—®é¢˜2ï¼šè®­ç»ƒä¸ç¨³å®š
- é™ä½å­¦ä¹ ç‡
- å¢åŠ warmupæ­¥æ•°
- ä½¿ç”¨æ¢¯åº¦è£å‰ª
- æ£€æŸ¥æ•°æ®è´¨é‡

é—®é¢˜3ï¼šè¿‡æ‹Ÿåˆ
- å¢åŠ dropout
- å‡å°‘è®­ç»ƒè½®æ•°
- ä½¿ç”¨æ›´å¤šæ•°æ®
- å¢åŠ æ­£åˆ™åŒ–

é—®é¢˜4ï¼šæ•ˆæœä¸ä½³
- æ£€æŸ¥æ•°æ®æ ‡æ³¨è´¨é‡
- è°ƒæ•´LoRAçš„rå€¼
- å°è¯•ä¸åŒçš„ç›®æ ‡æ¨¡å—
- å¢åŠ è®­ç»ƒæ•°æ®
"""

print("å¾®è°ƒæŒ‡å—å®Œæˆï¼")
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [RAGç³»ç»Ÿ](rag_systems.md) - æ£€ç´¢å¢å¼ºç”Ÿæˆ
- [LLMéƒ¨ç½²](llm_deployment.md) - å¤§æ¨¡å‹éƒ¨ç½²
- [NLPæ¨¡å‹](nlp_models.md) - NLPæ¶æ„è¯¦è§£