# æ·±åº¦å­¦ä¹ åŸºç¡€ï¼šä»ç¥ç»å…ƒåˆ°æ·±åº¦ç½‘ç»œ ğŸ§ 

å…¨é¢æŒæ¡æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µã€æ¶æ„å’Œå®ç°ã€‚

## 1. æ·±åº¦å­¦ä¹ æ¦‚è¿° ğŸŒŸ

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class DeepLearningBasics:
    """æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ"""
    
    def __init__(self):
        self.history = {
            "1943": "McCulloch-Pittsç¥ç»å…ƒæ¨¡å‹",
            "1957": "Perceptronæ„ŸçŸ¥æœº",
            "1969": "Minskyè¯æ˜XORé—®é¢˜",
            "1986": "åå‘ä¼ æ’­ç®—æ³•",
            "1989": "CNN (LeNet)",
            "1997": "LSTM",
            "2006": "æ·±åº¦ä¿¡å¿µç½‘ç»œ",
            "2012": "AlexNet",
            "2014": "GAN",
            "2017": "Transformer",
            "2018": "BERT/GPT",
            "2020": "GPT-3",
            "2023": "GPT-4"
        }
    
    def demonstrate_neuron(self):
        """æ¼”ç¤ºå•ä¸ªç¥ç»å…ƒ"""
        # ç®€å•çš„ç¥ç»å…ƒå®ç°
        class Neuron:
            def __init__(self, n_inputs):
                self.weights = np.random.randn(n_inputs)
                self.bias = np.random.randn()
            
            def forward(self, inputs):
                # çº¿æ€§ç»„åˆ
                z = np.dot(inputs, self.weights) + self.bias
                # æ¿€æ´»å‡½æ•°ï¼ˆsigmoidï¼‰
                output = 1 / (1 + np.exp(-z))
                return output
        
        # ç¤ºä¾‹
        neuron = Neuron(3)
        inputs = np.array([1.0, 2.0, 3.0])
        output = neuron.forward(inputs)
        
        print("å•ä¸ªç¥ç»å…ƒç¤ºä¾‹:")
        print(f"è¾“å…¥: {inputs}")
        print(f"æƒé‡: {neuron.weights}")
        print(f"åç½®: {neuron.bias:.4f}")
        print(f"è¾“å‡º: {output:.4f}")
        
        return neuron
    
    def gradient_descent_visualization(self):
        """æ¢¯åº¦ä¸‹é™å¯è§†åŒ–"""
        # åˆ›å»ºç®€å•çš„æŸå¤±å‡½æ•°æ™¯è§‚
        x = np.linspace(-5, 5, 100)
        y = np.linspace(-5, 5, 100)
        X, Y = np.meshgrid(x, y)
        Z = X**2 + Y**2  # ç®€å•çš„äºŒæ¬¡å‡½æ•°
        
        # æ¢¯åº¦ä¸‹é™è½¨è¿¹
        learning_rate = 0.1
        current_pos = np.array([4.0, 4.0])
        trajectory = [current_pos.copy()]
        
        for _ in range(20):
            gradient = 2 * current_pos  # æ¢¯åº¦
            current_pos = current_pos - learning_rate * gradient
            trajectory.append(current_pos.copy())
        
        trajectory = np.array(trajectory)
        
        # å¯è§†åŒ–
        plt.figure(figsize=(10, 8))
        plt.contour(X, Y, Z, levels=20, alpha=0.6)
        plt.plot(trajectory[:, 0], trajectory[:, 1], 'ro-', 
                markersize=8, linewidth=2, label='æ¢¯åº¦ä¸‹é™è·¯å¾„')
        plt.plot(0, 0, 'g*', markersize=15, label='æœ€ä¼˜ç‚¹')
        plt.xlabel('å‚æ•°1')
        plt.ylabel('å‚æ•°2')
        plt.title('æ¢¯åº¦ä¸‹é™å¯è§†åŒ–')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
```

## 2. ç¥ç»ç½‘ç»œåŸºç¡€ ğŸ”—

```python
class NeuralNetworkBasics:
    """ç¥ç»ç½‘ç»œåŸºç¡€å®ç°"""
    
    def __init__(self):
        self.models = {}
    
    def build_simple_network(self, input_size=784, hidden_size=128, output_size=10):
        """æ„å»ºç®€å•çš„å…¨è¿æ¥ç½‘ç»œ"""
        class SimpleNet(nn.Module):
            def __init__(self, input_size, hidden_size, output_size):
                super(SimpleNet, self).__init__()
                self.fc1 = nn.Linear(input_size, hidden_size)
                self.fc2 = nn.Linear(hidden_size, hidden_size)
                self.fc3 = nn.Linear(hidden_size, output_size)
                self.dropout = nn.Dropout(0.2)
            
            def forward(self, x):
                x = x.view(x.size(0), -1)  # Flatten
                x = F.relu(self.fc1(x))
                x = self.dropout(x)
                x = F.relu(self.fc2(x))
                x = self.dropout(x)
                x = self.fc3(x)
                return x
        
        model = SimpleNet(input_size, hidden_size, output_size)
        self.models['simple_net'] = model
        
        # æ‰“å°æ¨¡å‹ç»“æ„
        print("ç®€å•ç¥ç»ç½‘ç»œç»“æ„:")
        print(model)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"\næ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
    
    def forward_propagation_demo(self):
        """å‰å‘ä¼ æ’­æ¼”ç¤º"""
        # åˆ›å»ºç®€å•ç½‘ç»œ
        input_size = 3
        hidden_size = 4
        output_size = 2
        
        # æ‰‹åŠ¨å®šä¹‰æƒé‡å’Œåç½®
        W1 = torch.randn(input_size, hidden_size)
        b1 = torch.randn(hidden_size)
        W2 = torch.randn(hidden_size, output_size)
        b2 = torch.randn(output_size)
        
        # è¾“å…¥
        x = torch.randn(1, input_size)
        
        print("å‰å‘ä¼ æ’­æ¼”ç¤º:")
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # ç¬¬ä¸€å±‚
        z1 = torch.matmul(x, W1) + b1
        a1 = F.relu(z1)
        print(f"éšè—å±‚è¾“å‡ºå½¢çŠ¶: {a1.shape}")
        
        # ç¬¬äºŒå±‚
        z2 = torch.matmul(a1, W2) + b2
        output = F.softmax(z2, dim=1)
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºå€¼: {output}")
        
        return output
    
    def backpropagation_demo(self):
        """åå‘ä¼ æ’­æ¼”ç¤º"""
        # åˆ›å»ºç®€å•çš„è®¡ç®—å›¾
        x = torch.tensor([1.0, 2.0], requires_grad=True)
        w = torch.tensor([0.5, 0.5], requires_grad=True)
        b = torch.tensor(1.0, requires_grad=True)
        
        # å‰å‘ä¼ æ’­
        z = torch.sum(x * w) + b
        y = torch.sigmoid(z)
        
        # å®šä¹‰æŸå¤±
        target = torch.tensor(1.0)
        loss = (y - target) ** 2
        
        print("åå‘ä¼ æ’­æ¼”ç¤º:")
        print(f"è¾“å…¥ x: {x.data}")
        print(f"æƒé‡ w: {w.data}")
        print(f"åç½® b: {b.data}")
        print(f"è¾“å‡º y: {y.data:.4f}")
        print(f"æŸå¤±: {loss.data:.4f}")
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        print("\næ¢¯åº¦:")
        print(f"dx: {x.grad}")
        print(f"dw: {w.grad}")
        print(f"db: {b.grad:.4f}")
        
        return x, w, b
```

## 3. æ¿€æ´»å‡½æ•°è¯¦è§£ âš¡

```python
class ActivationFunctions:
    """æ¿€æ´»å‡½æ•°å®ç°ä¸å¯è§†åŒ–"""
    
    def __init__(self):
        self.activations = {
            'sigmoid': torch.sigmoid,
            'tanh': torch.tanh,
            'relu': F.relu,
            'leaky_relu': F.leaky_relu,
            'elu': F.elu,
            'gelu': F.gelu,
            'swish': lambda x: x * torch.sigmoid(x)
        }
    
    def visualize_activations(self):
        """å¯è§†åŒ–æ¿€æ´»å‡½æ•°"""
        x = torch.linspace(-5, 5, 100)
        
        fig, axes = plt.subplots(2, 4, figsize=(15, 8))
        axes = axes.flatten()
        
        for idx, (name, func) in enumerate(self.activations.items()):
            y = func(x)
            
            # è®¡ç®—å¯¼æ•°ï¼ˆè¿‘ä¼¼ï¼‰
            x_grad = x.clone().requires_grad_(True)
            y_grad = func(x_grad)
            y_grad.sum().backward()
            grad = x_grad.grad
            
            axes[idx].plot(x.numpy(), y.numpy(), label=name, linewidth=2)
            axes[idx].plot(x.numpy(), grad.numpy(), '--', 
                          label=f'{name} gradient', alpha=0.7)
            axes[idx].set_title(name.upper())
            axes[idx].set_xlabel('x')
            axes[idx].set_ylabel('f(x)')
            axes[idx].grid(True, alpha=0.3)
            axes[idx].legend()
            axes[idx].axhline(y=0, color='k', linewidth=0.5)
            axes[idx].axvline(x=0, color='k', linewidth=0.5)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(self.activations), len(axes)):
            axes[idx].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def compare_activations(self, input_tensor):
        """æ¯”è¾ƒä¸åŒæ¿€æ´»å‡½æ•°çš„è¾“å‡º"""
        results = {}
        
        for name, func in self.activations.items():
            output = func(input_tensor)
            results[name] = {
                'mean': output.mean().item(),
                'std': output.std().item(),
                'min': output.min().item(),
                'max': output.max().item(),
                'zeros': (output == 0).sum().item()
            }
        
        # è½¬æ¢ä¸ºDataFrame
        import pandas as pd
        df = pd.DataFrame(results).T
        print("æ¿€æ´»å‡½æ•°è¾“å‡ºç»Ÿè®¡:")
        print(df.round(4))
        
        return df
```

## 4. ä¼˜åŒ–å™¨å®ç° ğŸ¯

```python
class Optimizers:
    """ä¼˜åŒ–å™¨å®ç°ä¸æ¯”è¾ƒ"""
    
    def __init__(self):
        self.optimizers = {}
    
    def create_optimizers(self, model, lr=0.01):
        """åˆ›å»ºä¸åŒçš„ä¼˜åŒ–å™¨"""
        self.optimizers = {
            'SGD': optim.SGD(model.parameters(), lr=lr),
            'SGD_Momentum': optim.SGD(model.parameters(), lr=lr, momentum=0.9),
            'Adam': optim.Adam(model.parameters(), lr=lr),
            'AdamW': optim.AdamW(model.parameters(), lr=lr),
            'RMSprop': optim.RMSprop(model.parameters(), lr=lr),
            'Adagrad': optim.Adagrad(model.parameters(), lr=lr)
        }
        
        return self.optimizers
    
    def compare_optimizers(self, loss_landscape):
        """æ¯”è¾ƒä¼˜åŒ–å™¨åœ¨æŸå¤±æ™¯è§‚ä¸Šçš„è¡¨ç°"""
        # ç®€åŒ–çš„2Dä¼˜åŒ–é—®é¢˜
        def rosenbrock(x, y):
            return (1 - x)**2 + 100 * (y - x**2)**2
        
        # ä¸åŒä¼˜åŒ–å™¨çš„è½¨è¿¹
        trajectories = {}
        
        for name in ['SGD', 'SGD_Momentum', 'Adam']:
            # åˆå§‹ç‚¹
            x = torch.tensor([2.0], requires_grad=True)
            y = torch.tensor([2.0], requires_grad=True)
            
            # é€‰æ‹©ä¼˜åŒ–å™¨
            if name == 'SGD':
                optimizer = optim.SGD([x, y], lr=0.001)
            elif name == 'SGD_Momentum':
                optimizer = optim.SGD([x, y], lr=0.001, momentum=0.9)
            else:  # Adam
                optimizer = optim.Adam([x, y], lr=0.01)
            
            # ä¼˜åŒ–è¿‡ç¨‹
            trajectory = []
            for _ in range(100):
                optimizer.zero_grad()
                loss = rosenbrock(x, y)
                loss.backward()
                optimizer.step()
                trajectory.append([x.item(), y.item()])
            
            trajectories[name] = np.array(trajectory)
        
        # å¯è§†åŒ–
        x_range = np.linspace(-2, 3, 100)
        y_range = np.linspace(-1, 3, 100)
        X, Y = np.meshgrid(x_range, y_range)
        Z = rosenbrock(X, Y)
        
        plt.figure(figsize=(12, 8))
        plt.contour(X, Y, Z, levels=50, alpha=0.6)
        
        colors = {'SGD': 'blue', 'SGD_Momentum': 'green', 'Adam': 'red'}
        for name, trajectory in trajectories.items():
            plt.plot(trajectory[:, 0], trajectory[:, 1], 
                    'o-', color=colors[name], label=name, 
                    markersize=4, alpha=0.7)
        
        plt.plot(1, 1, 'k*', markersize=15, label='æœ€ä¼˜ç‚¹')
        plt.xlabel('x')
        plt.ylabel('y')
        plt.title('ä¼˜åŒ–å™¨æ¯”è¾ƒï¼šRosenbrockå‡½æ•°')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return trajectories
```

## 5. æ­£åˆ™åŒ–æŠ€æœ¯ ğŸ›¡ï¸

```python
class RegularizationTechniques:
    """æ­£åˆ™åŒ–æŠ€æœ¯å®ç°"""
    
    def __init__(self):
        self.techniques = []
    
    def dropout_demo(self, p=0.5):
        """Dropoutæ¼”ç¤º"""
        # åˆ›å»ºè¾“å…¥
        x = torch.randn(100, 10)
        
        # è®­ç»ƒæ¨¡å¼çš„Dropout
        dropout = nn.Dropout(p=p)
        dropout.train()
        x_train = dropout(x)
        
        # è¯„ä¼°æ¨¡å¼çš„Dropout
        dropout.eval()
        x_eval = dropout(x)
        
        print(f"Dropoutæ¼”ç¤º (p={p}):")
        print(f"åŸå§‹è¾“å…¥å‡å€¼: {x.mean():.4f}")
        print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºå‡å€¼: {x_train.mean():.4f}")
        print(f"è¯„ä¼°æ¨¡å¼è¾“å‡ºå‡å€¼: {x_eval.mean():.4f}")
        print(f"è®­ç»ƒæ¨¡å¼é›¶å€¼æ¯”ä¾‹: {(x_train == 0).float().mean():.4f}")
        print(f"è¯„ä¼°æ¨¡å¼é›¶å€¼æ¯”ä¾‹: {(x_eval == 0).float().mean():.4f}")
        
        return x_train, x_eval
    
    def batch_norm_demo(self):
        """æ‰¹å½’ä¸€åŒ–æ¼”ç¤º"""
        # åˆ›å»ºæ‰¹å½’ä¸€åŒ–å±‚
        bn = nn.BatchNorm1d(10)
        
        # åˆ›å»ºè¾“å…¥ï¼ˆæ‰¹æ¬¡å¤§å°=32ï¼Œç‰¹å¾æ•°=10ï¼‰
        x = torch.randn(32, 10) * 5 + 2  # å‡å€¼â‰ˆ2ï¼Œæ ‡å‡†å·®â‰ˆ5
        
        print("æ‰¹å½’ä¸€åŒ–æ¼”ç¤º:")
        print(f"è¾“å…¥ç»Ÿè®¡ - å‡å€¼: {x.mean():.4f}, æ ‡å‡†å·®: {x.std():.4f}")
        
        # åº”ç”¨æ‰¹å½’ä¸€åŒ–
        bn.train()
        x_normalized = bn(x)
        
        print(f"è¾“å‡ºç»Ÿè®¡ - å‡å€¼: {x_normalized.mean():.4f}, "
              f"æ ‡å‡†å·®: {x_normalized.std():.4f}")
        
        return x_normalized
    
    def weight_decay_comparison(self):
        """æƒé‡è¡°å‡æ¯”è¾ƒ"""
        # åˆ›å»ºç®€å•æ¨¡å‹
        model_no_wd = nn.Linear(10, 1)
        model_with_wd = nn.Linear(10, 1)
        
        # å¤åˆ¶åˆå§‹æƒé‡
        model_with_wd.weight.data = model_no_wd.weight.data.clone()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer_no_wd = optim.SGD(model_no_wd.parameters(), lr=0.1)
        optimizer_with_wd = optim.SGD(model_with_wd.parameters(), 
                                     lr=0.1, weight_decay=0.01)
        
        # è®­ç»ƒæ­¥éª¤
        x = torch.randn(100, 10)
        y = torch.randn(100, 1)
        
        weight_norms_no_wd = []
        weight_norms_with_wd = []
        
        for _ in range(50):
            # æ— æƒé‡è¡°å‡
            optimizer_no_wd.zero_grad()
            loss = F.mse_loss(model_no_wd(x), y)
            loss.backward()
            optimizer_no_wd.step()
            weight_norms_no_wd.append(model_no_wd.weight.norm().item())
            
            # æœ‰æƒé‡è¡°å‡
            optimizer_with_wd.zero_grad()
            loss = F.mse_loss(model_with_wd(x), y)
            loss.backward()
            optimizer_with_wd.step()
            weight_norms_with_wd.append(model_with_wd.weight.norm().item())
        
        # å¯è§†åŒ–
        plt.figure(figsize=(10, 6))
        plt.plot(weight_norms_no_wd, label='æ— æƒé‡è¡°å‡')
        plt.plot(weight_norms_with_wd, label='æœ‰æƒé‡è¡°å‡ (0.01)')
        plt.xlabel('è®­ç»ƒæ­¥æ•°')
        plt.ylabel('æƒé‡èŒƒæ•°')
        plt.title('æƒé‡è¡°å‡æ•ˆæœæ¯”è¾ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return weight_norms_no_wd, weight_norms_with_wd
    
    def early_stopping(self, patience=5):
        """æ—©åœå®ç°"""
        class EarlyStopping:
            def __init__(self, patience=5, delta=0):
                self.patience = patience
                self.delta = delta
                self.counter = 0
                self.best_score = None
                self.early_stop = False
                self.val_loss_min = np.Inf
            
            def __call__(self, val_loss, model):
                score = -val_loss
                
                if self.best_score is None:
                    self.best_score = score
                    self.save_checkpoint(val_loss, model)
                elif score < self.best_score + self.delta:
                    self.counter += 1
                    print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
                    if self.counter >= self.patience:
                        self.early_stop = True
                else:
                    self.best_score = score
                    self.save_checkpoint(val_loss, model)
                    self.counter = 0
            
            def save_checkpoint(self, val_loss, model):
                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
                torch.save(model.state_dict(), 'checkpoint.pt')
                self.val_loss_min = val_loss
        
        return EarlyStopping(patience)
```

## 6. è®­ç»ƒå¾ªç¯å®ç° ğŸ”„

```python
class TrainingLoop:
    """å®Œæ•´çš„è®­ç»ƒå¾ªç¯å®ç°"""
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def train_epoch(self, dataloader, optimizer, criterion):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(self.device), target.to(self.device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            output = self.model(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate(self, dataloader, criterion):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(self.device), target.to(self.device)
                
                output = self.model(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader, epochs, optimizer, criterion):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        print("å¼€å§‹è®­ç»ƒ...")
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(
                train_loader, optimizer, criterion
            )
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader, criterion)
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            # æ‰“å°è¿›åº¦
            print(f'Epoch [{epoch+1}/{epochs}] '
                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '
                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        print("è®­ç»ƒå®Œæˆ!")
        return self.history
    
    def plot_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(self.history['train_loss'], label='Train Loss')
        ax1.plot(self.history['val_loss'], label='Val Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(self.history['train_acc'], label='Train Acc')
        ax2.plot(self.history['val_acc'], label='Val Acc')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.set_title('Training and Validation Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
```

## 7. å®æˆ˜é¡¹ç›®ï¼šMNISTåˆ†ç±» ğŸ”¢

```python
class MNISTProject:
    """MNISTæ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®"""
    
    def __init__(self):
        self.model = None
        self.train_loader = None
        self.test_loader = None
    
    def prepare_data(self, batch_size=64):
        """å‡†å¤‡MNISTæ•°æ®"""
        from torchvision import datasets, transforms
        
        # æ•°æ®å˜æ¢
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        # ä¸‹è½½å’ŒåŠ è½½æ•°æ®
        train_dataset = datasets.MNIST('./data', train=True, 
                                      download=True, transform=transform)
        test_dataset = datasets.MNIST('./data', train=False, 
                                     transform=transform)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                                      shuffle=True)
        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                                     shuffle=False)
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
        return self.train_loader, self.test_loader
    
    def create_cnn_model(self):
        """åˆ›å»ºCNNæ¨¡å‹"""
        class CNN(nn.Module):
            def __init__(self):
                super(CNN, self).__init__()
                self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
                self.pool = nn.MaxPool2d(2, 2)
                self.fc1 = nn.Linear(64 * 7 * 7, 128)
                self.fc2 = nn.Linear(128, 10)
                self.dropout = nn.Dropout(0.25)
            
            def forward(self, x):
                x = self.pool(F.relu(self.conv1(x)))
                x = self.pool(F.relu(self.conv2(x)))
                x = x.view(-1, 64 * 7 * 7)
                x = F.relu(self.fc1(x))
                x = self.dropout(x)
                x = self.fc2(x)
                return F.log_softmax(x, dim=1)
        
        self.model = CNN()
        print("CNNæ¨¡å‹ç»“æ„:")
        print(self.model)
        
        return self.model
    
    def train_model(self, epochs=10, lr=0.001):
        """è®­ç»ƒæ¨¡å‹"""
        if self.model is None:
            self.create_cnn_model()
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.model.to(device)
        
        optimizer = optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        trainer = TrainingLoop(self.model, device)
        history = trainer.train(
            self.train_loader, 
            self.test_loader,
            epochs,
            optimizer,
            criterion
        )
        
        trainer.plot_history()
        
        return history
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        correct = 0
        total = 0
        
        device = next(self.model.parameters()).device
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(device), target.to(device)
                output = self.model(data)
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        accuracy = 100. * correct / total
        print(f'æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%')
        
        return accuracy
    
    def visualize_predictions(self, n_samples=10):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        self.model.eval()
        device = next(self.model.parameters()).device
        
        # è·å–ä¸€æ‰¹æ•°æ®
        data, target = next(iter(self.test_loader))
        data, target = data[:n_samples].to(device), target[:n_samples]
        
        # é¢„æµ‹
        with torch.no_grad():
            output = self.model(data)
            _, predicted = output.max(1)
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(2, 5, figsize=(12, 6))
        axes = axes.flatten()
        
        for i in range(n_samples):
            img = data[i].cpu().numpy().squeeze()
            axes[i].imshow(img, cmap='gray')
            axes[i].set_title(f'True: {target[i].item()}, '
                            f'Pred: {predicted[i].item()}')
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.show()
```

## 8. è°ƒè¯•ä¸ä¼˜åŒ–æŠ€å·§ ğŸ”§

```python
class DebuggingTips:
    """æ·±åº¦å­¦ä¹ è°ƒè¯•æŠ€å·§"""
    
    @staticmethod
    def check_gradient_flow(model):
        """æ£€æŸ¥æ¢¯åº¦æµ"""
        # è¿›è¡Œä¸€æ¬¡å‰å‘å’Œåå‘ä¼ æ’­
        x = torch.randn(1, *model.input_shape)
        y = model(x)
        loss = y.sum()
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                print(f'{name}: grad_norm = {grad_norm:.6f}')
                
                if grad_norm == 0:
                    print(f'  âš ï¸ è­¦å‘Š: {name} çš„æ¢¯åº¦ä¸º0!')
                elif grad_norm > 100:
                    print(f'  âš ï¸ è­¦å‘Š: {name} çš„æ¢¯åº¦å¯èƒ½çˆ†ç‚¸!')
    
    @staticmethod
    def diagnose_training_issues(history):
        """è¯Šæ–­è®­ç»ƒé—®é¢˜"""
        train_loss = history['train_loss']
        val_loss = history['val_loss']
        
        # æ£€æŸ¥æ˜¯å¦å­¦ä¹ 
        if train_loss[-1] >= train_loss[0] * 0.95:
            print("âš ï¸ æ¨¡å‹ä¼¼ä¹æ²¡æœ‰å­¦ä¹ !")
            print("å»ºè®®:")
            print("- æ£€æŸ¥å­¦ä¹ ç‡ï¼ˆå¯èƒ½å¤ªå°æˆ–å¤ªå¤§ï¼‰")
            print("- æ£€æŸ¥æ•°æ®å’Œæ ‡ç­¾æ˜¯å¦æ­£ç¡®")
            print("- æ£€æŸ¥æŸå¤±å‡½æ•°æ˜¯å¦åˆé€‚")
        
        # æ£€æŸ¥è¿‡æ‹Ÿåˆ
        if val_loss[-1] > val_loss[min(5, len(val_loss)-1)] * 1.1:
            print("âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆ!")
            print("å»ºè®®:")
            print("- å¢åŠ æ­£åˆ™åŒ–ï¼ˆdropout, weight decayï¼‰")
            print("- å‡å°‘æ¨¡å‹å®¹é‡")
            print("- å¢åŠ æ•°æ®é‡æˆ–æ•°æ®å¢å¼º")
        
        # æ£€æŸ¥æ¬ æ‹Ÿåˆ
        if train_loss[-1] > 0.5:  # é˜ˆå€¼å–å†³äºä»»åŠ¡
            print("âš ï¸ å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆ!")
            print("å»ºè®®:")
            print("- å¢åŠ æ¨¡å‹å®¹é‡")
            print("- è®­ç»ƒæ›´é•¿æ—¶é—´")
            print("- å‡å°‘æ­£åˆ™åŒ–")
    
    @staticmethod
    def memory_optimization_tips():
        """å†…å­˜ä¼˜åŒ–æŠ€å·§"""
        tips = """
        GPUå†…å­˜ä¼˜åŒ–æŠ€å·§:
        
        1. å‡å°æ‰¹æ¬¡å¤§å°
        2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
        3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
        4. åŠæ—¶åˆ é™¤ä¸éœ€è¦çš„ä¸­é—´å˜é‡
        5. ä½¿ç”¨ torch.no_grad() åœ¨æ¨ç†æ—¶
        6. ä½¿ç”¨ checkpoint æŠ€æœ¯ï¼ˆæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
        7. ä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆnum_workers, pin_memoryï¼‰
        8. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ¨¡å‹å‰ªæ
        """
        print(tips)
        
        # æ˜¾ç¤ºå½“å‰GPUå†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            print(f"\nå½“å‰GPUå†…å­˜ä½¿ç”¨:")
            print(f"å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
            print(f"å·²ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“‹

```python
def deep_learning_best_practices():
    """æ·±åº¦å­¦ä¹ æœ€ä½³å®è·µ"""
    
    practices = {
        "æ•°æ®å‡†å¤‡": [
            "æ•°æ®å½’ä¸€åŒ–/æ ‡å‡†åŒ–",
            "æ•°æ®å¢å¼ºæé«˜æ³›åŒ–",
            "æ£€æŸ¥æ•°æ®åˆ†å¸ƒ",
            "å¤„ç†ç±»åˆ«ä¸å¹³è¡¡",
            "åˆç†çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ†å‰²"
        ],
        
        "æ¨¡å‹è®¾è®¡": [
            "ä»ç®€å•æ¨¡å‹å¼€å§‹",
            "é€æ­¥å¢åŠ å¤æ‚åº¦",
            "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹",
            "æ³¨æ„æ„Ÿå—é‡å¤§å°",
            "è€ƒè™‘è®¡ç®—æ•ˆç‡"
        ],
        
        "è®­ç»ƒæŠ€å·§": [
            "ä½¿ç”¨åˆé€‚çš„åˆå§‹åŒ–",
            "é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨",
            "å­¦ä¹ ç‡è°ƒåº¦",
            "æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ",
            "æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸"
        ],
        
        "è°ƒè¯•æ–¹æ³•": [
            "å¯è§†åŒ–ä¸­é—´å±‚è¾“å‡º",
            "ç›‘æ§æ¢¯åº¦æµ",
            "æ£€æŸ¥æŸå¤±æ›²çº¿",
            "ä½¿ç”¨TensorBoard",
            "å°æ•°æ®é›†å¿«é€Ÿè¿­ä»£"
        ],
        
        "æ€§èƒ½ä¼˜åŒ–": [
            "æ··åˆç²¾åº¦è®­ç»ƒ",
            "å¤šGPUå¹¶è¡Œ",
            "ä¼˜åŒ–æ•°æ®ç®¡é“",
            "æ¨¡å‹é‡åŒ–",
            "çŸ¥è¯†è’¸é¦"
        ]
    }
    
    return practices

# å¸¸è§é”™è¯¯
common_mistakes = """
æ·±åº¦å­¦ä¹ å¸¸è§é”™è¯¯ï¼š

1. å¿˜è®°å½’ä¸€åŒ–è¾“å…¥æ•°æ®
2. å­¦ä¹ ç‡è®¾ç½®ä¸å½“
3. æ‰¹æ¬¡å¤§å°å¤ªå¤§å¯¼è‡´å†…å­˜æº¢å‡º
4. å¿˜è®°è®¾ç½®model.eval()åœ¨æµ‹è¯•æ—¶
5. æ•°æ®æ³„éœ²ï¼ˆæµ‹è¯•æ•°æ®æ³„éœ²åˆ°è®­ç»ƒï¼‰
6. ä¸æ­£ç¡®çš„æŸå¤±å‡½æ•°
7. å¿˜è®°æ¢¯åº¦æ¸…é›¶
8. ç»´åº¦ä¸åŒ¹é…
9. ä½¿ç”¨äº†é”™è¯¯çš„æ¿€æ´»å‡½æ•°
10. è¿‡æ—©ä¼˜åŒ–ï¼ˆåº”è¯¥å…ˆè®©æ¨¡å‹å·¥ä½œï¼‰
"""

print("æ·±åº¦å­¦ä¹ åŸºç¡€æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [CNNæ¶æ„](cnn_architectures.md) - å·ç§¯ç¥ç»ç½‘ç»œæ·±å…¥
- [è®­ç»ƒæŠ€å·§](training_tricks.md) - é«˜çº§è®­ç»ƒæŠ€æœ¯
- [NLPæ¨¡å‹](nlp_models.md) - è‡ªç„¶è¯­è¨€å¤„ç†