# RAGç³»ç»Ÿå®æˆ˜ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆ ğŸ”

ä»åŸç†åˆ°å®è·µï¼Œæ„å»ºé«˜è´¨é‡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿã€‚

## 1. RAGæ ¸å¿ƒæ¦‚å¿µ ğŸ¯

```python
import numpy as np
from typing import List, Dict, Any
import faiss
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer

class RAGPipeline:
    """RAGç³»ç»ŸåŸºç¡€æ¶æ„"""
    
    def __init__(self, retriever_model="BAAI/bge-large-zh-v1.5", 
                 generator_model="meta-llama/Llama-2-7b-chat-hf"):
        # æ£€ç´¢æ¨¡å‹
        self.retriever = SentenceTransformer(retriever_model)
        
        # ç”Ÿæˆæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(generator_model)
        self.generator = AutoModel.from_pretrained(generator_model)
        
        # å‘é‡æ•°æ®åº“
        self.index = None
        self.documents = []
    
    def build_index(self, documents: List[str]):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        # ç¼–ç æ–‡æ¡£
        embeddings = self.retriever.encode(documents)
        
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings.astype('float32'))
        
        self.documents = documents
    
    def retrieve(self, query: str, k: int = 5) -> List[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.retriever.encode([query])
        
        # æœç´¢æœ€è¿‘é‚»
        distances, indices = self.index.search(
            query_embedding.astype('float32'), k
        )
        
        # è¿”å›æ–‡æ¡£
        return [self.documents[idx] for idx in indices[0]]
    
    def generate(self, query: str, context: List[str]) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
        # æ„å»ºæç¤º
        prompt = self._build_prompt(query, context)
        
        # ç”Ÿæˆå›ç­”
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(
            **inputs,
            max_length=512,
            temperature=0.7,
            do_sample=True
        )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    
    def _build_prompt(self, query: str, context: List[str]) -> str:
        """æ„å»ºæç¤ºæ¨¡æ¿"""
        context_str = "\n".join(context)
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context_str}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
        return prompt
    
    def query(self, question: str, k: int = 5) -> str:
        """å®Œæ•´çš„RAGæµç¨‹"""
        # 1. æ£€ç´¢
        relevant_docs = self.retrieve(question, k)
        
        # 2. ç”Ÿæˆ
        answer = self.generate(question, relevant_docs)
        
        return answer

# ä½¿ç”¨ç¤ºä¾‹
rag = RAGPipeline()
documents = [
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚",
    "Transformeræ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„ã€‚"
]
rag.build_index(documents)
answer = rag.query("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
print(answer)
```

## 2. æ–‡æ¡£å¤„ç†ä¸åˆ‡åˆ† ğŸ“„

```python
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    TokenTextSplitter,
    SentenceTransformersTokenTextSplitter
)
import tiktoken

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å’Œåˆ‡åˆ†"""
    
    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_by_characters(self, text: str) -> List[str]:
        """æŒ‰å­—ç¬¦åˆ‡åˆ†"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", " ", ""]
        )
        return splitter.split_text(text)
    
    def split_by_tokens(self, text: str, model="gpt-3.5-turbo") -> List[str]:
        """æŒ‰tokenåˆ‡åˆ†"""
        encoding = tiktoken.encoding_for_model(model)
        
        splitter = TokenTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            encoding_name=encoding.name
        )
        return splitter.split_text(text)
    
    def split_by_sentences(self, text: str) -> List[str]:
        """æŒ‰å¥å­åˆ‡åˆ†"""
        splitter = SentenceTransformersTokenTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=0
        )
        return splitter.split_text(text)
    
    def sliding_window(self, text: str, window_size: int, stride: int) -> List[str]:
        """æ»‘åŠ¨çª—å£åˆ‡åˆ†"""
        chunks = []
        sentences = text.split('ã€‚')
        
        for i in range(0, len(sentences) - window_size + 1, stride):
            chunk = 'ã€‚'.join(sentences[i:i + window_size])
            chunks.append(chunk)
        
        return chunks
    
    def hierarchical_split(self, text: str) -> Dict[str, List[str]]:
        """å±‚æ¬¡åŒ–åˆ‡åˆ†"""
        # å¤§å—
        large_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
        large_chunks = large_splitter.split_text(text)
        
        # å°å—
        small_splitter = RecursiveCharacterTextSplitter(
            chunk_size=200,
            chunk_overlap=20
        )
        
        result = {
            "large": large_chunks,
            "small": []
        }
        
        for chunk in large_chunks:
            small_chunks = small_splitter.split_text(chunk)
            result["small"].extend(small_chunks)
        
        return result

# æ™ºèƒ½åˆ‡åˆ†ç­–ç•¥
class SmartChunker:
    """æ™ºèƒ½æ–‡æ¡£åˆ‡åˆ†"""
    
    def __init__(self):
        self.processor = DocumentProcessor()
    
    def chunk_by_document_type(self, text: str, doc_type: str) -> List[str]:
        """æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆ‡åˆ†ç­–ç•¥"""
        
        strategies = {
            "code": {"size": 300, "overlap": 50},
            "paper": {"size": 500, "overlap": 100},
            "dialogue": {"size": 200, "overlap": 20},
            "narrative": {"size": 800, "overlap": 150}
        }
        
        if doc_type in strategies:
            config = strategies[doc_type]
            self.processor.chunk_size = config["size"]
            self.processor.chunk_overlap = config["overlap"]
        
        return self.processor.split_by_characters(text)
    
    def semantic_chunking(self, text: str, threshold: float = 0.7) -> List[str]:
        """è¯­ä¹‰åˆ‡åˆ†ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦"""
        from sentence_transformers import SentenceTransformer
        
        model = SentenceTransformer('all-MiniLM-L6-v2')
        sentences = text.split('ã€‚')
        
        # è®¡ç®—å¥å­åµŒå…¥
        embeddings = model.encode(sentences)
        
        chunks = []
        current_chunk = [sentences[0]]
        
        for i in range(1, len(sentences)):
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = np.dot(embeddings[i-1], embeddings[i]) / (
                np.linalg.norm(embeddings[i-1]) * np.linalg.norm(embeddings[i])
            )
            
            if similarity < threshold:
                # è¯­ä¹‰è½¬æŠ˜ï¼Œåˆ›å»ºæ–°å—
                chunks.append('ã€‚'.join(current_chunk))
                current_chunk = [sentences[i]]
            else:
                current_chunk.append(sentences[i])
        
        if current_chunk:
            chunks.append('ã€‚'.join(current_chunk))
        
        return chunks
```

## 3. å‘é‡æ•°æ®åº“å®æˆ˜ ğŸ—„ï¸

```python
# Chromaå‘é‡æ•°æ®åº“
import chromadb
from chromadb.utils import embedding_functions

class ChromaVectorStore:
    """Chromaå‘é‡æ•°æ®åº“å°è£…"""
    
    def __init__(self, collection_name="documents"):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        
        # ä½¿ç”¨OpenAIåµŒå…¥
        self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
            api_key="your-api-key",
            model_name="text-embedding-ada-002"
        )
        
        self.collection = self.client.create_collection(
            name=collection_name,
            embedding_function=self.embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    def add_documents(self, documents: List[str], metadatas: List[Dict] = None):
        """æ·»åŠ æ–‡æ¡£"""
        ids = [f"doc_{i}" for i in range(len(documents))]
        
        self.collection.add(
            documents=documents,
            metadatas=metadatas or [{}] * len(documents),
            ids=ids
        )
    
    def search(self, query: str, k: int = 5) -> Dict:
        """æœç´¢æ–‡æ¡£"""
        results = self.collection.query(
            query_texts=[query],
            n_results=k,
            include=["documents", "metadatas", "distances"]
        )
        return results
    
    def hybrid_search(self, query: str, k: int = 5) -> List[str]:
        """æ··åˆæœç´¢ï¼šå‘é‡ + å…³é”®è¯"""
        # å‘é‡æœç´¢
        vector_results = self.search(query, k * 2)
        
        # å…³é”®è¯æœç´¢ï¼ˆç®€åŒ–ç‰ˆï¼‰
        keyword_results = self.collection.query(
            where_document={"$contains": query.split()[0]},
            n_results=k
        )
        
        # åˆå¹¶ç»“æœ
        all_docs = set()
        for docs in vector_results["documents"]:
            all_docs.update(docs)
        for docs in keyword_results["documents"]:
            all_docs.update(docs)
        
        return list(all_docs)[:k]

# Pineconeå‘é‡æ•°æ®åº“
import pinecone

class PineconeVectorStore:
    """Pineconeå‘é‡æ•°æ®åº“"""
    
    def __init__(self, index_name="rag-index"):
        pinecone.init(
            api_key="your-api-key",
            environment="us-west1-gcp"
        )
        
        # åˆ›å»ºç´¢å¼•
        if index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=index_name,
                dimension=768,
                metric="cosine"
            )
        
        self.index = pinecone.Index(index_name)
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def upsert(self, documents: List[str], ids: List[str] = None):
        """æ’å…¥æˆ–æ›´æ–°æ–‡æ¡£"""
        embeddings = self.encoder.encode(documents)
        
        if ids is None:
            ids = [str(i) for i in range(len(documents))]
        
        vectors = [
            (ids[i], embeddings[i].tolist(), {"text": documents[i]})
            for i in range(len(documents))
        ]
        
        self.index.upsert(vectors)
    
    def query(self, query_text: str, top_k: int = 5) -> List[Dict]:
        """æŸ¥è¯¢ç›¸ä¼¼æ–‡æ¡£"""
        query_embedding = self.encoder.encode([query_text])[0]
        
        results = self.index.query(
            vector=query_embedding.tolist(),
            top_k=top_k,
            include_metadata=True
        )
        
        return results.matches

# Weaviateå‘é‡æ•°æ®åº“
import weaviate

class WeaviateVectorStore:
    """Weaviateå‘é‡æ•°æ®åº“"""
    
    def __init__(self):
        self.client = weaviate.Client("http://localhost:8080")
        self._create_schema()
    
    def _create_schema(self):
        """åˆ›å»ºschema"""
        schema = {
            "class": "Document",
            "vectorizer": "text2vec-transformers",
            "properties": [
                {
                    "name": "content",
                    "dataType": ["text"]
                },
                {
                    "name": "source",
                    "dataType": ["string"]
                }
            ]
        }
        
        try:
            self.client.schema.create_class(schema)
        except:
            pass  # Schema already exists
    
    def add_document(self, content: str, source: str = ""):
        """æ·»åŠ æ–‡æ¡£"""
        self.client.data_object.create(
            {
                "content": content,
                "source": source
            },
            "Document"
        )
    
    def search(self, query: str, limit: int = 5) -> List[Dict]:
        """å‘é‡æœç´¢"""
        result = (
            self.client.query
            .get("Document", ["content", "source"])
            .with_near_text({"concepts": [query]})
            .with_limit(limit)
            .do()
        )
        
        return result["data"]["Get"]["Document"]
```

## 4. æ£€ç´¢ä¼˜åŒ–æŠ€æœ¯ ğŸš€

```python
class RetrievalOptimizer:
    """æ£€ç´¢ä¼˜åŒ–æŠ€æœ¯"""
    
    def __init__(self):
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[str]:
        """é‡æ’åºï¼šä½¿ç”¨äº¤å‰ç¼–ç å™¨"""
        from sentence_transformers import CrossEncoder
        
        # äº¤å‰ç¼–ç å™¨è¯„åˆ†
        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        
        pairs = [[query, doc] for doc in documents]
        scores = reranker.predict(pairs)
        
        # æ’åºå¹¶è¿”å›
        sorted_indices = np.argsort(scores)[::-1][:top_k]
        return [documents[i] for i in sorted_indices]
    
    def mmr_selection(self, query: str, documents: List[str], 
                     lambda_param: float = 0.5, k: int = 5) -> List[str]:
        """æœ€å¤§è¾¹é™…ç›¸å…³æ€§(MMR)é€‰æ‹©"""
        # ç¼–ç 
        query_embedding = self.encoder.encode([query])[0]
        doc_embeddings = self.encoder.encode(documents)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(doc_embeddings, query_embedding)
        
        selected = []
        selected_indices = []
        
        for _ in range(min(k, len(documents))):
            if not selected_indices:
                # é€‰æ‹©æœ€ç›¸ä¼¼çš„
                idx = np.argmax(similarities)
            else:
                # MMRåˆ†æ•°
                mmr_scores = []
                for i, doc_emb in enumerate(doc_embeddings):
                    if i in selected_indices:
                        mmr_scores.append(-np.inf)
                        continue
                    
                    # ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
                    query_sim = similarities[i]
                    
                    # ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦
                    selected_embs = doc_embeddings[selected_indices]
                    max_doc_sim = np.max(np.dot(selected_embs, doc_emb))
                    
                    # MMRåˆ†æ•°
                    mmr = lambda_param * query_sim - (1 - lambda_param) * max_doc_sim
                    mmr_scores.append(mmr)
                
                idx = np.argmax(mmr_scores)
            
            selected.append(documents[idx])
            selected_indices.append(idx)
        
        return selected
    
    def query_expansion(self, query: str) -> List[str]:
        """æŸ¥è¯¢æ‰©å±•"""
        expanded_queries = [query]
        
        # 1. åŒä¹‰è¯æ‰©å±•
        from nltk.corpus import wordnet
        words = query.split()
        for word in words:
            synsets = wordnet.synsets(word)
            if synsets:
                synonyms = set()
                for syn in synsets[:2]:  # å–å‰ä¸¤ä¸ªå«ä¹‰
                    for lemma in syn.lemmas()[:3]:  # æ¯ä¸ªå«ä¹‰å–3ä¸ªåŒä¹‰è¯
                        synonyms.add(lemma.name())
                
                for synonym in synonyms:
                    expanded = query.replace(word, synonym)
                    if expanded != query:
                        expanded_queries.append(expanded)
        
        # 2. å›è¯‘æ‰©å±•
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦è°ƒç”¨ç¿»è¯‘API
        
        return expanded_queries[:5]  # é™åˆ¶æ•°é‡
    
    def hypothetical_document_embedding(self, query: str, llm_model) -> np.ndarray:
        """HyDEï¼šç”Ÿæˆå‡è®¾æ–‡æ¡£è¿›è¡Œæ£€ç´¢"""
        # è®©LLMç”Ÿæˆå‡è®¾ç­”æ¡ˆ
        prompt = f"""è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ç­”æ¡ˆï¼š
        
        é—®é¢˜ï¼š{query}
        
        ç­”æ¡ˆï¼š"""
        
        hypothetical_answer = llm_model.generate(prompt)
        
        # ç¼–ç å‡è®¾ç­”æ¡ˆ
        embedding = self.encoder.encode([hypothetical_answer])[0]
        
        return embedding

# å¤šè·¯æ£€ç´¢
class MultiPathRetrieval:
    """å¤šè·¯å¾„æ£€ç´¢ç­–ç•¥"""
    
    def __init__(self):
        self.dense_retriever = SentenceTransformer('all-MiniLM-L6-v2')
        self.sparse_retriever = None  # BM25ç­‰
    
    def ensemble_retrieval(self, query: str, documents: List[str], 
                          alpha: float = 0.5) -> List[str]:
        """é›†æˆæ£€ç´¢ï¼šå¯†é›†+ç¨€ç–"""
        # å¯†é›†æ£€ç´¢
        dense_scores = self._dense_search(query, documents)
        
        # ç¨€ç–æ£€ç´¢(BM25)
        sparse_scores = self._bm25_search(query, documents)
        
        # åˆ†æ•°èåˆ
        final_scores = alpha * dense_scores + (1 - alpha) * sparse_scores
        
        # æ’åºè¿”å›
        sorted_indices = np.argsort(final_scores)[::-1]
        return [documents[i] for i in sorted_indices]
    
    def _dense_search(self, query: str, documents: List[str]) -> np.ndarray:
        """å¯†é›†å‘é‡æœç´¢"""
        query_emb = self.dense_retriever.encode([query])[0]
        doc_embs = self.dense_retriever.encode(documents)
        
        scores = np.dot(doc_embs, query_emb)
        return scores
    
    def _bm25_search(self, query: str, documents: List[str]) -> np.ndarray:
        """BM25ç¨€ç–æ£€ç´¢"""
        from rank_bm25 import BM25Okapi
        
        tokenized_docs = [doc.split() for doc in documents]
        bm25 = BM25Okapi(tokenized_docs)
        
        tokenized_query = query.split()
        scores = bm25.get_scores(tokenized_query)
        
        return scores
```

## 5. æç¤ºå·¥ç¨‹ä¸ä¸Šä¸‹æ–‡ç®¡ç† ğŸ“

```python
class PromptManager:
    """æç¤ºå·¥ç¨‹ç®¡ç†"""
    
    def __init__(self):
        self.templates = {
            "qa": """æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š""",
            
            "summary": """è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£çš„ä¸»è¦å†…å®¹ï¼š

{context}

æ€»ç»“ï¼š""",
            
            "extraction": """ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–{entity_type}ï¼š

æ–‡æœ¬ï¼š{context}

æå–ç»“æœï¼š""",
            
            "comparison": """æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªè§‚ç‚¹ï¼š

è§‚ç‚¹1ï¼š{context1}

è§‚ç‚¹2ï¼š{context2}

æ¯”è¾ƒåˆ†æï¼š"""
        }
    
    def format_prompt(self, template_name: str, **kwargs) -> str:
        """æ ¼å¼åŒ–æç¤º"""
        template = self.templates.get(template_name, self.templates["qa"])
        return template.format(**kwargs)
    
    def create_few_shot_prompt(self, examples: List[Dict], query: str) -> str:
        """å°‘æ ·æœ¬æç¤º"""
        prompt = "ä»¥ä¸‹æ˜¯ä¸€äº›é—®ç­”ç¤ºä¾‹ï¼š\n\n"
        
        for example in examples:
            prompt += f"é—®é¢˜ï¼š{example['question']}\n"
            prompt += f"ç­”æ¡ˆï¼š{example['answer']}\n\n"
        
        prompt += f"ç°åœ¨è¯·å›ç­”ï¼š\né—®é¢˜ï¼š{query}\nç­”æ¡ˆï¼š"
        
        return prompt
    
    def chain_of_thought_prompt(self, question: str, context: str) -> str:
        """æ€ç»´é“¾æç¤º"""
        return f"""è¯·ä¸€æ­¥ä¸€æ­¥æ€è€ƒå¹¶å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š
1. é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£é—®é¢˜åœ¨é—®ä»€ä¹ˆ
2. ç„¶åï¼Œæˆ‘éœ€è¦åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
3. æœ€åï¼ŒåŸºäºæ‰¾åˆ°çš„ä¿¡æ¯å½¢æˆç­”æ¡ˆ

æ€è€ƒè¿‡ç¨‹ï¼š"""

class ContextManager:
    """ä¸Šä¸‹æ–‡ç®¡ç†"""
    
    def __init__(self, max_context_length: int = 2048):
        self.max_context_length = max_context_length
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def compress_context(self, documents: List[str]) -> str:
        """å‹ç¼©ä¸Šä¸‹æ–‡"""
        # è®¡ç®—tokenæ•°
        full_context = "\n".join(documents)
        tokens = self.tokenizer.encode(full_context)
        
        if len(tokens) <= self.max_context_length:
            return full_context
        
        # å‹ç¼©ç­–ç•¥
        compressed_docs = []
        current_tokens = 0
        
        for doc in documents:
            doc_tokens = self.tokenizer.encode(doc)
            if current_tokens + len(doc_tokens) > self.max_context_length:
                # æˆªæ–­æ–‡æ¡£
                remaining = self.max_context_length - current_tokens
                truncated = self.tokenizer.decode(doc_tokens[:remaining])
                compressed_docs.append(truncated + "...")
                break
            else:
                compressed_docs.append(doc)
                current_tokens += len(doc_tokens)
        
        return "\n".join(compressed_docs)
    
    def extract_key_sentences(self, text: str, num_sentences: int = 5) -> str:
        """æå–å…³é”®å¥å­"""
        from sumy.parsers.plaintext import PlaintextParser
        from sumy.nlp.tokenizers import Tokenizer
        from sumy.summarizers.text_rank import TextRankSummarizer
        
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summarizer = TextRankSummarizer()
        
        summary = summarizer(parser.document, num_sentences)
        
        return " ".join([str(sentence) for sentence in summary])
```

## 6. é«˜çº§RAGæŠ€æœ¯ ğŸ“

```python
class AdvancedRAG:
    """é«˜çº§RAGæŠ€æœ¯"""
    
    def __init__(self):
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def iterative_retrieval(self, query: str, documents: List[str], 
                           max_iterations: int = 3) -> List[str]:
        """è¿­ä»£æ£€ç´¢"""
        retrieved_docs = []
        current_query = query
        
        for iteration in range(max_iterations):
            # æ£€ç´¢
            new_docs = self._retrieve(current_query, documents, k=3)
            retrieved_docs.extend(new_docs)
            
            # åŸºäºå·²æ£€ç´¢æ–‡æ¡£ç”Ÿæˆæ–°æŸ¥è¯¢
            current_query = self._generate_followup_query(query, retrieved_docs)
            
            # å»é‡
            documents = [d for d in documents if d not in retrieved_docs]
            
            if not documents:
                break
        
        return retrieved_docs
    
    def _generate_followup_query(self, original_query: str, 
                                 retrieved_docs: List[str]) -> str:
        """ç”Ÿæˆåç»­æŸ¥è¯¢"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦LLM
        # æå–å…³é”®è¯å¹¶æ‰©å±•æŸ¥è¯¢
        keywords = set()
        for doc in retrieved_docs:
            # ç®€å•æå–åè¯
            words = doc.split()
            keywords.update(words[:5])  # å–å‰5ä¸ªè¯
        
        expanded_query = original_query + " " + " ".join(keywords)
        return expanded_query
    
    def graph_based_retrieval(self, query: str, 
                            knowledge_graph: Dict) -> List[str]:
        """åŸºäºå›¾çš„æ£€ç´¢"""
        import networkx as nx
        
        # æ„å»ºå›¾
        G = nx.Graph()
        for entity, relations in knowledge_graph.items():
            for relation, target in relations:
                G.add_edge(entity, target, relation=relation)
        
        # æ‰¾åˆ°æŸ¥è¯¢ä¸­çš„å®ä½“
        query_entities = self._extract_entities(query)
        
        # å›¾éå†è·å–ç›¸å…³èŠ‚ç‚¹
        relevant_nodes = set()
        for entity in query_entities:
            if entity in G:
                # è·å–é‚»å±…èŠ‚ç‚¹
                neighbors = list(G.neighbors(entity))
                relevant_nodes.update(neighbors[:5])
                
                # è·å–2è·³é‚»å±…
                for neighbor in neighbors[:3]:
                    second_neighbors = list(G.neighbors(neighbor))
                    relevant_nodes.update(second_neighbors[:2])
        
        # æ„å»ºæ–‡æ¡£
        documents = []
        for node in relevant_nodes:
            # è·å–èŠ‚ç‚¹ç›¸å…³çš„æ–‡æœ¬
            doc = self._get_node_text(node, G)
            documents.append(doc)
        
        return documents
    
    def _extract_entities(self, text: str) -> List[str]:
        """å®ä½“æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…åº”ä½¿ç”¨NERæ¨¡å‹
        import spacy
        nlp = spacy.load("en_core_web_sm")
        doc = nlp(text)
        entities = [ent.text for ent in doc.ents]
        return entities
    
    def _get_node_text(self, node: str, graph) -> str:
        """è·å–èŠ‚ç‚¹æ–‡æœ¬"""
        # æ„å»ºèŠ‚ç‚¹æè¿°
        edges = graph.edges(node, data=True)
        text = f"{node}: "
        relations = []
        for _, target, data in edges:
            relation = data.get('relation', 'related to')
            relations.append(f"{relation} {target}")
        text += ", ".join(relations)
        return text

# è‡ªé€‚åº”RAG
class AdaptiveRAG:
    """è‡ªé€‚åº”RAGç³»ç»Ÿ"""
    
    def __init__(self):
        self.query_classifier = self._init_classifier()
        self.strategies = {
            "factual": self._factual_strategy,
            "analytical": self._analytical_strategy,
            "creative": self._creative_strategy
        }
    
    def _init_classifier(self):
        """åˆå§‹åŒ–æŸ¥è¯¢åˆ†ç±»å™¨"""
        # ç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦è®­ç»ƒåˆ†ç±»å™¨
        return lambda x: "factual"  # é»˜è®¤è¿”å›factual
    
    def query(self, question: str) -> str:
        """è‡ªé€‚åº”æŸ¥è¯¢"""
        # åˆ†ç±»æŸ¥è¯¢ç±»å‹
        query_type = self.query_classifier(question)
        
        # é€‰æ‹©ç­–ç•¥
        strategy = self.strategies.get(query_type, self._factual_strategy)
        
        # æ‰§è¡Œç­–ç•¥
        return strategy(question)
    
    def _factual_strategy(self, question: str) -> str:
        """äº‹å®å‹é—®é¢˜ç­–ç•¥"""
        # å¯†é›†æ£€ç´¢ + ç²¾ç¡®åŒ¹é…
        # è¾ƒå°‘çš„ä¸Šä¸‹æ–‡
        # æ¸©åº¦å‚æ•°ä½
        return "Factual answer"
    
    def _analytical_strategy(self, question: str) -> str:
        """åˆ†æå‹é—®é¢˜ç­–ç•¥"""
        # å¤šè·¯æ£€ç´¢
        # æ›´å¤šä¸Šä¸‹æ–‡
        # æ€ç»´é“¾æç¤º
        return "Analytical answer"
    
    def _creative_strategy(self, question: str) -> str:
        """åˆ›é€ å‹é—®é¢˜ç­–ç•¥"""
        # å°‘é‡æ£€ç´¢
        # æ›´é«˜æ¸©åº¦
        # æ›´å¤šç”Ÿæˆ
        return "Creative answer"
```

## 7. è¯„ä¼°ä¸ç›‘æ§ ğŸ“Š

```python
class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°"""
    
    def __init__(self):
        self.metrics = {}
    
    def evaluate_retrieval(self, queries: List[str], 
                          ground_truth: List[List[str]], 
                          retrieved: List[List[str]]) -> Dict:
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        from sklearn.metrics import precision_recall_fscore_support
        
        metrics = {
            "precision": [],
            "recall": [],
            "f1": [],
            "mrr": []  # Mean Reciprocal Rank
        }
        
        for i, (gt, ret) in enumerate(zip(ground_truth, retrieved)):
            # Precision & Recall
            gt_set = set(gt)
            ret_set = set(ret)
            
            if ret_set:
                precision = len(gt_set & ret_set) / len(ret_set)
                metrics["precision"].append(precision)
            
            if gt_set:
                recall = len(gt_set & ret_set) / len(gt_set)
                metrics["recall"].append(recall)
            
            # F1
            if precision + recall > 0:
                f1 = 2 * precision * recall / (precision + recall)
                metrics["f1"].append(f1)
            
            # MRR
            for rank, doc in enumerate(ret, 1):
                if doc in gt_set:
                    metrics["mrr"].append(1 / rank)
                    break
            else:
                metrics["mrr"].append(0)
        
        # è®¡ç®—å¹³å‡å€¼
        return {
            metric: np.mean(values) 
            for metric, values in metrics.items()
        }
    
    def evaluate_generation(self, questions: List[str], 
                          generated_answers: List[str], 
                          reference_answers: List[str]) -> Dict:
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        from rouge import Rouge
        from bert_score import score
        
        rouge = Rouge()
        
        # ROUGEåˆ†æ•°
        rouge_scores = rouge.get_scores(generated_answers, reference_answers, avg=True)
        
        # BERT Score
        P, R, F1 = score(generated_answers, reference_answers, lang="en")
        
        # BLEUåˆ†æ•°
        from nltk.translate.bleu_score import sentence_bleu
        bleu_scores = []
        for gen, ref in zip(generated_answers, reference_answers):
            score = sentence_bleu([ref.split()], gen.split())
            bleu_scores.append(score)
        
        return {
            "rouge-1": rouge_scores["rouge-1"]["f"],
            "rouge-2": rouge_scores["rouge-2"]["f"],
            "rouge-l": rouge_scores["rouge-l"]["f"],
            "bert_score": F1.mean().item(),
            "bleu": np.mean(bleu_scores)
        }
    
    def faithfulness_score(self, answer: str, context: str) -> float:
        """å¿ å®åº¦è¯„åˆ†ï¼šç­”æ¡ˆæ˜¯å¦å¿ äºä¸Šä¸‹æ–‡"""
        # ä½¿ç”¨NLIæ¨¡å‹åˆ¤æ–­
        from transformers import pipeline
        
        nli = pipeline("text-classification", 
                      model="roberta-large-mnli")
        
        result = nli(f"{context} {answer}")
        
        # è¿”å›entailmentåˆ†æ•°
        for item in result:
            if item['label'] == 'ENTAILMENT':
                return item['score']
        return 0.0
    
    def relevance_score(self, answer: str, question: str) -> float:
        """ç›¸å…³æ€§è¯„åˆ†"""
        encoder = SentenceTransformer('all-MiniLM-L6-v2')
        
        q_emb = encoder.encode([question])[0]
        a_emb = encoder.encode([answer])[0]
        
        similarity = np.dot(q_emb, a_emb) / (
            np.linalg.norm(q_emb) * np.linalg.norm(a_emb)
        )
        
        return similarity

# ç›‘æ§ç³»ç»Ÿ
class RAGMonitor:
    """RAGç³»ç»Ÿç›‘æ§"""
    
    def __init__(self):
        self.metrics_history = []
    
    def log_query(self, query_data: Dict):
        """è®°å½•æŸ¥è¯¢"""
        import time
        
        query_data['timestamp'] = time.time()
        self.metrics_history.append(query_data)
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.metrics_history:
            return {}
        
        latencies = [m['latency'] for m in self.metrics_history]
        retrieval_counts = [m['num_retrieved'] for m in self.metrics_history]
        
        return {
            "avg_latency": np.mean(latencies),
            "p95_latency": np.percentile(latencies, 95),
            "avg_docs_retrieved": np.mean(retrieval_counts),
            "total_queries": len(self.metrics_history)
        }
    
    def detect_anomalies(self) -> List[Dict]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        
        for metric in self.metrics_history[-100:]:  # æœ€è¿‘100ä¸ªæŸ¥è¯¢
            # é«˜å»¶è¿Ÿ
            if metric['latency'] > 5.0:
                anomalies.append({
                    "type": "high_latency",
                    "query": metric['query'],
                    "latency": metric['latency']
                })
            
            # æ— ç»“æœ
            if metric['num_retrieved'] == 0:
                anomalies.append({
                    "type": "no_results",
                    "query": metric['query']
                })
        
        return anomalies
```

## 8. ç”Ÿäº§éƒ¨ç½² ğŸš¢

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import asyncio
from typing import Optional

app = FastAPI()

class QueryRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5
    temperature: Optional[float] = 0.7

class QueryResponse(BaseModel):
    answer: str
    sources: List[str]
    confidence: float

class ProductionRAG:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""
    
    def __init__(self):
        self.rag_pipeline = RAGPipeline()
        self.cache = {}
        self.monitor = RAGMonitor()
    
    async def process_query(self, request: QueryRequest) -> QueryResponse:
        """å¤„ç†æŸ¥è¯¢"""
        import time
        start_time = time.time()
        
        # ç¼“å­˜æ£€æŸ¥
        cache_key = f"{request.question}_{request.top_k}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            # æ£€ç´¢
            sources = await self._async_retrieve(
                request.question, 
                request.top_k
            )
            
            # ç”Ÿæˆ
            answer = await self._async_generate(
                request.question,
                sources,
                request.temperature
            )
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(answer, sources)
            
            response = QueryResponse(
                answer=answer,
                sources=sources,
                confidence=confidence
            )
            
            # ç¼“å­˜ç»“æœ
            self.cache[cache_key] = response
            
            # è®°å½•æŒ‡æ ‡
            self.monitor.log_query({
                "query": request.question,
                "latency": time.time() - start_time,
                "num_retrieved": len(sources)
            })
            
            return response
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    async def _async_retrieve(self, query: str, k: int) -> List[str]:
        """å¼‚æ­¥æ£€ç´¢"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, 
            self.rag_pipeline.retrieve, 
            query, 
            k
        )
    
    async def _async_generate(self, query: str, 
                             context: List[str], 
                             temperature: float) -> str:
        """å¼‚æ­¥ç”Ÿæˆ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self.rag_pipeline.generate,
            query,
            context
        )
    
    def _calculate_confidence(self, answer: str, sources: List[str]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        # ç®€åŒ–ç‰ˆï¼šåŸºäºç­”æ¡ˆé•¿åº¦å’Œæºæ–‡æ¡£æ•°é‡
        if not answer or not sources:
            return 0.0
        
        length_score = min(len(answer) / 500, 1.0)
        source_score = min(len(sources) / 5, 1.0)
        
        return (length_score + source_score) / 2

# åˆå§‹åŒ–ç³»ç»Ÿ
rag_system = ProductionRAG()

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """æŸ¥è¯¢ç«¯ç‚¹"""
    return await rag_system.process_query(request)

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

@app.get("/metrics")
async def metrics():
    """è·å–æŒ‡æ ‡"""
    return rag_system.monitor.get_statistics()

# Dockeréƒ¨ç½²
dockerfile = """
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
"""

# Kuberneteséƒ¨ç½²
k8s_deployment = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rag
  template:
    metadata:
      labels:
        app: rag
    spec:
      containers:
      - name: rag-container
        image: rag-system:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
---
apiVersion: v1
kind: Service
metadata:
  name: rag-service
spec:
  selector:
    app: rag
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
"""
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“‹

```python
def rag_best_practices():
    """RAGæœ€ä½³å®è·µ"""
    
    practices = {
        "æ–‡æ¡£å¤„ç†": [
            "é€‰æ‹©åˆé€‚çš„chunkå¤§å°ï¼ˆé€šå¸¸200-800 tokensï¼‰",
            "ä¿ç•™chunkä¹‹é—´çš„é‡å ï¼ˆ10-20%ï¼‰",
            "ä¿å­˜å…ƒæ•°æ®ï¼ˆæ¥æºã€æ—¥æœŸã€ç« èŠ‚ç­‰ï¼‰",
            "è€ƒè™‘æ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ï¼‰"
        ],
        
        "æ£€ç´¢ä¼˜åŒ–": [
            "ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆå¯†é›†+ç¨€ç–ï¼‰",
            "å®æ–½é‡æ’åºæœºåˆ¶",
            "åº”ç”¨MMRå‡å°‘å†—ä½™",
            "ç¼“å­˜å¸¸è§æŸ¥è¯¢ç»“æœ"
        ],
        
        "ç”Ÿæˆè´¨é‡": [
            "è®¾è®¡æ¸…æ™°çš„æç¤ºæ¨¡æ¿",
            "å®æ–½ç­”æ¡ˆéªŒè¯æœºåˆ¶",
            "æ§åˆ¶ç”Ÿæˆé•¿åº¦å’Œæ¸©åº¦",
            "æ·»åŠ å¼•ç”¨æ¥æº"
        ],
        
        "ç³»ç»Ÿè®¾è®¡": [
            "å¼‚æ­¥å¤„ç†æé«˜ååé‡",
            "å®æ–½ç†”æ–­æœºåˆ¶",
            "ç›‘æ§å…³é”®æŒ‡æ ‡",
            "å®šæœŸæ›´æ–°çŸ¥è¯†åº“"
        ],
        
        "å®‰å…¨è€ƒè™‘": [
            "è¾“å…¥éªŒè¯å’Œæ¸…æ´—",
            "é˜²æ­¢æç¤ºæ³¨å…¥",
            "é™åˆ¶APIè°ƒç”¨é¢‘ç‡",
            "æ•æ„Ÿä¿¡æ¯è¿‡æ»¤"
        ]
    }
    
    return practices

# å¸¸è§é—®é¢˜è§£å†³
troubleshooting = """
é—®é¢˜1ï¼šæ£€ç´¢ç»“æœä¸ç›¸å…³
- æ£€æŸ¥embeddingæ¨¡å‹æ˜¯å¦é€‚åˆé¢†åŸŸ
- è°ƒæ•´chunkå¤§å°
- å°è¯•æŸ¥è¯¢æ‰©å±•
- ä½¿ç”¨é‡æ’åº

é—®é¢˜2ï¼šç”Ÿæˆå¹»è§‰
- é™ä½temperature
- å¢åŠ ä¸Šä¸‹æ–‡éªŒè¯
- ä½¿ç”¨æ›´ä¸¥æ ¼çš„æç¤º
- å®æ–½äº‹å®æ£€æŸ¥

é—®é¢˜3ï¼šå»¶è¿Ÿè¿‡é«˜
- ä½¿ç”¨ç¼“å­˜
- å¼‚æ­¥å¤„ç†
- ä¼˜åŒ–å‘é‡ç´¢å¼•
- å‡å°‘æ£€ç´¢æ•°é‡

é—®é¢˜4ï¼šæˆæœ¬è¿‡é«˜
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- å®æ–½æ™ºèƒ½ç¼“å­˜
- æ‰¹å¤„ç†è¯·æ±‚
- ä¼˜åŒ–tokenä½¿ç”¨
"""

print("RAGç³»ç»Ÿå®æˆ˜æŒ‡å—å®Œæˆï¼")
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [æ¨¡å‹å¾®è°ƒ](finetuning.md) - å®šåˆ¶åŒ–ä½ çš„æ¨¡å‹
- [NLPæ¨¡å‹](nlp_models.md) - æ·±å…¥ç†è§£è¯­è¨€æ¨¡å‹
- [LLMéƒ¨ç½²](llm_deployment.md) - å¤§æ¨¡å‹ç”Ÿäº§éƒ¨ç½²