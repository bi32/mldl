# å¤§è¯­è¨€æ¨¡å‹(LLM)éƒ¨ç½²å®æˆ˜ ğŸ¤–

ä»Transformersåˆ°vLLMã€Ollamaï¼ŒæŒæ¡å¤§æ¨¡å‹é«˜æ•ˆéƒ¨ç½²çš„æ‰€æœ‰æŠ€æœ¯æ ˆã€‚

## 1. Transformersåº“åŸºç¡€ ğŸ“š

```python
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    pipeline,
    BitsAndBytesConfig
)
import torch

# åŸºç¡€ä½¿ç”¨
def basic_transformers_usage():
    """Transformersåº“åŸºç¡€ä½¿ç”¨"""
    
    # 1. ä½¿ç”¨pipelineï¼ˆæœ€ç®€å•ï¼‰
    generator = pipeline('text-generation', model='gpt2')
    result = generator("Hello, I'm a language model", 
                      max_length=50, 
                      num_return_sequences=2)
    print(result)
    
    # 2. æ‰‹åŠ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "microsoft/DialoGPT-medium"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # å¯¹è¯ç¤ºä¾‹
    def chat(input_text, chat_history_ids=None):
        # ç¼–ç è¾“å…¥
        new_input_ids = tokenizer.encode(
            input_text + tokenizer.eos_token, 
            return_tensors='pt'
        )
        
        # æ‹¼æ¥å†å²
        if chat_history_ids is not None:
            bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)
        else:
            bot_input_ids = new_input_ids
        
        # ç”Ÿæˆå›å¤
        chat_history_ids = model.generate(
            bot_input_ids,
            max_length=1000,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            top_p=0.95,
            temperature=0.8
        )
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(
            chat_history_ids[:, bot_input_ids.shape[-1]:][0], 
            skip_special_tokens=True
        )
        
        return response, chat_history_ids
    
    # æµ‹è¯•å¯¹è¯
    response, history = chat("How are you?")
    print(f"Bot: {response}")

# åŠ è½½å¤§æ¨¡å‹ï¼ˆé‡åŒ–ï¼‰
def load_large_model_quantized():
    """åŠ è½½é‡åŒ–çš„å¤§æ¨¡å‹"""
    
    # 4bité‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-2-7b-chat-hf",
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        "meta-llama/Llama-2-7b-chat-hf"
    )
    
    return model, tokenizer

# æµå¼ç”Ÿæˆ
class StreamingLLM:
    def __init__(self, model_name="gpt2"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
    def generate_stream(self, prompt, max_new_tokens=100):
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            for _ in range(max_new_tokens):
                outputs = self.model(**inputs)
                next_token_logits = outputs.logits[0, -1, :]
                next_token_id = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)
                
                # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                next_token = self.tokenizer.decode(next_token_id[0])
                yield next_token
                
                # æ›´æ–°è¾“å…¥
                inputs["input_ids"] = torch.cat([inputs["input_ids"], next_token_id], dim=1)
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if next_token_id.item() == self.tokenizer.eos_token_id:
                    break

# ä½¿ç”¨ç¤ºä¾‹
streamer = StreamingLLM()
for token in streamer.generate_stream("Once upon a time"):
    print(token, end='', flush=True)
```

## 2. vLLM - é«˜æ€§èƒ½æ¨ç†å¼•æ“ âš¡

```python
# å®‰è£…: pip install vllm

from vllm import LLM, SamplingParams

class vLLMDeployment:
    """vLLMéƒ¨ç½²ç±»"""
    
    def __init__(self, model_name="facebook/opt-125m", gpu_memory_utilization=0.9):
        """
        åˆå§‹åŒ–vLLM
        gpu_memory_utilization: GPUå†…å­˜ä½¿ç”¨ç‡
        """
        self.llm = LLM(
            model=model_name,
            gpu_memory_utilization=gpu_memory_utilization,
            tensor_parallel_size=1,  # å¼ é‡å¹¶è¡Œ
            dtype="half",  # ä½¿ç”¨FP16
            max_model_len=2048
        )
    
    def generate(self, prompts, **kwargs):
        """æ‰¹é‡ç”Ÿæˆ"""
        sampling_params = SamplingParams(
            temperature=kwargs.get('temperature', 0.8),
            top_p=kwargs.get('top_p', 0.95),
            max_tokens=kwargs.get('max_tokens', 100),
            presence_penalty=kwargs.get('presence_penalty', 0.0),
            frequency_penalty=kwargs.get('frequency_penalty', 0.0)
        )
        
        outputs = self.llm.generate(prompts, sampling_params)
        
        results = []
        for output in outputs:
            results.append({
                'prompt': output.prompt,
                'generated_text': output.outputs[0].text,
                'finish_reason': output.outputs[0].finish_reason
            })
        
        return results
    
    def benchmark(self, num_prompts=100, prompt_len=128, output_len=128):
        """æ€§èƒ½æµ‹è¯•"""
        import time
        
        # ç”Ÿæˆæµ‹è¯•prompts
        prompts = ["Write a story about a robot"] * num_prompts
        
        start_time = time.time()
        outputs = self.generate(prompts, max_tokens=output_len)
        total_time = time.time() - start_time
        
        # è®¡ç®—ååé‡
        total_tokens = sum(len(o['generated_text'].split()) for o in outputs)
        throughput = total_tokens / total_time
        
        print(f"å¤„ç† {num_prompts} ä¸ªè¯·æ±‚")
        print(f"æ€»æ—¶é—´: {total_time:.2f} ç§’")
        print(f"ååé‡: {throughput:.2f} tokens/ç§’")
        print(f"å¹³å‡å»¶è¿Ÿ: {total_time/num_prompts:.3f} ç§’/è¯·æ±‚")
        
        return throughput

# vLLMæœåŠ¡å™¨éƒ¨ç½²
def start_vllm_server():
    """å¯åŠ¨vLLM APIæœåŠ¡å™¨"""
    import subprocess
    
    # å¯åŠ¨å‘½ä»¤
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", "facebook/opt-125m",
        "--port", "8000",
        "--max-model-len", "2048",
        "--gpu-memory-utilization", "0.9"
    ]
    
    subprocess.run(cmd)

# å®¢æˆ·ç«¯è°ƒç”¨
def vllm_client_example():
    """vLLMå®¢æˆ·ç«¯ç¤ºä¾‹"""
    import openai
    
    # è®¾ç½®APIç«¯ç‚¹
    openai.api_base = "http://localhost:8000/v1"
    openai.api_key = "dummy"
    
    # è°ƒç”¨
    response = openai.Completion.create(
        model="facebook/opt-125m",
        prompt="Hello, my name is",
        max_tokens=100,
        temperature=0.8
    )
    
    print(response.choices[0].text)
```

## 3. Ollama - æœ¬åœ°LLMè¿è¡Œ ğŸ¦™

```python
# å®‰è£…: curl https://ollama.ai/install.sh | sh

import requests
import json

class OllamaClient:
    """Ollamaå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
    
    def pull_model(self, model_name="llama2"):
        """æ‹‰å–æ¨¡å‹"""
        response = requests.post(
            f"{self.base_url}/api/pull",
            json={"name": model_name}
        )
        return response.json()
    
    def generate(self, model, prompt, stream=False):
        """ç”Ÿæˆæ–‡æœ¬"""
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        
        response = requests.post(
            f"{self.base_url}/api/generate",
            json=payload,
            stream=stream
        )
        
        if stream:
            for line in response.iter_lines():
                if line:
                    yield json.loads(line)
        else:
            return response.json()
    
    def chat(self, model, messages):
        """å¯¹è¯æ¥å£"""
        payload = {
            "model": model,
            "messages": messages
        }
        
        response = requests.post(
            f"{self.base_url}/api/chat",
            json=payload
        )
        
        return response.json()
    
    def list_models(self):
        """åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹"""
        response = requests.get(f"{self.base_url}/api/tags")
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
ollama = OllamaClient()

# æ‹‰å–æ¨¡å‹
# ollama.pull_model("llama2:7b")

# ç”Ÿæˆæ–‡æœ¬
result = ollama.generate(
    model="llama2",
    prompt="What is machine learning?"
)
print(result['response'])

# æµå¼ç”Ÿæˆ
for chunk in ollama.generate("llama2", "Tell me a joke", stream=True):
    print(chunk['response'], end='', flush=True)

# å¯¹è¯
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's the weather like?"}
]
response = ollama.chat("llama2", messages)
print(response['message']['content'])

# Ollamaæ¨¡å‹æ–‡ä»¶
def create_modelfile():
    """åˆ›å»ºè‡ªå®šä¹‰Ollamaæ¨¡å‹"""
    modelfile = """
    FROM llama2
    
    # è®¾ç½®å‚æ•°
    PARAMETER temperature 0.8
    PARAMETER top_p 0.9
    
    # è®¾ç½®ç³»ç»Ÿæç¤º
    SYSTEM You are a helpful AI assistant specialized in coding.
    
    # è®¾ç½®æ¨¡æ¿
    TEMPLATE \"\"\"
    {{ .System }}
    User: {{ .Prompt }}
    Assistant: \"\"\"
    """
    
    with open("Modelfile", "w") as f:
        f.write(modelfile)
    
    # åˆ›å»ºæ¨¡å‹
    # ollama create mymodel -f Modelfile
```

## 4. ä¼˜åŒ–æŠ€æœ¯ ğŸš€

```python
# Flash Attention
def setup_flash_attention():
    """é…ç½®Flash AttentionåŠ é€Ÿ"""
    from flash_attn import flash_attn_func
    
    class FlashAttentionLLM(nn.Module):
        def __init__(self, config):
            super().__init__()
            self.flash_attn = flash_attn_func
            # ... å…¶ä»–åˆå§‹åŒ–
        
        def forward(self, hidden_states):
            # ä½¿ç”¨Flash Attention
            attn_output = self.flash_attn(
                hidden_states,
                hidden_states,
                hidden_states,
                causal=True
            )
            return attn_output

# PagedAttention (vLLMæ ¸å¿ƒæŠ€æœ¯)
class PagedAttention:
    """åˆ†é¡µæ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, block_size=16, num_blocks=256):
        self.block_size = block_size
        self.num_blocks = num_blocks
        self.kv_cache = self._init_cache()
    
    def _init_cache(self):
        """åˆå§‹åŒ–KVç¼“å­˜"""
        return {
            'keys': torch.zeros(self.num_blocks, self.block_size, 768),
            'values': torch.zeros(self.num_blocks, self.block_size, 768),
            'block_table': {}  # è™šæ‹Ÿåˆ°ç‰©ç†å—çš„æ˜ å°„
        }
    
    def allocate_blocks(self, seq_id, num_tokens):
        """ä¸ºåºåˆ—åˆ†é…å†…å­˜å—"""
        num_blocks_needed = (num_tokens + self.block_size - 1) // self.block_size
        # åˆ†é…é€»è¾‘...

# Continuous Batching
class ContinuousBatchingServer:
    """è¿ç»­æ‰¹å¤„ç†æœåŠ¡å™¨"""
    def __init__(self, model, max_batch_size=32):
        self.model = model
        self.max_batch_size = max_batch_size
        self.request_queue = []
        self.active_sequences = {}
    
    def add_request(self, prompt, request_id):
        """æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        self.request_queue.append({
            'id': request_id,
            'prompt': prompt,
            'generated_tokens': [],
            'finished': False
        })
    
    def process_batch(self):
        """å¤„ç†ä¸€æ‰¹è¯·æ±‚"""
        # æ„å»ºæ‰¹æ¬¡
        batch = self._build_batch()
        
        # æ¨¡å‹æ¨ç†
        outputs = self.model.generate(batch)
        
        # æ›´æ–°åºåˆ—çŠ¶æ€
        self._update_sequences(outputs)
        
        # è¿”å›å®Œæˆçš„åºåˆ—
        return self._get_finished_sequences()
    
    def _build_batch(self):
        """æ„å»ºæ‰¹æ¬¡ï¼ˆå¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼‰"""
        # å®ç°ç»†èŠ‚...
        pass
```

## 5. é‡åŒ–å’Œå‹ç¼© ğŸ“¦

```python
# GPTQé‡åŒ–
def gptq_quantization(model_name):
    """GPTQé‡åŒ–"""
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
    
    # é‡åŒ–é…ç½®
    quantize_config = BaseQuantizeConfig(
        bits=4,
        group_size=128,
        desc_act=False
    )
    
    # åŠ è½½å’Œé‡åŒ–
    model = AutoGPTQForCausalLM.from_pretrained(
        model_name,
        quantize_config=quantize_config
    )
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    model.save_quantized("model-gptq-4bit")
    
    return model

# AWQé‡åŒ–
def awq_quantization(model_name):
    """AWQé‡åŒ–"""
    from awq import AutoAWQForCausalLM
    
    model = AutoAWQForCausalLM.from_pretrained(model_name)
    
    # é‡åŒ–é…ç½®
    quant_config = {
        "zero_point": True,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "GEMM"
    }
    
    # é‡åŒ–
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        calib_data=calibration_data
    )
    
    # ä¿å­˜
    model.save_quantized("model-awq-4bit")
    
    return model

# SmoothQuant
def smooth_quant(model):
    """SmoothQuanté‡åŒ–"""
    from smoothquant import smooth_quantize
    
    # å¹³æ»‘é‡åŒ–
    smooth_model = smooth_quantize(
        model,
        alpha=0.5,  # å¹³æ»‘å› å­
        quant_mode="int8"
    )
    
    return smooth_model
```

## 6. åˆ†å¸ƒå¼éƒ¨ç½² ğŸŒ

```python
# Ray Serveéƒ¨ç½²
import ray
from ray import serve

@serve.deployment(
    num_replicas=2,
    ray_actor_options={"num_gpus": 1}
)
class LLMDeployment:
    def __init__(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.model = AutoModelForCausalLM.from_pretrained("gpt2")
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
    
    async def __call__(self, request):
        prompt = request.query_params["prompt"]
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(**inputs, max_length=100)
        
        response = self.tokenizer.decode(outputs[0])
        return {"generated_text": response}

# å¯åŠ¨Ray Serve
ray.init()
serve.start()
LLMDeployment.deploy()

# Kuberneteséƒ¨ç½²
k8s_deployment = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-server
  template:
    metadata:
      labels:
        app: llm-server
    spec:
      containers:
      - name: llm-container
        image: vllm/vllm-openai:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_NAME
          value: "meta-llama/Llama-2-7b-hf"
---
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm-server
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
"""
```

## 7. ç›‘æ§å’Œä¼˜åŒ– ğŸ“Š

```python
class LLMMonitor:
    """LLMç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.metrics = {
            'latency': [],
            'throughput': [],
            'memory_usage': [],
            'gpu_utilization': []
        }
    
    def log_request(self, request_id, prompt_len, output_len, latency):
        """è®°å½•è¯·æ±‚æŒ‡æ ‡"""
        self.metrics['latency'].append(latency)
        throughput = (prompt_len + output_len) / latency
        self.metrics['throughput'].append(throughput)
    
    def get_gpu_metrics(self):
        """è·å–GPUæŒ‡æ ‡"""
        import pynvml
        
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        
        # GPUåˆ©ç”¨ç‡
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        
        # å†…å­˜ä½¿ç”¨
        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        
        return {
            'gpu_util': util.gpu,
            'memory_used': mem_info.used / 1024**3,  # GB
            'memory_total': mem_info.total / 1024**3
        }
    
    def optimize_batch_size(self, model, test_prompts):
        """è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤§å°"""
        best_throughput = 0
        best_batch_size = 1
        
        for batch_size in [1, 2, 4, 8, 16, 32]:
            try:
                throughput = self._test_batch_size(model, test_prompts, batch_size)
                if throughput > best_throughput:
                    best_throughput = throughput
                    best_batch_size = batch_size
            except RuntimeError:  # OOM
                break
        
        return best_batch_size

# PrometheusæŒ‡æ ‡å¯¼å‡º
from prometheus_client import Counter, Histogram, Gauge

request_count = Counter('llm_requests_total', 'Total LLM requests')
request_latency = Histogram('llm_request_latency_seconds', 'LLM request latency')
gpu_memory = Gauge('llm_gpu_memory_bytes', 'GPU memory usage')

@request_latency.time()
def serve_request(prompt):
    """å¤„ç†è¯·æ±‚å¹¶è®°å½•æŒ‡æ ‡"""
    request_count.inc()
    # å¤„ç†é€»è¾‘...
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“

```python
def deployment_recommendations():
    """éƒ¨ç½²å»ºè®®"""
    
    recommendations = {
        "å°æ¨¡å‹(<7B)": {
            "æ¡†æ¶": "Transformers + FastAPI",
            "é‡åŒ–": "INT8 åŠ¨æ€é‡åŒ–",
            "éƒ¨ç½²": "å•GPUæˆ–CPU"
        },
        "ä¸­æ¨¡å‹(7B-13B)": {
            "æ¡†æ¶": "vLLMæˆ–TGI",
            "é‡åŒ–": "GPTQ 4-bit",
            "éƒ¨ç½²": "å•GPU (24GB+)"
        },
        "å¤§æ¨¡å‹(30B+)": {
            "æ¡†æ¶": "vLLM + Ray",
            "é‡åŒ–": "AWQ 4-bit",
            "éƒ¨ç½²": "å¤šGPUå¹¶è¡Œ"
        },
        "ç”Ÿäº§ç¯å¢ƒ": {
            "æœåŠ¡": "Triton Inference Server",
            "ç¼–æ’": "Kubernetes",
            "ç›‘æ§": "Prometheus + Grafana"
        }
    }
    
    return recommendations

# æ€§èƒ½ä¼˜åŒ–æ¸…å•
optimization_checklist = """
âœ… ä½¿ç”¨é€‚å½“çš„é‡åŒ–æ–¹æ³•ï¼ˆGPTQ/AWQ/SmoothQuantï¼‰
âœ… å¯ç”¨Flash Attentionæˆ–ç±»ä¼¼ä¼˜åŒ–
âœ… ä½¿ç”¨è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰
âœ… å®ç°KVç¼“å­˜å¤ç”¨
âœ… ä¼˜åŒ–é‡‡æ ·å‚æ•°ï¼ˆæ¸©åº¦ã€top_pç­‰ï¼‰
âœ… ä½¿ç”¨æ¨¡å‹å¹¶è¡Œæˆ–å¼ é‡å¹¶è¡Œï¼ˆå¤§æ¨¡å‹ï¼‰
âœ… ç›‘æ§GPUå†…å­˜å’Œåˆ©ç”¨ç‡
âœ… å®ç°è¯·æ±‚é˜Ÿåˆ—å’Œè´Ÿè½½å‡è¡¡
âœ… ä½¿ç”¨CDNç¼“å­˜å¸¸è§å“åº”
âœ… å®ç°æ¨¡å‹A/Bæµ‹è¯•
"""

print(optimization_checklist)
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [NLPæ¨¡å‹](nlp_models.md) - BERTã€GPTã€T5è¯¦è§£
- [æ¨¡å‹å¾®è°ƒ](finetuning.md) - LoRAã€QLoRAç­‰é«˜æ•ˆå¾®è°ƒ
- [RAGç³»ç»Ÿ](rag_systems.md) - æ£€ç´¢å¢å¼ºç”Ÿæˆ