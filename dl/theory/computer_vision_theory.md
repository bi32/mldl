# 计算机视觉原理：从像素到理解 👁️

## 1. 视觉感知的计算原理

### 1.1 人类视觉系统的启发

人类视觉系统是一个层级处理系统：
- **视网膜**：1.3亿个感光细胞，但只有100万个神经节细胞输出
- **LGN（外侧膝状体）**：初步处理，增强对比度
- **V1（初级视觉皮层）**：检测边缘和方向
- **V2-V4**：检测形状、颜色、纹理
- **IT（下颞叶皮层）**：物体识别

### 1.2 Hubel & Wiesel的发现

1959年的诺贝尔奖研究发现：
1. **简单细胞**：响应特定方向的边缘
2. **复杂细胞**：位置不变性
3. **超复杂细胞**：端点检测

这些发现直接启发了卷积神经网络的设计！

### 1.3 计算机视觉的核心挑战

#### 视角变化
同一物体从不同角度看完全不同：
- 正面的脸 vs 侧面的脸
- 站立的猫 vs 蜷缩的猫

#### 光照变化
- 阴影可以完全改变物体外观
- 颜色在不同光源下变化（白金裙子之谜）

#### 遮挡问题
- 部分可见如何推断整体？
- 多个物体重叠如何分离？

#### 类内差异
- 所有的椅子都不同，但都是"椅子"
- 抽象概念的视觉化

## 2. 卷积神经网络的数学原理

### 2.1 卷积的本质

#### 信号处理视角
卷积是一种线性时不变系统：
```
(f * g)(t) = ∫ f(τ)g(t-τ)dτ
```

在图像中变为二维：
```
(I * K)(x,y) = ΣΣ I(x-i, y-j)K(i,j)
```

#### 为什么是"卷积"？
1. **模板匹配**：卷积核是一个模板
2. **特征提取**：不同卷积核提取不同特征
3. **平移等变性**：移动输入，输出相应移动

### 2.2 感受野的层级增长

#### 理论感受野
L层网络的感受野：
```
RF_L = 1 + Σ(k_i - 1) × Π s_j
```

其中k是核大小，s是步长。

#### 有效感受野
实际贡献呈高斯分布：
- 中心像素贡献最大
- 边缘像素贡献递减
- 深层网络的有效感受野远小于理论值

### 2.3 特征层级

#### 低层特征（边缘检测器）
```
Sobel_x = [[-1, 0, 1],
           [-2, 0, 2],
           [-1, 0, 1]]
```
检测垂直边缘

#### 中层特征（纹理检测器）
- Gabor滤波器组合
- 局部二值模式(LBP)
- 方向梯度直方图(HOG)

#### 高层特征（语义部件）
- 眼睛检测器
- 轮子检测器
- 面部检测器

## 3. 经典架构的设计哲学

### 3.1 LeNet-5 (1998) - 奠基者

设计原则：
- **局部感受野**：每个神经元只看局部区域
- **权重共享**：同一特征在所有位置检测
- **子采样**：逐步降低分辨率

关键创新：
- 首次成功应用于实际问题（支票识别）
- 证明了深度学习的可行性

### 3.2 AlexNet (2012) - 深度学习复兴

#### 为什么是2012？
1. **数据**：ImageNet提供了100万张标注图像
2. **计算**：GPU使训练时间从月缩短到周
3. **技巧**：ReLU、Dropout、数据增强

#### 关键洞察
- 深度很重要（8层 vs LeNet的5层）
- ReLU解决梯度消失
- Dropout防止过拟合

### 3.3 VGGNet (2014) - 简洁之美

#### 设计哲学
"使用最小的卷积核（3×3）重复堆叠"

#### 为什么3×3？
两个3×3卷积的感受野等于一个5×5：
- 参数更少：2×(3×3) = 18 < 25
- 非线性更多：两次ReLU vs 一次
- 正则化效果更好

### 3.4 ResNet (2015) - 残差革命

#### 退化问题
深度网络训练误差反而增加，不是过拟合，是优化困难！

#### 残差学习
不学习H(x)，而是学习F(x) = H(x) - x：
```
y = F(x) + x
```

#### 为什么有效？
1. **恒等映射**：最差情况下F(x)=0，保持输入
2. **梯度高速公路**：梯度可以直接反向传播
3. **集成学习视角**：不同深度的路径集成

### 3.5 DenseNet (2017) - 特征复用

#### 密集连接
每层都连接到所有后续层：
```
x_l = H_l([x_0, x_1, ..., x_{l-1}])
```

#### 优势
- **特征复用**：最大化特征利用
- **梯度流动**：每层都有直接监督
- **参数效率**：不需要重新学习特征

## 4. 注意力机制在视觉中的应用

### 4.1 空间注意力

#### SE（Squeeze-and-Excitation）
通道注意力机制：
1. Squeeze：全局平均池化
2. Excitation：两层全连接
3. Scale：通道加权

```
Attention = σ(W_2 · ReLU(W_1 · GAP(x)))
Output = x × Attention
```

### 4.2 CBAM（Convolutional Block Attention Module）

结合通道和空间注意力：
```
F' = M_c(F) ⊗ F  # 通道注意力
F'' = M_s(F') ⊗ F'  # 空间注意力
```

### 4.3 Non-local Neural Networks

自注意力在CNN中的应用：
```
y_i = (1/C(x)) Σ_j f(x_i, x_j)g(x_j)
```

捕获长程依赖，突破局部感受野限制。

## 5. Vision Transformer：范式转变

### 5.1 为什么Transformer能用于视觉？

#### 图像即序列
- 将图像分成16×16的patch
- 每个patch是一个"token"
- 224×224图像 → 196个tokens

#### 归纳偏置的权衡
CNN的归纳偏置：
- ✅ 局部性
- ✅ 平移等变性
- ❌ 需要大量架构设计

ViT的特点：
- ❌ 没有视觉特定的归纳偏置
- ✅ 灵活的全局建模
- ✅ 与NLP统一架构

### 5.2 位置编码的重要性

#### 为什么需要位置编码？
Transformer是置换不变的，但图像的空间结构很重要。

#### 可学习 vs 固定编码
- 固定：正弦编码，可外推
- 可学习：更灵活，但需要更多数据

### 5.3 ViT的扩展性法则

数据量与模型大小的关系：
- 小数据：CNN > ViT
- 中等数据：CNN ≈ ViT
- 大数据：ViT >> CNN

这解释了为什么ViT在JFT-300M上训练效果惊人。

## 6. 目标检测的演化

### 6.1 两阶段 vs 单阶段

#### R-CNN系列（两阶段）
1. **区域提议**：SelectiveSearch/RPN
2. **分类回归**：对每个提议分类

优势：精度高
劣势：速度慢

#### YOLO/SSD（单阶段）
直接预测类别和位置

优势：速度快
劣势：小物体检测困难

### 6.2 锚框（Anchor）机制

#### 为什么需要锚框？
- 物体大小差异巨大
- 长宽比变化多样
- 提供合理的初始猜测

#### 锚框设计
- 多尺度：在不同层级检测
- 多比例：1:1, 1:2, 2:1等
- 密集采样：每个位置多个锚框

### 6.3 NMS（非极大值抑制）

#### 问题
一个物体产生多个检测框

#### 解决方案
1. 按置信度排序
2. 保留最高分框
3. 抑制IoU > 阈值的框
4. 重复直到没有框

#### 改进
- Soft-NMS：渐进抑制而非直接删除
- DIoU-NMS：考虑中心距离

## 7. 语义分割的像素级理解

### 7.1 全卷积网络（FCN）

#### 核心思想
将全连接层转换为1×1卷积，保持空间维度。

#### 上采样策略
- 反卷积（转置卷积）
- 双线性插值
- 反池化

### 7.2 编码器-解码器架构

#### U-Net的对称设计
```
编码器：逐步降采样，提取语义
解码器：逐步上采样，恢复细节
跳跃连接：融合多尺度特征
```

#### 为什么跳跃连接重要？
- 保留空间细节
- 缓解梯度消失
- 多尺度特征融合

### 7.3 空洞卷积（Atrous/Dilated Convolution）

#### 动机
增大感受野而不降低分辨率

#### 原理
在卷积核中插入"空洞"：
```
正常3×3: [* * *]
         [* * *]
         [* * *]

空洞率2: [*   *   *]
        [         ]
        [*   *   *]
        [         ]
        [*   *   *]
```

## 8. 自监督学习在视觉中的应用

### 8.1 对比学习

#### SimCLR原理
1. 数据增强产生正样本对
2. 其他样本作为负样本
3. 最大化正样本相似度

损失函数：
```
L = -log(exp(sim(z_i, z_j)/τ) / Σ exp(sim(z_i, z_k)/τ))
```

### 8.2 MAE（Masked Autoencoder）

#### 核心思想
随机遮挡75%的图像块，重建被遮挡部分。

#### 为什么75%这么高？
- 图像有冗余性
- 强迫模型理解语义
- 类似BERT的15%遮挡

### 8.3 DINO：无监督的ViT

知识蒸馏without标签：
- 学生网络预测
- 教师网络（EMA）提供目标
- 自举式学习

## 9. 多模态学习：CLIP

### 9.1 对比学习框架

同时训练图像和文本编码器：
```
图像 → 图像编码器 → 图像特征
文本 → 文本编码器 → 文本特征
目标：匹配的图文对相似度最大
```

### 9.2 Zero-shot能力

通过文本提示实现零样本分类：
```
分类 = argmax(图像特征 · 文本特征)
```

### 9.3 为什么CLIP如此强大？

1. **数据规模**：4亿图文对
2. **自然语言监督**：比标签更丰富
3. **对比学习**：高效的表示学习

## 10. 计算机视觉的未来趋势

### 10.1 统一架构
- Vision Transformer统一视觉任务
- 多任务学习
- 通用视觉模型

### 10.2 自监督预训练
- 减少标注依赖
- 更好的泛化能力
- 持续学习

### 10.3 3D视觉
- NeRF（神经辐射场）
- 3D重建
- 增强现实

### 10.4 高效推理
- 模型压缩
- 神经架构搜索
- 边缘部署

## 思考题

1. 为什么CNN在小数据集上比ViT表现更好？
2. 残差连接除了梯度流动，还有什么深层含义？
3. 为什么目标检测比分类困难得多？
4. 自注意力和卷积的本质区别是什么？
5. CLIP的成功对未来CV发展有何启示？

## 深入阅读

- "ImageNet Classification with Deep Convolutional Neural Networks" - AlexNet
- "Deep Residual Learning for Image Recognition" - ResNet
- "An Image is Worth 16x16 Words" - Vision Transformer
- "Learning Transferable Visual Models From Natural Language Supervision" - CLIP
- "Masked Autoencoders Are Scalable Vision Learners" - MAE
- 《Computer Vision: Algorithms and Applications》- Richard Szeliski