# 深度学习基础原理：从感知机到深度网络 🧠

## 1. 从生物神经元到人工神经元

### 1.1 生物学启发

人脑约有860亿个神经元，每个神经元通过突触与约7000个其他神经元连接。神经元的工作原理：

1. **树突**（Dendrites）：接收输入信号
2. **胞体**（Cell Body）：整合信号
3. **轴突**（Axon）：传递输出
4. **突触**（Synapses）：连接强度可调

### 1.2 MP神经元模型（1943）

McCulloch-Pitts神经元：
```
y = f(Σᵢ wᵢxᵢ - θ)
```

其中：
- xᵢ：输入信号
- wᵢ：连接权重（突触强度）
- θ：阈值
- f：激活函数（最初是阶跃函数）

### 1.3 感知机（Perceptron, 1957）

Rosenblatt的感知机是第一个可学习的神经元：

**学习规则**：
```
wᵢ := wᵢ + η(y - ŷ)xᵢ
```

**几何意义**：
- 感知机定义一个超平面：w^T x + b = 0
- 学习过程就是调整超平面位置
- 只能解决线性可分问题

### 1.4 XOR问题与AI寒冬

Minsky和Papert（1969）证明单层感知机无法解决XOR问题：

| x₁ | x₂ | XOR |
|----|----|----|
| 0  | 0  | 0  |
| 0  | 1  | 1  |
| 1  | 0  | 1  |
| 1  | 1  | 0  |

这导致了第一次AI寒冬（1970s-1980s）。

## 2. 多层感知机与反向传播

### 2.1 万能近似定理

**Cybenko定理（1989）**：
一个具有单个隐藏层的前馈神经网络，如果隐藏层有足够多的神经元，可以以任意精度逼近任何连续函数。

但是：
- "足够多"可能是指数级的
- 深层网络更有效率

### 2.2 反向传播算法

#### 前向传播
```
z^[l] = W^[l] a^[l-1] + b^[l]
a^[l] = g^[l](z^[l])
```

#### 反向传播
链式法则的优雅应用：
```
δ^[l] = ∂L/∂z^[l]
     = (W^[l+1])^T δ^[l+1] ⊙ g'^[l](z^[l])
```

梯度计算：
```
∂L/∂W^[l] = δ^[l] (a^[l-1])^T
∂L/∂b^[l] = δ^[l]
```

#### 计算图视角

将计算表示为有向无环图（DAG）：
- 节点：操作
- 边：数据流
- 前向：计算输出
- 反向：计算梯度

### 2.3 梯度消失与爆炸

#### 问题根源

考虑深度为L的网络：
```
∂L/∂W^[1] = ∂L/∂a^[L] × ∏ᵢ₌₁^L ∂a^[i]/∂a^[i-1]
```

如果每层梯度<1：指数级消失
如果每层梯度>1：指数级爆炸

#### Sigmoid的问题
```
σ'(x) = σ(x)(1-σ(x))
```
最大值：σ'(0) = 0.25

L层网络：梯度 ≤ (0.25)^L → 0

## 3. 激活函数的演进

### 3.1 为什么需要非线性？

没有激活函数的多层网络：
```
y = W₃(W₂(W₁x)) = (W₃W₂W₁)x = Wx
```
仍然是线性变换！

### 3.2 激活函数家族

#### Sigmoid
```
σ(x) = 1/(1 + e^(-x))
```
- 优点：平滑、概率解释
- 缺点：梯度消失、非零中心、计算昂贵

#### Tanh
```
tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
```
- 优点：零中心
- 缺点：仍有梯度消失

#### ReLU革命
```
ReLU(x) = max(0, x)
```
- 优点：
  - 梯度不消失（x>0时）
  - 计算简单
  - 稀疏激活
- 缺点：
  - 死亡ReLU问题
  - 非零中心

#### ReLU变体
- **Leaky ReLU**：`max(αx, x)`，α≈0.01
- **PReLU**：α可学习
- **ELU**：`x if x>0 else α(e^x - 1)`
- **GELU**：`x × Φ(x)`（Φ是高斯CDF）
- **Swish**：`x × σ(βx)`

### 3.3 激活函数的选择

| 场景 | 推荐 | 原因 |
|------|------|------|
| 隐藏层 | ReLU/GELU | 快速、有效 |
| 输出层（分类） | Softmax | 概率解释 |
| 输出层（回归） | None/Linear | 不限制范围 |
| LSTM/GRU | Tanh/Sigmoid | 门控机制 |
| Transformer | GELU | 平滑、性能好 |

## 4. 优化算法深度剖析

### 4.1 SGD及其变体

#### 动量（Momentum）
物理直觉：球滚下山坡

```
v_t = βv_{t-1} + (1-β)∇L
w_t = w_{t-1} - αv_t
```

- 加速收敛
- 减少振荡
- 帮助跳出局部最小值

#### Nesterov加速梯度（NAG）
"向前看"的动量：

```
v_t = βv_{t-1} + (1-β)∇L(w_{t-1} - βv_{t-1})
w_t = w_{t-1} - αv_t
```

在动量方向上计算梯度，更快的收敛。

### 4.2 自适应学习率算法

#### AdaGrad
累积历史梯度平方：
```
G_t = G_{t-1} + g_t²
w_t = w_{t-1} - α/√(G_t + ε) × g_t
```

问题：学习率单调递减至0

#### RMSprop
指数移动平均：
```
G_t = βG_{t-1} + (1-β)g_t²
w_t = w_{t-1} - α/√(G_t + ε) × g_t
```

#### Adam：最佳组合
一阶矩（动量）+ 二阶矩（自适应）：
```
m_t = β₁m_{t-1} + (1-β₁)g_t
v_t = β₂v_{t-1} + (1-β₂)g_t²
m̂_t = m_t/(1-β₁^t)  # 偏差修正
v̂_t = v_t/(1-β₂^t)
w_t = w_{t-1} - α × m̂_t/(√v̂_t + ε)
```

### 4.3 学习率调度

#### 阶梯衰减
```
lr = lr₀ × γ^(epoch // step_size)
```

#### 余弦退火
```
lr = lr_min + 0.5(lr_max - lr_min)(1 + cos(πt/T))
```

#### Warmup
开始时逐渐增加学习率：
```
lr = lr₀ × t/warmup_steps  (t < warmup_steps)
```

重要性：
- 稳定训练初期
- 对大batch size至关重要

## 5. 正则化技术

### 5.1 Dropout：随机失活

训练时随机"关闭"神经元：
```
h = dropout(activate(Wx + b), p)
```

#### 为什么有效？

1. **集成学习视角**：训练指数级个子网络
2. **协同适应**：防止神经元协同适应
3. **贝叶斯视角**：近似贝叶斯推断

#### 测试时的缩放
```
训练：h = mask ⊙ activate(Wx + b) / p
测试：h = activate(Wx + b)
```

### 5.2 Batch Normalization

#### 内部协变量偏移（ICS）
深层网络中，每层输入分布不断变化，导致训练困难。

#### BN的解决方案
```
μ_B = (1/m)Σᵢ xᵢ
σ²_B = (1/m)Σᵢ (xᵢ - μ_B)²
x̂ᵢ = (xᵢ - μ_B)/√(σ²_B + ε)
yᵢ = γx̂ᵢ + β
```

γ和β是可学习参数，保持表达能力。

#### 为什么BN如此有效？

最新研究表明，BN的作用可能是：
1. **平滑损失景观**
2. **使优化更稳定**
3. **隐式正则化**

### 5.3 Layer Normalization

对特征维度归一化（而非batch维度）：
```
μ = (1/d)Σⱼ xⱼ
σ² = (1/d)Σⱼ (xⱼ - μ)²
```

优势：
- 不依赖batch size
- 适合RNN和Transformer

### 5.4 权重初始化

#### Xavier/Glorot初始化
保持方差不变：
```
W ~ N(0, 2/(n_in + n_out))
```

#### He初始化
针对ReLU：
```
W ~ N(0, 2/n_in)
```

#### 为什么初始化如此重要？

不当的初始化导致：
- 梯度消失/爆炸
- 死亡神经元
- 收敛极慢

## 6. 卷积神经网络原理

### 6.1 卷积的动机

#### 全连接的问题
图像28×28×3 → 隐层1000：
参数数量 = 28×28×3×1000 = 2,352,000

#### 卷积的假设
1. **局部连接**：像素与邻近像素相关
2. **权重共享**：同一特征在不同位置
3. **平移不变性**：物体位置不影响识别

### 6.2 卷积运算

连续形式：
```
(f * g)(t) = ∫ f(τ)g(t-τ)dτ
```

离散形式（实际是互相关）：
```
S(i,j) = (I * K)(i,j) = ΣₘΣₙ I(i+m, j+n)K(m,n)
```

### 6.3 感受野

#### 理论感受野
```
RF_l = RF_{l-1} + (K_l - 1) × ∏ᵢ₌₁^{l-1} S_i
```

#### 有效感受野
实际上呈高斯分布，中心权重更大。

### 6.4 池化的作用

1. **降维**：减少参数和计算
2. **平移不变性**：小的位移不影响输出
3. **特征选择**：Max pooling选择最强响应

争议：现代架构趋向于使用stride卷积替代池化。

## 7. 循环神经网络原理

### 7.1 序列建模的挑战

- 可变长度输入
- 长程依赖
- 时序关系

### 7.2 RNN的计算图

展开的RNN：
```
h_t = tanh(W_hh h_{t-1} + W_xh x_t + b_h)
y_t = W_hy h_t + b_y
```

### 7.3 BPTT（时间反向传播）

梯度计算：
```
∂L/∂W = Σₜ ∂L_t/∂W
```

问题：梯度需要经过t步传播。

### 7.4 LSTM：长短期记忆

#### 门控机制
```
f_t = σ(W_f[h_{t-1}, x_t] + b_f)  # 遗忘门
i_t = σ(W_i[h_{t-1}, x_t] + b_i)  # 输入门
C̃_t = tanh(W_C[h_{t-1}, x_t] + b_C)  # 候选值
C_t = f_t ⊙ C_{t-1} + i_t ⊙ C̃_t  # 细胞状态
o_t = σ(W_o[h_{t-1}, x_t] + b_o)  # 输出门
h_t = o_t ⊙ tanh(C_t)  # 隐状态
```

#### 为什么LSTM有效？

1. **恒定误差流**：细胞状态提供梯度高速公路
2. **选择性记忆**：门控制信息流
3. **梯度裁剪**：sigmoid限制梯度范围

### 7.5 GRU：简化版LSTM

```
z_t = σ(W_z[h_{t-1}, x_t])  # 更新门
r_t = σ(W_r[h_{t-1}, x_t])  # 重置门
h̃_t = tanh(W[r_t ⊙ h_{t-1}, x_t])
h_t = (1-z_t) ⊙ h_{t-1} + z_t ⊙ h̃_t
```

更少的参数，相似的性能。

## 8. Transformer的革命

### 8.1 注意力机制

#### 缩放点积注意力
```
Attention(Q,K,V) = softmax(QK^T/√d_k)V
```

为什么除以√d_k？
- 点积的方差随维度增长
- Softmax在大值时梯度消失

### 8.2 多头注意力

```
MultiHead(Q,K,V) = Concat(head₁,...,head_h)W^O
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

不同的头学习不同的关系。

### 8.3 位置编码

正弦编码：
```
PE_{(pos,2i)} = sin(pos/10000^{2i/d})
PE_{(pos,2i+1)} = cos(pos/10000^{2i/d})
```

性质：
- 相对位置可以表示为线性变换
- 可以外推到更长序列

### 8.4 为什么Transformer如此成功？

1. **并行化**：没有序列依赖
2. **长程依赖**：直接连接
3. **表达能力**：自注意力是动态的
4. **可扩展性**：容易扩展到大模型

## 9. 深度学习的理论困惑

### 9.1 过参数化悖论

深度网络通常有数百万参数，却在测试集上表现良好。这违反了传统学习理论。

可能的解释：
1. **隐式正则化**：SGD偏好简单解
2. **双下降现象**：过参数化后测试误差再次下降
3. **彩票假设**：大网络包含性能好的子网络

### 9.2 深度的价值

为什么深度网络比宽网络更有效？

1. **特征层次**：逐层抽象
2. **指数表达能力**：深度k可以表达2^k复杂度的函数
3. **组合性**：复用低层特征

### 9.3 泛化之谜

Zhang等人（2017）：深度网络可以完美拟合随机标签，但为什么能泛化？

当前理解：
- **平坦最小值**：泛化更好
- **隐式偏置**：SGD找到的解有特殊性质
- **数据流形**：真实数据在低维流形上

## 思考题

1. 为什么ReLU这么简单的函数如此有效？
2. BatchNorm真的是在解决内部协变量偏移吗？
3. 为什么Transformer在NLP领域击败了RNN？
4. 深度学习的成功是因为理论还是工程？
5. 神经网络是在记忆还是在理解？

## 推荐阅读

- 《Deep Learning》- Goodfellow, Bengio, Courville
- 《Understanding Deep Learning》- Simon Prince
- 《The Principles of Deep Learning Theory》- Roberts et al.
- 《Dive into Deep Learning》- Zhang et al.
- 关键论文：
  - "Attention Is All You Need"
  - "Deep Residual Learning for Image Recognition"
  - "Understanding Deep Learning Requires Rethinking Generalization"