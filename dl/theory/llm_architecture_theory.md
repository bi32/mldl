# å¤§è¯­è¨€æ¨¡å‹æ¶æ„ä¸è®­ç»ƒç†è®º ğŸ¤–

æ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåŸç†ã€æ¶æ„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ã€‚

## 1. Transformer æ¶æ„æ·±å…¥ ğŸ—ï¸

### 1.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•°å­¦åŸç†

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import math

class MultiHeadAttentionTheory:
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ç†è®ºè§£æ"""
    
    def __init__(self, d_model=512, n_heads=8):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
    
    def scaled_dot_product_attention_analysis(self):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›åˆ†æ"""
        print("=== ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶åˆ†æ ===")
        
        # æ³¨æ„åŠ›å…¬å¼ï¼šAttention(Q,K,V) = softmax(QK^T / sqrt(d_k))V
        print("æ•°å­¦å…¬å¼:")
        print("Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V")
        print()
        
        # 1. ä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼Ÿ
        print("1. ç¼©æ”¾å› å­ 1/âˆšd_k çš„ä½œç”¨:")
        print("   - å½“d_kå¾ˆå¤§æ—¶ï¼ŒQK^Tçš„å€¼ä¼šå˜å¾—å¾ˆå¤§")
        print("   - å¤§çš„å€¼ä¼šä½¿softmaxè¿›å…¥é¥±å’ŒåŒºï¼Œæ¢¯åº¦å˜å°")
        print("   - ç¼©æ”¾å¯ä»¥ä¿æŒæ¢¯åº¦ç¨³å®š")
        
        # å®éªŒéªŒè¯
        d_k_values = [16, 64, 256, 1024]
        for d_k in d_k_values:
            Q = torch.randn(10, d_k)
            K = torch.randn(10, d_k)
            
            scores_unscaled = torch.matmul(Q, K.transpose(-2, -1))
            scores_scaled = scores_unscaled / math.sqrt(d_k)
            
            print(f"   d_k={d_k}: æœªç¼©æ”¾æ–¹å·®={scores_unscaled.var():.4f}, "
                  f"ç¼©æ”¾åæ–¹å·®={scores_scaled.var():.4f}")
        print()
        
        # 2. æ³¨æ„åŠ›æƒé‡çš„å«ä¹‰
        print("2. æ³¨æ„åŠ›æƒé‡è§£é‡Š:")
        print("   - æƒé‡Î±_ijè¡¨ç¤ºä½ç½®iå¯¹ä½ç½®jçš„å…³æ³¨ç¨‹åº¦")
        print("   - Î£_j Î±_ij = 1 (æ¯è¡Œæƒé‡å’Œä¸º1)")
        print("   - é«˜æƒé‡ = å¼ºç›¸å…³æ€§")
        
        return self.visualize_attention_pattern()
    
    def visualize_attention_pattern(self):
        """å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼"""
        # åˆ›å»ºç¤ºä¾‹å¥å­çš„æ³¨æ„åŠ›
        seq_len = 8
        torch.manual_seed(42)
        
        # æ¨¡æ‹ŸQ, K
        Q = torch.randn(1, seq_len, 64)
        K = torch.randn(1, seq_len, 64)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(64)
        attention_weights = F.softmax(scores, dim=-1)
        
        # å¯è§†åŒ–
        plt.figure(figsize=(8, 6))
        plt.imshow(attention_weights[0].numpy(), cmap='Blues', interpolation='nearest')
        plt.colorbar()
        plt.xlabel('Key Position')
        plt.ylabel('Query Position')
        plt.title('Attention Weights Visualization')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(seq_len):
            for j in range(seq_len):
                plt.text(j, i, f'{attention_weights[0, i, j]:.2f}', 
                        ha='center', va='center', color='red')
        
        plt.tight_layout()
        plt.show()
        
        return attention_weights

class TransformerAnalysis:
    """Transformeræ¶æ„åˆ†æ"""
    
    def __init__(self):
        self.components = {
            "Encoder": ["Multi-Head Attention", "Feed Forward", "Layer Norm", "Residual Connection"],
            "Decoder": ["Masked Self-Attention", "Cross-Attention", "Feed Forward", "Layer Norm"]
        }
    
    def position_encoding_analysis(self):
        """ä½ç½®ç¼–ç åˆ†æ"""
        print("=== ä½ç½®ç¼–ç åˆ†æ ===")
        
        def get_positional_encoding(seq_len, d_model):
            """è·å–ä½ç½®ç¼–ç """
            pe = torch.zeros(seq_len, d_model)
            position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
            
            # è®¡ç®—é™¤æ³•é¡¹
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                               (-math.log(10000.0) / d_model))
            
            # è®¡ç®—sinå’Œcos
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            
            return pe
        
        # ç”Ÿæˆä½ç½®ç¼–ç 
        seq_len, d_model = 100, 512
        pe = get_positional_encoding(seq_len, d_model)
        
        print("ä½ç½®ç¼–ç å…¬å¼:")
        print("PE(pos, 2i) = sin(pos / 10000^(2i/d_model))")
        print("PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))")
        print()
        
        # å¯è§†åŒ–ä½ç½®ç¼–ç 
        plt.figure(figsize=(12, 8))
        
        # æ˜¾ç¤ºå‰64ä¸ªç»´åº¦çš„ä½ç½®ç¼–ç 
        plt.subplot(2, 2, 1)
        plt.imshow(pe[:50, :64].T, cmap='RdBu', aspect='auto')
        plt.colorbar()
        plt.xlabel('Position')
        plt.ylabel('Embedding Dimension')
        plt.title('Positional Encoding Pattern')
        
        # æ˜¾ç¤ºç‰¹å®šä½ç½®çš„ç¼–ç 
        plt.subplot(2, 2, 2)
        positions = [0, 10, 50, 99]
        for pos in positions:
            plt.plot(pe[pos, :64].numpy(), label=f'Position {pos}')
        plt.xlabel('Dimension')
        plt.ylabel('Value')
        plt.title('Positional Encoding for Different Positions')
        plt.legend()
        
        # æ˜¾ç¤ºç›¸å¯¹ä½ç½®å…³ç³»
        plt.subplot(2, 2, 3)
        # è®¡ç®—ä¸åŒä½ç½®é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for i in range(20):
            sim = F.cosine_similarity(pe[i:i+1], pe[i+1:i+21], dim=1)
            similarities.append(sim.numpy())
        
        similarities = np.array(similarities)
        plt.imshow(similarities, cmap='coolwarm', aspect='auto')
        plt.colorbar()
        plt.xlabel('Relative Distance')
        plt.ylabel('Reference Position')
        plt.title('Positional Similarity Matrix')
        
        # é¢‘ç‡åˆ†æ
        plt.subplot(2, 2, 4)
        frequencies = 1 / (10000 ** (torch.arange(0, d_model, 2) / d_model))
        plt.plot(frequencies.numpy())
        plt.xlabel('Dimension Pair')
        plt.ylabel('Frequency')
        plt.title('Encoding Frequencies')
        plt.yscale('log')
        
        plt.tight_layout()
        plt.show()
        
        return pe
    
    def feed_forward_analysis(self):
        """å‰é¦ˆç½‘ç»œåˆ†æ"""
        print("=== å‰é¦ˆç½‘ç»œåˆ†æ ===")
        
        class FFN(nn.Module):
            def __init__(self, d_model, d_ff):
                super().__init__()
                self.linear1 = nn.Linear(d_model, d_ff)
                self.linear2 = nn.Linear(d_ff, d_model)
                self.dropout = nn.Dropout(0.1)
            
            def forward(self, x):
                return self.linear2(self.dropout(F.relu(self.linear1(x))))
        
        # åˆ†æFFNçš„ä½œç”¨
        d_model, d_ff = 512, 2048
        ffn = FFN(d_model, d_ff)
        
        print(f"FFNç»“æ„: {d_model} â†’ {d_ff} â†’ {d_model}")
        print(f"å‚æ•°é‡: {d_model * d_ff + d_ff * d_model + d_ff + d_model:,}")
        print()
        
        # åˆ†ææ¿€æ´»æ¨¡å¼
        x = torch.randn(1, 10, d_model)
        
        # ç¬¬ä¸€å±‚è¾“å‡º
        hidden = F.relu(ffn.linear1(x))
        activation_ratio = (hidden > 0).float().mean()
        print(f"ReLUæ¿€æ´»æ¯”ä¾‹: {activation_ratio:.2%}")
        
        # åˆ†æä¸åŒæ¿€æ´»å‡½æ•°çš„æ•ˆæœ
        activations = {
            'ReLU': F.relu,
            'GELU': F.gelu,
            'SiLU': F.silu
        }
        
        plt.figure(figsize=(12, 4))
        x_range = torch.linspace(-3, 3, 100)
        
        for i, (name, func) in enumerate(activations.items()):
            plt.subplot(1, 3, i+1)
            y = func(x_range)
            plt.plot(x_range.numpy(), y.numpy(), linewidth=2)
            plt.title(f'{name} Activation')
            plt.xlabel('Input')
            plt.ylabel('Output')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return ffn

# Layer Normalization vs Batch Normalization
class NormalizationAnalysis:
    """å½’ä¸€åŒ–æŠ€æœ¯åˆ†æ"""
    
    @staticmethod
    def compare_normalizations():
        """æ¯”è¾ƒä¸åŒå½’ä¸€åŒ–æ–¹æ³•"""
        print("=== å½’ä¸€åŒ–æŠ€æœ¯æ¯”è¾ƒ ===")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size, seq_len, d_model = 4, 10, 512
        x = torch.randn(batch_size, seq_len, d_model) * 2 + 1
        
        print("è¾“å…¥ç»Ÿè®¡:")
        print(f"å½¢çŠ¶: {x.shape}")
        print(f"å‡å€¼: {x.mean():.4f}")
        print(f"æ ‡å‡†å·®: {x.std():.4f}")
        print()
        
        # Layer Normalization
        layer_norm = nn.LayerNorm(d_model)
        x_ln = layer_norm(x)
        
        print("Layer Normalization:")
        print(f"å‡å€¼: {x_ln.mean():.4f}")
        print(f"æ ‡å‡†å·®: {x_ln.std():.4f}")
        print(f"æ¯ä¸ªæ ·æœ¬çš„å‡å€¼: {x_ln.mean(dim=-1).mean():.4f}")
        print()
        
        # Batch Normalization (éœ€è¦é‡æ–°æ’åˆ—ç»´åº¦)
        x_bn_input = x.transpose(1, 2)  # (batch, d_model, seq_len)
        batch_norm = nn.BatchNorm1d(d_model)
        x_bn = batch_norm(x_bn_input).transpose(1, 2)
        
        print("Batch Normalization:")
        print(f"å‡å€¼: {x_bn.mean():.4f}")
        print(f"æ ‡å‡†å·®: {x_bn.std():.4f}")
        print()
        
        # RMS Norm (Root Mean Square Layer Normalization)
        def rms_norm(x, eps=1e-6):
            return x / torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + eps)
        
        x_rms = rms_norm(x)
        print("RMS Normalization:")
        print(f"å‡å€¼: {x_rms.mean():.4f}")
        print(f"æ ‡å‡†å·®: {x_rms.std():.4f}")
        
        return x_ln, x_bn, x_rms
```

## 2. å¤§è¯­è¨€æ¨¡å‹æ¶æ„æ¼”è¿› ğŸ“ˆ

```python
class LLMArchitectureEvolution:
    """å¤§è¯­è¨€æ¨¡å‹æ¶æ„æ¼”è¿›"""
    
    def __init__(self):
        self.models = {
            "GPT-1": {"params": "117M", "layers": 12, "heads": 12, "d_model": 768},
            "GPT-2": {"params": "1.5B", "layers": 48, "heads": 25, "d_model": 1600},
            "GPT-3": {"params": "175B", "layers": 96, "heads": 96, "d_model": 12288},
            "GPT-4": {"params": "1.8T", "layers": "Unknown", "heads": "Unknown", "d_model": "Unknown"},
            "BERT-Base": {"params": "110M", "layers": 12, "heads": 12, "d_model": 768},
            "T5-Large": {"params": "770M", "layers": 24, "heads": 16, "d_model": 1024}
        }
    
    def scaling_laws_analysis(self):
        """ç¼©æ”¾å®šå¾‹åˆ†æ"""
        print("=== å¤§æ¨¡å‹ç¼©æ”¾å®šå¾‹ ===")
        
        # Kaplan et al. 2020 ç¼©æ”¾å®šå¾‹
        print("1. Kaplanç¼©æ”¾å®šå¾‹:")
        print("   L(N) = (Nc/N)^Î±N")
        print("   å…¶ä¸­:")
        print("   - L: æŸå¤±å‡½æ•°")
        print("   - N: æ¨¡å‹å‚æ•°é‡")
        print("   - Nc: ä¸´ç•Œå‚æ•°é‡")
        print("   - Î±N â‰ˆ 0.076 (å‚æ•°ç¼©æ”¾æŒ‡æ•°)")
        print()
        
        # Chinchillaç¼©æ”¾å®šå¾‹
        print("2. Chinchillaç¼©æ”¾å®šå¾‹ (Hoffmann et al. 2022):")
        print("   æœ€ä¼˜è®¡ç®—åˆ†é…ï¼šå‚æ•°å’Œæ•°æ®åº”è¯¥ç­‰æ¯”ä¾‹å¢é•¿")
        print("   Nopt âˆ C^a, Dopt âˆ C^b")
        print("   å…¶ä¸­ a â‰ˆ 0.5, b â‰ˆ 0.5")
        print()
        
        # å¯è§†åŒ–ç¼©æ”¾å…³ç³»
        params = np.logspace(6, 12, 50)  # ä»1Måˆ°1Tå‚æ•°
        loss_kaplan = 1.69 * (params / 8e6) ** (-0.076)
        
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.loglog(params, loss_kaplan, 'b-', linewidth=2, label='Kaplan Scaling Law')
        
        # æ ‡æ³¨å®é™…æ¨¡å‹
        model_params = [117e6, 1.5e9, 175e9]  # GPT-1, GPT-2, GPT-3
        model_names = ['GPT-1', 'GPT-2', 'GPT-3']
        for i, (param, name) in enumerate(zip(model_params, model_names)):
            loss_est = 1.69 * (param / 8e6) ** (-0.076)
            plt.scatter(param, loss_est, s=100, c='red', zorder=5)
            plt.annotate(name, (param, loss_est), xytext=(10, 10), 
                        textcoords='offset points', fontsize=10)
        
        plt.xlabel('Parameters')
        plt.ylabel('Cross-entropy Loss')
        plt.title('Model Scaling Laws')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # è®¡ç®—æˆæœ¬åˆ†æ
        plt.subplot(1, 2, 2)
        compute_costs = params * 1000  # å‡è®¾è®­ç»ƒæˆæœ¬ä¸å‚æ•°é‡çº¿æ€§ç›¸å…³
        performance = 1 / loss_kaplan  # æ€§èƒ½ä¸æŸå¤±æˆåæ¯”
        
        plt.loglog(compute_costs, performance, 'g-', linewidth=2, label='Performance vs Compute')
        plt.xlabel('Training Compute (FLOPS)')
        plt.ylabel('Performance (1/Loss)')
        plt.title('Compute Efficiency')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return params, loss_kaplan
    
    def architecture_comparison(self):
        """æ¶æ„æ¯”è¾ƒ"""
        print("=== æ¶æ„è®¾è®¡é€‰æ‹© ===")
        
        architectures = {
            "Encoder-Only (BERT)": {
                "ä¼˜ç‚¹": ["åŒå‘ä¸Šä¸‹æ–‡", "é€‚åˆç†è§£ä»»åŠ¡", "MASKæœºåˆ¶"],
                "ç¼ºç‚¹": ["ä¸é€‚åˆç”Ÿæˆ", "éœ€è¦ç‰¹æ®Šé¢„è®­ç»ƒ"],
                "åº”ç”¨": ["åˆ†ç±»", "é—®ç­”", "NER"]
            },
            "Decoder-Only (GPT)": {
                "ä¼˜ç‚¹": ["ç»Ÿä¸€æ¶æ„", "é€‚åˆç”Ÿæˆ", "ç®€å•é«˜æ•ˆ"],
                "ç¼ºç‚¹": ["å•å‘ä¸Šä¸‹æ–‡", "å¯èƒ½å¿½ç•¥åç»­ä¿¡æ¯"],
                "åº”ç”¨": ["æ–‡æœ¬ç”Ÿæˆ", "å¯¹è¯", "ä»£ç ç”Ÿæˆ"]
            },
            "Encoder-Decoder (T5)": {
                "ä¼˜ç‚¹": ["çµæ´»æ€§é«˜", "é€‚åˆseq2seq", "åŒå‘ç¼–ç "],
                "ç¼ºç‚¹": ["æ¶æ„å¤æ‚", "å‚æ•°å†—ä½™"],
                "åº”ç”¨": ["ç¿»è¯‘", "æ‘˜è¦", "é—®ç­”"]
            }
        }
        
        for arch, details in architectures.items():
            print(f"\n{arch}:")
            for key, values in details.items():
                print(f"  {key}: {', '.join(values)}")
        
        return architectures

class AttentionMechanisms:
    """æ³¨æ„åŠ›æœºåˆ¶å˜ä½“"""
    
    @staticmethod
    def attention_variants():
        """æ³¨æ„åŠ›æœºåˆ¶å˜ä½“åˆ†æ"""
        print("=== æ³¨æ„åŠ›æœºåˆ¶å˜ä½“ ===")
        
        variants = {
            "Multi-Head Attention": {
                "å¤æ‚åº¦": "O(nÂ²d)",
                "ä¼˜ç‚¹": "æ•è·ä¸åŒç±»å‹å…³ç³»",
                "ç¼ºç‚¹": "è®¡ç®—é‡å¤§"
            },
            "Sparse Attention": {
                "å¤æ‚åº¦": "O(nâˆšn d)",
                "ä¼˜ç‚¹": "é™ä½è®¡ç®—å¤æ‚åº¦",
                "ç¼ºç‚¹": "å¯èƒ½ä¸¢å¤±é•¿è·ç¦»ä¾èµ–"
            },
            "Linear Attention": {
                "å¤æ‚åº¦": "O(ndÂ²)",
                "ä¼˜ç‚¹": "çº¿æ€§å¤æ‚åº¦",
                "ç¼ºç‚¹": "è¿‘ä¼¼ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™"
            },
            "Flash Attention": {
                "å¤æ‚åº¦": "O(nÂ²d)",
                "ä¼˜ç‚¹": "å†…å­˜é«˜æ•ˆ",
                "ç¼ºç‚¹": "å®ç°å¤æ‚"
            }
        }
        
        for variant, details in variants.items():
            print(f"\n{variant}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # å¤æ‚åº¦æ¯”è¾ƒå¯è§†åŒ–
        n_values = np.logspace(2, 4, 50)  # åºåˆ—é•¿åº¦ä»100åˆ°10000
        d = 512  # æ¨¡å‹ç»´åº¦
        
        complexities = {
            "Standard": n_values**2 * d,
            "Sparse": n_values * np.sqrt(n_values) * d,
            "Linear": n_values * d**2
        }
        
        plt.figure(figsize=(10, 6))
        for name, complexity in complexities.items():
            plt.loglog(n_values, complexity, linewidth=2, label=f'{name} Attention')
        
        plt.xlabel('Sequence Length')
        plt.ylabel('Computational Complexity')
        plt.title('Attention Mechanism Complexity Comparison')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return variants
```

## 3. é¢„è®­ç»ƒç­–ç•¥ ğŸ¯

```python
class PretrainingStrategies:
    """é¢„è®­ç»ƒç­–ç•¥åˆ†æ"""
    
    def __init__(self):
        self.strategies = {}
    
    def language_modeling_objectives(self):
        """è¯­è¨€å»ºæ¨¡ç›®æ ‡å‡½æ•°"""
        print("=== è¯­è¨€å»ºæ¨¡ç›®æ ‡å‡½æ•° ===")
        
        objectives = {
            "Causal LM (GPT)": {
                "å…¬å¼": "L = -Î£ log P(x_i | x_<i)",
                "æè¿°": "ä»å·¦åˆ°å³é¢„æµ‹ä¸‹ä¸€ä¸ªtoken",
                "ä¼˜ç‚¹": "ç®€å•ã€é€‚åˆç”Ÿæˆ",
                "ç¼ºç‚¹": "åªåˆ©ç”¨å‰æ–‡ä¿¡æ¯"
            },
            "Masked LM (BERT)": {
                "å…¬å¼": "L = -Î£ log P(x_i | x_\\{i})",
                "æè¿°": "é¢„æµ‹è¢«maskçš„token",
                "ä¼˜ç‚¹": "åˆ©ç”¨åŒå‘ä¿¡æ¯",
                "ç¼ºç‚¹": "é¢„è®­ç»ƒ-å¾®è°ƒgap"
            },
            "Prefix LM (GLM)": {
                "å…¬å¼": "æ··åˆå› æœå’Œæ©ç å»ºæ¨¡",
                "æè¿°": "å‰ç¼€åŒå‘ï¼Œåç¼€å•å‘",
                "ä¼˜ç‚¹": "ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆ",
                "ç¼ºç‚¹": "è®­ç»ƒå¤æ‚"
            }
        }
        
        for obj, details in objectives.items():
            print(f"\n{obj}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        return objectives
    
    def training_dynamics_analysis(self):
        """è®­ç»ƒåŠ¨æ€åˆ†æ"""
        print("=== è®­ç»ƒåŠ¨æ€åˆ†æ ===")
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ›²çº¿
        epochs = np.linspace(0, 100, 1000)
        
        # Lossæ›²çº¿ï¼ˆä¸åŒé˜¶æ®µï¼‰
        def training_curve(epochs):
            """æ¨¡æ‹Ÿè®­ç»ƒæ›²çº¿"""
            # é˜¶æ®µ1ï¼šå¿«é€Ÿä¸‹é™
            phase1 = 5 * np.exp(-epochs * 0.2)
            # é˜¶æ®µ2ï¼šå¹³ç¨³ä¸‹é™
            phase2 = 2 * np.exp(-epochs * 0.02)
            # é˜¶æ®µ3ï¼šæ”¶æ•›
            phase3 = 0.5 * np.exp(-epochs * 0.001)
            
            return phase1 + phase2 + phase3 + np.random.normal(0, 0.05, len(epochs))
        
        train_loss = training_curve(epochs)
        val_loss = training_curve(epochs) + 0.2  # éªŒè¯æŸå¤±ç¨é«˜
        
        plt.figure(figsize=(15, 5))
        
        # è®­ç»ƒæ›²çº¿
        plt.subplot(1, 3, 1)
        plt.plot(epochs, train_loss, label='Training Loss', alpha=0.8)
        plt.plot(epochs, val_loss, label='Validation Loss', alpha=0.8)
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('Training Dynamics')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        plt.subplot(1, 3, 2)
        
        # Cosine Annealing
        lr_cosine = 0.5 * (1 + np.cos(np.pi * epochs / 100))
        
        # Warm-up + Cosine
        warmup_epochs = 10
        lr_warmup = np.minimum(epochs / warmup_epochs, 1.0)
        lr_schedule = lr_warmup * lr_cosine
        
        plt.plot(epochs, lr_schedule, label='Warmup + Cosine', linewidth=2)
        plt.xlabel('Epochs')
        plt.ylabel('Learning Rate')
        plt.title('Learning Rate Schedule')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Gradient Norm
        plt.subplot(1, 3, 3)
        grad_norm = 10 * np.exp(-epochs * 0.01) + np.random.exponential(2, len(epochs))
        grad_norm = np.clip(grad_norm, 0, 50)  # æ¢¯åº¦è£å‰ª
        
        plt.plot(epochs, grad_norm, alpha=0.7)
        plt.axhline(y=1.0, color='red', linestyle='--', label='Clip Threshold')
        plt.xlabel('Epochs')
        plt.ylabel('Gradient Norm')
        plt.title('Gradient Dynamics')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return train_loss, val_loss, lr_schedule

class TrainingOptimizations:
    """è®­ç»ƒä¼˜åŒ–æŠ€æœ¯"""
    
    @staticmethod
    def mixed_precision_training():
        """æ··åˆç²¾åº¦è®­ç»ƒ"""
        print("=== æ··åˆç²¾åº¦è®­ç»ƒ ===")
        
        print("FP16 vs FP32:")
        print("- FP32: 32ä½æµ®ç‚¹æ•°ï¼Œé«˜ç²¾åº¦ï¼Œå ç”¨å†…å­˜å¤§")
        print("- FP16: 16ä½æµ®ç‚¹æ•°ï¼Œä½ç²¾åº¦ï¼Œå ç”¨å†…å­˜å°")
        print("- æ··åˆç²¾åº¦: å‰å‘FP16ï¼Œåå‘FP32ï¼Œå…¼é¡¾é€Ÿåº¦å’Œç²¾åº¦")
        print()
        
        # æ•°å€¼èŒƒå›´æ¯”è¾ƒ
        fp32_range = (1.4e-45, 3.4e38)
        fp16_range = (5.96e-8, 65504)
        
        print(f"FP32 èŒƒå›´: {fp32_range[0]:.2e} ~ {fp32_range[1]:.2e}")
        print(f"FP16 èŒƒå›´: {fp16_range[0]:.2e} ~ {fp16_range[1]:.2e}")
        print()
        
        # Loss Scaling
        print("Loss ScalingæŠ€æœ¯:")
        print("- æ¢¯åº¦å€¼å¤ªå°å¯èƒ½ä¸‹æº¢ä¸º0")
        print("- å°†lossä¹˜ä»¥ç¼©æ”¾å› å­ï¼Œé˜²æ­¢æ¢¯åº¦ä¸‹æº¢")
        print("- åœ¨å‚æ•°æ›´æ–°å‰é™¤ä»¥ç¼©æ”¾å› å­æ¢å¤")
        
    @staticmethod
    def gradient_checkpointing():
        """æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        print("=== æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ ===")
        
        print("åŸç†:")
        print("- å‰å‘ä¼ æ’­æ—¶ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼")
        print("- åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—æ¿€æ´»å€¼")
        print("- ç”¨è®¡ç®—æ—¶é—´æ¢å†…å­˜ç©ºé—´")
        print()
        
        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        layers = np.arange(1, 49)  # 48å±‚Transformer
        
        # ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹ï¼šçº¿æ€§å¢é•¿
        memory_no_checkpoint = layers * 100  # MB per layer
        
        # ä½¿ç”¨æ£€æŸ¥ç‚¹ï¼šå¹³æ–¹æ ¹å¢é•¿
        memory_checkpoint = np.sqrt(layers) * 100
        
        plt.figure(figsize=(10, 6))
        plt.plot(layers, memory_no_checkpoint, label='No Checkpointing', linewidth=2)
        plt.plot(layers, memory_checkpoint, label='With Checkpointing', linewidth=2)
        plt.xlabel('Number of Layers')
        plt.ylabel('Memory Usage (MB)')
        plt.title('Memory Usage: Gradient Checkpointing')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return memory_no_checkpoint, memory_checkpoint

# æ¨¡å‹å¹¶è¡Œç­–ç•¥
class ModelParallelism:
    """æ¨¡å‹å¹¶è¡Œç­–ç•¥"""
    
    @staticmethod
    def parallelism_strategies():
        """å¹¶è¡Œç­–ç•¥åˆ†æ"""
        print("=== æ¨¡å‹å¹¶è¡Œç­–ç•¥ ===")
        
        strategies = {
            "æ•°æ®å¹¶è¡Œ (Data Parallel)": {
                "åŸç†": "ä¸åŒGPUå¤„ç†ä¸åŒæ‰¹æ¬¡æ•°æ®",
                "é€šä¿¡": "æ¢¯åº¦åŒæ­¥",
                "é€‚ç”¨": "å°æ¨¡å‹ï¼Œå¤§æ‰¹æ¬¡",
                "æ•ˆç‡": "é«˜ï¼ˆé€šä¿¡å°‘ï¼‰"
            },
            "æ¨¡å‹å¹¶è¡Œ (Model Parallel)": {
                "åŸç†": "æ¨¡å‹åˆ†å±‚æ”¾åœ¨ä¸åŒGPU",
                "é€šä¿¡": "æ¿€æ´»å€¼ä¼ é€’",
                "é€‚ç”¨": "å¤§æ¨¡å‹ï¼Œæ— æ³•æ”¾å…¥å•GPU",
                "æ•ˆç‡": "ä½ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰"
            },
            "æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallel)": {
                "åŸç†": "æ¨¡å‹åˆ†æ®µï¼Œæ‰¹æ¬¡æµæ°´æ‰§è¡Œ",
                "é€šä¿¡": "ä¸­é—´æ¿€æ´»å€¼",
                "é€‚ç”¨": "æ·±åº¦æ¨¡å‹",
                "æ•ˆç‡": "ä¸­ç­‰ï¼ˆæœ‰æ°”æ³¡ï¼‰"
            },
            "å¼ é‡å¹¶è¡Œ (Tensor Parallel)": {
                "åŸç†": "å•å±‚å†…éƒ¨å¹¶è¡ŒåŒ–",
                "é€šä¿¡": "All-Reduceæ“ä½œ",
                "é€‚ç”¨": "å®½ç½‘ç»œï¼ˆå¤§hidden sizeï¼‰",
                "æ•ˆç‡": "é«˜ï¼ˆå¹¶è¡Œåº¦é«˜ï¼‰"
            },
            "3Då¹¶è¡Œ": {
                "åŸç†": "ç»“åˆæ•°æ®ã€æµæ°´çº¿ã€å¼ é‡å¹¶è¡Œ",
                "é€šä¿¡": "å¤æ‚é€šä¿¡æ¨¡å¼",
                "é€‚ç”¨": "è¶…å¤§æ¨¡å‹",
                "æ•ˆç‡": "æœ€é«˜ï¼ˆå……åˆ†åˆ©ç”¨èµ„æºï¼‰"
            }
        }
        
        for strategy, details in strategies.items():
            print(f"\n{strategy}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        return strategies
```

## 4. æ¶Œç°èƒ½åŠ›åˆ†æ âœ¨

```python
class EmergentAbilities:
    """æ¶Œç°èƒ½åŠ›åˆ†æ"""
    
    def __init__(self):
        self.abilities = {
            "Few-shot Learning": "å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›",
            "Chain-of-Thought": "é“¾å¼æ¨ç†èƒ½åŠ›", 
            "In-context Learning": "ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›",
            "Code Generation": "ä»£ç ç”Ÿæˆèƒ½åŠ›",
            "Instruction Following": "æŒ‡ä»¤éµå¾ªèƒ½åŠ›"
        }
    
    def scaling_and_emergence(self):
        """ç¼©æ”¾ä¸æ¶Œç°å…³ç³»"""
        print("=== ç¼©æ”¾ä¸æ¶Œç°èƒ½åŠ› ===")
        
        # æ¨¡æ‹Ÿæ¶Œç°æ›²çº¿
        model_sizes = np.logspace(7, 11, 50)  # ä»10Måˆ°100Bå‚æ•°
        
        def emergence_curve(threshold, steepness=10):
            """æ¶Œç°èƒ½åŠ›æ›²çº¿"""
            return 1 / (1 + np.exp(-steepness * (np.log10(model_sizes) - np.log10(threshold))))
        
        # ä¸åŒèƒ½åŠ›çš„æ¶Œç°é˜ˆå€¼
        abilities_thresholds = {
            "Few-shot Learning": 1e9,     # 1Bå‚æ•°
            "Chain-of-Thought": 10e9,     # 10Bå‚æ•°
            "Code Generation": 50e9,      # 50Bå‚æ•°
            "Complex Reasoning": 100e9    # 100Bå‚æ•°
        }
        
        plt.figure(figsize=(12, 8))
        
        colors = ['blue', 'green', 'red', 'purple']
        for i, (ability, threshold) in enumerate(abilities_thresholds.items()):
            performance = emergence_curve(threshold) * 100
            plt.plot(model_sizes, performance, linewidth=3, 
                    label=ability, color=colors[i])
            
            # æ ‡æ³¨æ¶Œç°ç‚¹
            plt.axvline(x=threshold, color=colors[i], linestyle='--', alpha=0.7)
        
        plt.xscale('log')
        plt.xlabel('Model Parameters')
        plt.ylabel('Performance (%)')
        plt.title('Emergent Abilities vs Model Scale')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ¨¡å‹æ ‡æ³¨
        model_points = {
            "GPT-1": 117e6,
            "GPT-2": 1.5e9, 
            "GPT-3": 175e9,
            "GPT-4": 1.8e12
        }
        
        for model, params in model_points.items():
            plt.axvline(x=params, color='black', linestyle=':', alpha=0.5)
            plt.text(params, 50, model, rotation=90, ha='right', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        return model_sizes, abilities_thresholds
    
    def in_context_learning_analysis(self):
        """ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†æ"""
        print("=== ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ ===")
        
        print("ä¸Šä¸‹æ–‡å­¦ä¹ ç‰¹ç‚¹:")
        print("1. æ— éœ€å‚æ•°æ›´æ–°")
        print("2. åŸºäºæ¼”ç¤ºæ ·æœ¬å­¦ä¹ ")
        print("3. æ€§èƒ½éšæ ·æœ¬æ•°å¢é•¿")
        print("4. å¯¹æ ·æœ¬é¡ºåºæ•æ„Ÿ")
        print()
        
        # æ¨¡æ‹ŸICLæ€§èƒ½æ›²çº¿
        n_shots = np.arange(0, 33)
        
        # ä¸åŒæ¨¡å‹å¤§å°çš„ICLèƒ½åŠ›
        model_sizes = [1e9, 10e9, 100e9, 1e12]  # 1B, 10B, 100B, 1T
        
        plt.figure(figsize=(10, 6))
        
        for size in model_sizes:
            # æ€§èƒ½éšshotså¢é•¿ï¼Œä½†è¾¹é™…æ•ˆåº”é€’å‡
            performance = 60 * (1 - np.exp(-n_shots * np.log10(size) / 20))
            # æ·»åŠ å™ªå£°
            performance += np.random.normal(0, 2, len(n_shots))
            performance = np.clip(performance, 0, 100)
            
            plt.plot(n_shots, performance, 'o-', linewidth=2, 
                    label=f'{size:.0e} params', markersize=4)
        
        plt.xlabel('Number of Shots')
        plt.ylabel('Performance (%)')
        plt.title('In-Context Learning Performance')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return n_shots, model_sizes
```

## 5. ç†è®ºåŸºç¡€ä¸æœªæ¥æ–¹å‘ ğŸ”®

```python
class TheoreticalFoundations:
    """ç†è®ºåŸºç¡€"""
    
    @staticmethod
    def attention_theory():
        """æ³¨æ„åŠ›æœºåˆ¶çš„ç†è®ºè§£é‡Š"""
        print("=== æ³¨æ„åŠ›æœºåˆ¶ç†è®ºåŸºç¡€ ===")
        
        theories = {
            "Memory Mechanism": {
                "æè¿°": "æ³¨æ„åŠ›ä½œä¸ºå¯å¾®åˆ†çš„è®°å¿†è®¿é—®æœºåˆ¶",
                "å…³é”®ç‚¹": "è½¯å¯»å€ã€å†…å®¹å¯»å€ã€è”æƒ³è®°å¿†"
            },
            "Kernel Method": {
                "æè¿°": "æ³¨æ„åŠ›ç­‰ä»·äºæ ¸æ–¹æ³•ä¸­çš„ç›¸ä¼¼æ€§åº¦é‡",
                "å…³é”®ç‚¹": "é«˜æ–¯æ ¸ã€RBFæ ¸ã€å†…ç§¯æ ¸"
            },
            "Graph Neural Network": {
                "æè¿°": "è‡ªæ³¨æ„åŠ›ç­‰ä»·äºå…¨è¿æ¥å›¾ä¸Šçš„æ¶ˆæ¯ä¼ é€’",
                "å…³é”®ç‚¹": "å›¾å·ç§¯ã€æ¶ˆæ¯èšåˆã€èŠ‚ç‚¹æ›´æ–°"
            },
            "Optimization Perspective": {
                "æè¿°": "æ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºä¼˜åŒ–é—®é¢˜çš„è§£",
                "å…³é”®ç‚¹": "æœ€ä¼˜ä¼ è¾“ã€ç†µæ­£åˆ™åŒ–ã€Sinkhornè¿­ä»£"
            }
        }
        
        for theory, details in theories.items():
            print(f"\n{theory}:")
            print(f"  æè¿°: {details['æè¿°']}")
            print(f"  å…³é”®ç‚¹: {details['å…³é”®ç‚¹']}")
        
        return theories
    
    @staticmethod
    def universality_theory():
        """é€šç”¨æ€§ç†è®º"""
        print("=== Transformeré€šç”¨æ€§ç†è®º ===")
        
        print("é€šç”¨è¿‘ä¼¼å®šç† (Universal Approximation):")
        print("- Transformerå¯ä»¥è¿‘ä¼¼ä»»æ„åºåˆ—åˆ°åºåˆ—å‡½æ•°")
        print("- è¶³å¤Ÿå®½çš„Transformerå…·æœ‰é€šç”¨æ€§")
        print("- æ·±åº¦å½±å“è¡¨è¾¾æ•ˆç‡")
        print()
        
        print("è¡¨è¾¾èƒ½åŠ›åˆ†æ:")
        print("- å•å¤´æ³¨æ„åŠ›: è¡¨è¾¾æœ‰é™æ¨¡å¼")
        print("- å¤šå¤´æ³¨æ„åŠ›: å¹¶è¡Œå¤„ç†å¤šç§å…³ç³»")
        print("- æ·±å±‚ç½‘ç»œ: ç»„åˆå¤æ‚æ¨¡å¼")
        print()
        
        print("è®¡ç®—å¤æ‚æ€§:")
        print("- æ—¶é—´å¤æ‚åº¦: O(nÂ²d + ndÂ²)")
        print("- ç©ºé—´å¤æ‚åº¦: O(nÂ²) (attention map)")
        print("- å¹¶è¡Œåº¦: é«˜ï¼ˆçŸ©é˜µæ“ä½œï¼‰")

class FutureDirections:
    """æœªæ¥å‘å±•æ–¹å‘"""
    
    @staticmethod
    def research_frontiers():
        """ç ”ç©¶å‰æ²¿"""
        print("=== å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶å‰æ²¿ ===")
        
        frontiers = {
            "æ¶æ„åˆ›æ–°": [
                "Mixture of Experts (MoE)",
                "State Space Models (Mamba)",
                "Retrieval-Augmented Architecture",
                "Sparse Transformers",
                "Compositional Architectures"
            ],
            "è®­ç»ƒæ•ˆç‡": [
                "Parameter-Efficient Fine-tuning",
                "Progressive Training",
                "Curriculum Learning",
                "Meta-Learning",
                "Continual Learning"
            ],
            "èƒ½åŠ›å¢å¼º": [
                "Multimodal Integration",
                "Tool Use and API Calling",
                "Planning and Reasoning", 
                "Memory and Knowledge Update",
                "Causal Understanding"
            ],
            "å®‰å…¨å¯¹é½": [
                "RLHF (Reinforcement Learning from Human Feedback)",
                "Constitutional AI",
                "Interpretability and Explainability",
                "Robustness and Adversarial Defense",
                "Value Alignment"
            ]
        }
        
        for category, topics in frontiers.items():
            print(f"\n{category}:")
            for topic in topics:
                print(f"  â€¢ {topic}")
        
        return frontiers
    
    @staticmethod
    def scaling_future():
        """ç¼©æ”¾çš„æœªæ¥"""
        print("=== ç¼©æ”¾æ³•åˆ™çš„æœªæ¥ ===")
        
        print("ç¡¬ä»¶é™åˆ¶:")
        print("- æ‘©å°”å®šå¾‹æ”¾ç¼“")
        print("- å†…å­˜å¢™é—®é¢˜")
        print("- åŠŸè€—é™åˆ¶")
        print()
        
        print("æ–°çš„ç¼©æ”¾æ–¹å‘:")
        print("- æ•°æ®è´¨é‡ > æ•°æ®é‡")
        print("- ç®—æ³•æ•ˆç‡ > æš´åŠ›è®¡ç®—")
        print("- ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿ")
        print("- åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–")
        print()
        
        print("åç¼©æ”¾æ—¶ä»£:")
        print("- å‚æ•°æ•ˆç‡ä¼˜åŒ–")
        print("- çŸ¥è¯†è’¸é¦å‹ç¼©")
        print("- ä¸ªæ€§åŒ–å°æ¨¡å‹")
        print("- æ¨¡å‹ç»„åˆé›†æˆ")

def comprehensive_summary():
    """ç»¼åˆæ€»ç»“"""
    print("=== å¤§è¯­è¨€æ¨¡å‹ç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "æ ¸å¿ƒç»„ä»¶": {
            "Self-Attention": "æ•è·åºåˆ—å†…ä¾èµ–å…³ç³»",
            "Position Encoding": "æä¾›ä½ç½®ä¿¡æ¯",
            "Feed-Forward Network": "å¢åŠ éçº¿æ€§å˜æ¢",
            "Layer Normalization": "ç¨³å®šè®­ç»ƒè¿‡ç¨‹"
        },
        
        "å…³é”®æŠ€æœ¯": {
            "Transformeræ¶æ„": "ç»Ÿä¸€ç¼–ç è§£ç æ¡†æ¶",
            "é¢„è®­ç»ƒ-å¾®è°ƒ": "é€šç”¨è¡¨ç¤ºå­¦ä¹ ",
            "ç¼©æ”¾å®šå¾‹": "æ€§èƒ½ä¸è§„æ¨¡å…³ç³»",
            "æ¶Œç°èƒ½åŠ›": "è§„æ¨¡é©±åŠ¨çš„è´¨å˜"
        },
        
        "è®­ç»ƒç­–ç•¥": {
            "è¯­è¨€å»ºæ¨¡": "è‡ªç›‘ç£å­¦ä¹ ç›®æ ‡",
            "æ··åˆç²¾åº¦": "åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹",
            "æ¢¯åº¦æ£€æŸ¥ç‚¹": "èŠ‚çœå†…å­˜ä½¿ç”¨",
            "æ¨¡å‹å¹¶è¡Œ": "çªç ´ç¡¬ä»¶é™åˆ¶"
        },
        
        "æœªæ¥è¶‹åŠ¿": {
            "å¤šæ¨¡æ€èåˆ": "è§†è§‰è¯­è¨€ç»Ÿä¸€",
            "å·¥å…·ä½¿ç”¨": "å¤–éƒ¨èƒ½åŠ›æ‰©å±•",
            "æ¨ç†è§„åˆ’": "é«˜å±‚è®¤çŸ¥èƒ½åŠ›",
            "å®‰å…¨å¯¹é½": "äººç±»ä»·å€¼ä¸€è‡´"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("å¤§è¯­è¨€æ¨¡å‹æ¶æ„ä¸è®­ç»ƒç†è®ºæŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Vaswani et al. (2017): "Attention Is All You Need"
- Kaplan et al. (2020): "Scaling Laws for Neural Language Models"  
- Brown et al. (2020): "Language Models are Few-Shot Learners"
- Hoffmann et al. (2022): "Training Compute-Optimal Large Language Models"
- Wei et al. (2022): "Emergent Abilities of Large Language Models"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [ä¼˜åŒ–ç®—æ³•ç†è®º](optimization_theory.md) - æ·±å…¥ç†è§£è®­ç»ƒä¼˜åŒ–
- [NLPç†è®ºåŸºç¡€](nlp_theory.md) - è‡ªç„¶è¯­è¨€å¤„ç†ç†è®º
- [æ¨¡å‹è§£é‡Š](../../ml/model_interpretation.md) - å¯è§£é‡Šæ€§åˆ†æ