# å¼ºåŒ–å­¦ä¹ ç†è®ºï¼šæ™ºèƒ½å†³ç­–çš„æ•°å­¦åŸºç¡€ ğŸ®

æ·±å…¥ç†è§£å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç†è®ºã€ç®—æ³•å’Œåº”ç”¨ã€‚

## 1. å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¡†æ¶ ğŸ—ï¸

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd
from collections import defaultdict, deque
import random
import warnings
warnings.filterwarnings('ignore')

class ReinforcementLearningFramework:
    """å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¡†æ¶"""
    
    def __init__(self):
        self.components = {}
    
    def mdp_framework(self):
        """é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¡†æ¶"""
        print("=== é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP) ===")
        
        print("MDPå®šä¹‰: M = (S, A, P, R, Î³)")
        print("å…¶ä¸­:")
        print("- S: çŠ¶æ€ç©ºé—´")
        print("- A: åŠ¨ä½œç©ºé—´") 
        print("- P: çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a)")
        print("- R: å¥–åŠ±å‡½æ•° R(s,a,s')")
        print("- Î³: æŠ˜æ‰£å› å­ [0,1]")
        print()
        
        print("é©¬å°”å¯å¤«æ€§è´¨:")
        print("P(S_{t+1}|S_t, A_t, S_{t-1}, A_{t-1}, ...) = P(S_{t+1}|S_t, A_t)")
        print("å³ï¼šæœªæ¥åªä¾èµ–äºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä¸å†å²æ— å…³")
        print()
        
        # MDPç»„ä»¶åˆ†æ
        components = {
            "çŠ¶æ€ç©ºé—´": {
                "ç±»å‹": ["ç¦»æ•£", "è¿ç»­", "æ··åˆ"],
                "ç‰¹ç‚¹": "å®Œå…¨å¯è§‚æµ‹ã€éƒ¨åˆ†å¯è§‚æµ‹",
                "ç¤ºä¾‹": "æ£‹ç›˜ä½ç½®ã€æœºå™¨äººåæ ‡ã€è‚¡ä»·"
            },
            "åŠ¨ä½œç©ºé—´": {
                "ç±»å‹": ["ç¦»æ•£", "è¿ç»­"],
                "çº¦æŸ": "åŠ¨ä½œå¯è¡Œæ€§ã€ç‰©ç†é™åˆ¶",
                "ç¤ºä¾‹": "è±¡æ£‹èµ°æ³•ã€æ–¹å‘ç›˜è½¬è§’ã€æŠ•èµ„æ¯”ä¾‹"
            },
            "å¥–åŠ±å‡½æ•°": {
                "è®¾è®¡": "ç¨ å¯†å¥–åŠ± vs ç¨€ç–å¥–åŠ±",
                "é—®é¢˜": "å¥–åŠ±å¡‘å½¢ã€å»¶è¿Ÿå¥–åŠ±",
                "ç¤ºä¾‹": "æ¸¸æˆå¾—åˆ†ã€äº¤æ˜“æ”¶ç›Šã€ä»»åŠ¡å®Œæˆ"
            },
            "è½¬ç§»æ¦‚ç‡": {
                "ç¡®å®šæ€§": "ç¡®å®šæ€§ vs éšæœºæ€§",
                "å»ºæ¨¡": "å·²çŸ¥æ¨¡å‹ vs å…æ¨¡å‹",
                "ç¤ºä¾‹": "è§„åˆ™æ¸¸æˆã€ç°å®ç‰©ç†ã€å¸‚åœºåŠ¨æ€"
            }
        }
        
        for component, details in components.items():
            print(f"{component}:")
            for key, value in details.items():
                if isinstance(value, list):
                    print(f"  {key}: {', '.join(value)}")
                else:
                    print(f"  {key}: {value}")
            print()
        
        # å¯è§†åŒ–MDP
        self.visualize_mdp_concept()
        
        return components
    
    def visualize_mdp_concept(self):
        """å¯è§†åŒ–MDPæ¦‚å¿µ"""
        print("=== MDPå¯è§†åŒ– ===")
        
        # åˆ›å»ºç®€å•çš„ç½‘æ ¼ä¸–ç•ŒMDP
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. çŠ¶æ€ç©ºé—´ï¼ˆç½‘æ ¼ä¸–ç•Œï¼‰
        grid = np.zeros((4, 4))
        grid[0, 3] = 1  # ç›®æ ‡çŠ¶æ€
        grid[1, 1] = -1  # é™·é˜±çŠ¶æ€
        grid[2, 2] = -1  # é™·é˜±çŠ¶æ€
        
        im1 = axes[0, 0].imshow(grid, cmap='RdYlGn', interpolation='nearest')
        axes[0, 0].set_title('çŠ¶æ€ç©ºé—´\n(ç»¿è‰²=ç›®æ ‡, çº¢è‰²=é™·é˜±)')
        axes[0, 0].set_xticks(range(4))
        axes[0, 0].set_yticks(range(4))
        
        # æ ‡æ³¨çŠ¶æ€
        for i in range(4):
            for j in range(4):
                if grid[i, j] == 1:
                    axes[0, 0].text(j, i, 'G', ha='center', va='center', fontsize=16, color='white')
                elif grid[i, j] == -1:
                    axes[0, 0].text(j, i, 'T', ha='center', va='center', fontsize=16, color='white')
                else:
                    axes[0, 0].text(j, i, f'({i},{j})', ha='center', va='center', fontsize=8)
        
        # 2. åŠ¨ä½œç©ºé—´ï¼ˆç®­å¤´ï¼‰
        actions = ['â†‘', 'â†’', 'â†“', 'â†']
        action_effects = [(-1, 0), (0, 1), (1, 0), (0, -1)]
        
        axes[0, 1].set_xlim(-0.5, 3.5)
        axes[0, 1].set_ylim(-0.5, 3.5)
        
        # åœ¨ä¸­å¿ƒä½ç½®æ˜¾ç¤ºå¯èƒ½çš„åŠ¨ä½œ
        center = (1.5, 1.5)
        for i, (action, (di, dj)) in enumerate(zip(actions, action_effects)):
            start_x, start_y = center
            end_x, end_y = start_x + dj * 0.3, start_y + di * 0.3
            axes[0, 1].arrow(start_x, start_y, end_x - start_x, end_y - start_y,
                            head_width=0.1, head_length=0.1, fc='blue', ec='blue')
            axes[0, 1].text(end_x + dj * 0.2, end_y + di * 0.2, action,
                           ha='center', va='center', fontsize=12)
        
        axes[0, 1].set_title('åŠ¨ä½œç©ºé—´\n(å››ä¸ªæ–¹å‘ç§»åŠ¨)')
        axes[0, 1].set_aspect('equal')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å¥–åŠ±å‡½æ•°
        rewards = np.array([
            [-0.1, -0.1, -0.1, +10],
            [-0.1, -10, -0.1, -0.1],
            [-0.1, -0.1, -10, -0.1],
            [-0.1, -0.1, -0.1, -0.1]
        ])
        
        im3 = axes[1, 0].imshow(rewards, cmap='RdYlGn', interpolation='nearest')
        axes[1, 0].set_title('å¥–åŠ±å‡½æ•°')
        
        for i in range(4):
            for j in range(4):
                axes[1, 0].text(j, i, f'{rewards[i, j]:.1f}', 
                               ha='center', va='center', 
                               color='white' if abs(rewards[i, j]) > 5 else 'black')
        
        # 4. ä»·å€¼å‡½æ•°ç¤ºä¾‹
        # ç®€åŒ–çš„ä»·å€¼å‡½æ•°ï¼ˆæ‰‹å·¥è®¾å®šç”¨äºæ¼”ç¤ºï¼‰
        values = np.array([
            [6.8, 7.7, 8.8, 10.0],
            [6.1, 0.0, 7.9, 8.8],
            [5.5, 6.1, 0.0, 7.9],
            [4.9, 5.5, 6.1, 7.0]
        ])
        
        im4 = axes[1, 1].imshow(values, cmap='Blues', interpolation='nearest')
        axes[1, 1].set_title('çŠ¶æ€ä»·å€¼å‡½æ•° V(s)')
        
        for i in range(4):
            for j in range(4):
                axes[1, 1].text(j, i, f'{values[i, j]:.1f}', 
                               ha='center', va='center', color='white')
        
        plt.tight_layout()
        plt.show()
    
    def value_functions(self):
        """ä»·å€¼å‡½æ•°ç†è®º"""
        print("=== ä»·å€¼å‡½æ•°ç†è®º ===")
        
        print("çŠ¶æ€ä»·å€¼å‡½æ•°:")
        print("V^Ï€(s) = E_Ï€[G_t | S_t = s]")
        print("      = E_Ï€[Î£_{k=0}^âˆ Î³^k R_{t+k+1} | S_t = s]")
        print()
        
        print("åŠ¨ä½œä»·å€¼å‡½æ•°:")
        print("Q^Ï€(s,a) = E_Ï€[G_t | S_t = s, A_t = a]")
        print("         = E_Ï€[Î£_{k=0}^âˆ Î³^k R_{t+k+1} | S_t = s, A_t = a]")
        print()
        
        print("Bellmanæ–¹ç¨‹:")
        print("V^Ï€(s) = Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]")
        print("Q^Ï€(s,a) = Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³ Î£_{a'} Ï€(a'|s')Q^Ï€(s',a')]")
        print()
        
        print("æœ€ä¼˜æ€§æ–¹ç¨‹:")
        print("V*(s) = max_a Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V*(s')]")
        print("Q*(s,a) = Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³ max_{a'} Q*(s',a')]")
        
        # ä»·å€¼å‡½æ•°è®¡ç®—ç¤ºä¾‹
        self.demonstrate_value_iteration()
        
        return self.policy_types()
    
    def demonstrate_value_iteration(self):
        """æ¼”ç¤ºä»·å€¼è¿­ä»£ç®—æ³•"""
        print("\n=== ä»·å€¼è¿­ä»£ç®—æ³•æ¼”ç¤º ===")
        
        # ç®€å•çš„ç½‘æ ¼ä¸–ç•Œ
        # çŠ¶æ€: (row, col), åŠ¨ä½œ: 0=ä¸Š, 1=å³, 2=ä¸‹, 3=å·¦
        rows, cols = 3, 4
        n_states = rows * cols
        n_actions = 4
        
        # çŠ¶æ€ç¼–ç 
        def state_to_idx(r, c):
            return r * cols + c
        
        def idx_to_state(idx):
            return idx // cols, idx % cols
        
        # è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±
        def get_transitions(state, action):
            r, c = state
            # åŠ¨ä½œæ•ˆæœ
            moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # ä¸Šå³ä¸‹å·¦
            dr, dc = moves[action]
            
            # æ–°ä½ç½®
            new_r, new_c = r + dr, c + dc
            
            # è¾¹ç•Œæ£€æŸ¥
            if new_r < 0 or new_r >= rows or new_c < 0 or new_c >= cols:
                new_r, new_c = r, c  # æ’å¢™ä¸åŠ¨
            
            # å¥–åŠ±
            if (new_r, new_c) == (0, 3):  # ç›®æ ‡ä½ç½®
                reward = 10
            elif (new_r, new_c) == (1, 1):  # é™·é˜±
                reward = -10
            else:
                reward = -0.1  # ç”Ÿå­˜æƒ©ç½š
            
            return (new_r, new_c), reward
        
        # ä»·å€¼è¿­ä»£
        gamma = 0.9
        theta = 1e-6
        V = np.zeros(n_states)
        
        iteration = 0
        delta_history = []
        
        while True:
            delta = 0
            V_new = np.zeros(n_states)
            
            for s in range(n_states):
                state = idx_to_state(s)
                
                # è·³è¿‡ç»ˆæ­¢çŠ¶æ€
                if state == (0, 3) or state == (1, 1):
                    V_new[s] = V[s]
                    continue
                
                # è®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„ä»·å€¼
                action_values = []
                for a in range(n_actions):
                    next_state, reward = get_transitions(state, a)
                    next_s = state_to_idx(*next_state)
                    action_value = reward + gamma * V[next_s]
                    action_values.append(action_value)
                
                V_new[s] = max(action_values)
                delta = max(delta, abs(V_new[s] - V[s]))
            
            V = V_new
            delta_history.append(delta)
            iteration += 1
            
            if delta < theta:
                break
        
        print(f"ä»·å€¼è¿­ä»£æ”¶æ•›ï¼Œè¿­ä»£æ¬¡æ•°: {iteration}")
        
        # å¯è§†åŒ–ç»“æœ
        self.visualize_value_iteration_results(V, rows, cols, delta_history)
        
        return V, iteration
    
    def visualize_value_iteration_results(self, V, rows, cols, delta_history):
        """å¯è§†åŒ–ä»·å€¼è¿­ä»£ç»“æœ"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # ä»·å€¼å‡½æ•°å¯è§†åŒ–
        V_grid = V.reshape(rows, cols)
        im = axes[0].imshow(V_grid, cmap='coolwarm', interpolation='nearest')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(rows):
            for j in range(cols):
                axes[0].text(j, i, f'{V_grid[i, j]:.2f}', 
                           ha='center', va='center', 
                           color='white' if abs(V_grid[i, j]) > 3 else 'black')
        
        axes[0].set_title('æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•° V*(s)')
        axes[0].set_xticks(range(cols))
        axes[0].set_yticks(range(rows))
        plt.colorbar(im, ax=axes[0])
        
        # æ”¶æ•›è¿‡ç¨‹
        axes[1].plot(delta_history, 'b-', linewidth=2)
        axes[1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[1].set_ylabel('æœ€å¤§ä»·å€¼å˜åŒ– Î´')
        axes[1].set_title('ä»·å€¼è¿­ä»£æ”¶æ•›è¿‡ç¨‹')
        axes[1].set_yscale('log')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def policy_types(self):
        """ç­–ç•¥ç±»å‹"""
        print("\n=== ç­–ç•¥ç±»å‹ ===")
        
        policy_types = {
            "ç¡®å®šæ€§ç­–ç•¥": {
                "å®šä¹‰": "Ï€(s) = a",
                "ç‰¹ç‚¹": "æ¯ä¸ªçŠ¶æ€å¯¹åº”å”¯ä¸€åŠ¨ä½œ",
                "ä¼˜ç‚¹": "ç®€å•ã€æ‰§è¡Œæ•ˆç‡é«˜",
                "ç¼ºç‚¹": "ç¼ºä¹æ¢ç´¢èƒ½åŠ›"
            },
            "éšæœºç­–ç•¥": {
                "å®šä¹‰": "Ï€(a|s) = P(A_t=a | S_t=s)",
                "ç‰¹ç‚¹": "ç»™å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ",
                "ä¼˜ç‚¹": "è‡ªç„¶çš„æ¢ç´¢æœºåˆ¶",
                "ç¼ºç‚¹": "æ‰§è¡Œæœ‰éšæœºæ€§"
            },
            "Îµ-è´ªå¿ƒç­–ç•¥": {
                "å®šä¹‰": "Ï€(a|s) = 1-Îµ+Îµ/|A| if a=a*, Îµ/|A| otherwise",
                "ç‰¹ç‚¹": "å¹³è¡¡åˆ©ç”¨å’Œæ¢ç´¢",
                "ä¼˜ç‚¹": "ç®€å•æœ‰æ•ˆçš„æ¢ç´¢",
                "ç¼ºç‚¹": "æ¢ç´¢æ˜¯å‡åŒ€çš„"
            },
            "Softmaxç­–ç•¥": {
                "å®šä¹‰": "Ï€(a|s) = exp(Q(s,a)/Ï„) / Î£_b exp(Q(s,b)/Ï„)",
                "ç‰¹ç‚¹": "æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§",
                "ä¼˜ç‚¹": "åå‘é«˜ä»·å€¼åŠ¨ä½œ",
                "ç¼ºç‚¹": "éœ€è¦è°ƒèŠ‚æ¸©åº¦å‚æ•°"
            }
        }
        
        for policy_type, details in policy_types.items():
            print(f"{policy_type}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        return policy_types

class DynamicProgramming:
    """åŠ¨æ€è§„åˆ’æ–¹æ³•"""
    
    def __init__(self):
        pass
    
    def policy_evaluation(self):
        """ç­–ç•¥è¯„ä¼°"""
        print("=== ç­–ç•¥è¯„ä¼° (Policy Evaluation) ===")
        
        print("ç›®æ ‡: ç»™å®šç­–ç•¥Ï€ï¼Œè®¡ç®—çŠ¶æ€ä»·å€¼å‡½æ•°V^Ï€(s)")
        print()
        print("è¿­ä»£æ›´æ–°å…¬å¼:")
        print("V_{k+1}(s) = Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V_k(s')]")
        print()
        print("æ”¶æ•›æ€§: åœ¨Î³ < 1æ—¶ï¼ŒV_kæ”¶æ•›åˆ°V^Ï€")
        
        # ç­–ç•¥è¯„ä¼°å®ç°ç¤ºä¾‹
        return self.implement_policy_evaluation()
    
    def implement_policy_evaluation(self):
        """å®ç°ç­–ç•¥è¯„ä¼°"""
        print("\n=== ç­–ç•¥è¯„ä¼°å®ç° ===")
        
        # ä½¿ç”¨ç®€å•çš„2x2ç½‘æ ¼ä¸–ç•Œ
        # çŠ¶æ€: 0=(0,0), 1=(0,1), 2=(1,0), 3=(1,1)
        # çŠ¶æ€3æ˜¯ç»ˆæ­¢çŠ¶æ€ï¼ˆç›®æ ‡ï¼‰
        
        n_states = 4
        n_actions = 4  # ä¸Šå³ä¸‹å·¦
        gamma = 0.9
        
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„éšæœºç­–ç•¥
        policy = np.ones((n_states, n_actions)) / n_actions
        
        # è½¬ç§»æ¦‚ç‡çŸ©é˜µ P[s][a][s'] å’Œå¥–åŠ±çŸ©é˜µ R[s][a][s']
        P = np.zeros((n_states, n_actions, n_states))
        R = np.zeros((n_states, n_actions, n_states))
        
        # å®šä¹‰è½¬ç§»å’Œå¥–åŠ±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        # åŠ¨ä½œ: 0=ä¸Š, 1=å³, 2=ä¸‹, 3=å·¦
        transitions = {
            0: {0: 0, 1: 1, 2: 2, 3: 0},  # ä»çŠ¶æ€0
            1: {0: 1, 1: 1, 2: 3, 3: 0},  # ä»çŠ¶æ€1
            2: {0: 0, 1: 3, 2: 2, 3: 2},  # ä»çŠ¶æ€2
            3: {0: 3, 1: 3, 2: 3, 3: 3}   # ä»çŠ¶æ€3ï¼ˆç»ˆæ­¢çŠ¶æ€ï¼‰
        }
        
        # è®¾ç½®è½¬ç§»æ¦‚ç‡
        for s in range(n_states):
            for a in range(n_actions):
                next_s = transitions[s][a]
                P[s, a, next_s] = 1.0
                
                # å¥–åŠ±è®¾ç½®
                if next_s == 3:  # åˆ°è¾¾ç›®æ ‡
                    R[s, a, next_s] = 10.0
                else:
                    R[s, a, next_s] = -0.1
        
        # ç­–ç•¥è¯„ä¼°è¿­ä»£
        V = np.zeros(n_states)
        theta = 1e-6
        
        iteration = 0
        delta_history = []
        V_history = [V.copy()]
        
        while True:
            delta = 0
            V_new = np.zeros(n_states)
            
            for s in range(n_states):
                if s == 3:  # ç»ˆæ­¢çŠ¶æ€
                    V_new[s] = 0
                    continue
                
                v = 0
                for a in range(n_actions):
                    for s_prime in range(n_states):
                        v += policy[s, a] * P[s, a, s_prime] * \
                             (R[s, a, s_prime] + gamma * V[s_prime])
                
                V_new[s] = v
                delta = max(delta, abs(V_new[s] - V[s]))
            
            V = V_new
            delta_history.append(delta)
            V_history.append(V.copy())
            iteration += 1
            
            if delta < theta:
                break
        
        print(f"ç­–ç•¥è¯„ä¼°æ”¶æ•›ï¼Œè¿­ä»£æ¬¡æ•°: {iteration}")
        print(f"æœ€ç»ˆä»·å€¼å‡½æ•°: {V}")
        
        # å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹
        self.visualize_policy_evaluation(V_history, delta_history)
        
        return V, iteration
    
    def visualize_policy_evaluation(self, V_history, delta_history):
        """å¯è§†åŒ–ç­–ç•¥è¯„ä¼°è¿‡ç¨‹"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # ä»·å€¼å‡½æ•°æ”¶æ•›è¿‡ç¨‹
        V_array = np.array(V_history)
        for s in range(V_array.shape[1]):
            if s < 3:  # éç»ˆæ­¢çŠ¶æ€
                axes[0].plot(V_array[:, s], label=f'State {s}', linewidth=2)
        
        axes[0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0].set_ylabel('çŠ¶æ€ä»·å€¼')
        axes[0].set_title('ç­–ç•¥è¯„ä¼°æ”¶æ•›è¿‡ç¨‹')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # æœ€å¤§å˜åŒ–é‡
        axes[1].plot(delta_history, 'r-', linewidth=2)
        axes[1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[1].set_ylabel('æœ€å¤§ä»·å€¼å˜åŒ– Î´')
        axes[1].set_title('æ”¶æ•›é€Ÿåº¦')
        axes[1].set_yscale('log')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def policy_improvement(self):
        """ç­–ç•¥æ”¹è¿›"""
        print("=== ç­–ç•¥æ”¹è¿› (Policy Improvement) ===")
        
        print("ç­–ç•¥æ”¹è¿›å®šç†:")
        print("å¯¹äºæ‰€æœ‰çŠ¶æ€sï¼Œå¦‚æœQ^Ï€(s,Ï€'(s)) â‰¥ V^Ï€(s)ï¼Œ")
        print("åˆ™Ï€' â‰¥ Ï€ï¼ˆÏ€'ä¸å·®äºÏ€ï¼‰")
        print()
        print("è´ªå¿ƒç­–ç•¥æ”¹è¿›:")
        print("Ï€'(s) = argmax_a Q^Ï€(s,a)")
        print("      = argmax_a Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]")
        
        return self.policy_iteration_algorithm()
    
    def policy_iteration_algorithm(self):
        """ç­–ç•¥è¿­ä»£ç®—æ³•"""
        print("\n=== ç­–ç•¥è¿­ä»£ç®—æ³• ===")
        
        print("ç®—æ³•æµç¨‹:")
        print("1. åˆå§‹åŒ–: ä»»æ„ç­–ç•¥Ï€_0")
        print("2. ç­–ç•¥è¯„ä¼°: è®¡ç®—V^Ï€_k")
        print("3. ç­–ç•¥æ”¹è¿›: Ï€_{k+1}(s) = argmax_a Q^Ï€_k(s,a)")
        print("4. é‡å¤2-3ç›´åˆ°ç­–ç•¥æ”¶æ•›")
        print()
        
        # å®ç°ç­–ç•¥è¿­ä»£
        return self.implement_policy_iteration()
    
    def implement_policy_iteration(self):
        """å®ç°ç­–ç•¥è¿­ä»£ç®—æ³•"""
        print("=== ç­–ç•¥è¿­ä»£å®ç° ===")
        
        # ä½¿ç”¨ä¹‹å‰çš„ç½‘æ ¼ä¸–ç•Œè®¾ç½®
        n_states = 4
        n_actions = 4
        gamma = 0.9
        
        # è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±ï¼ˆå¤ç”¨ä¹‹å‰çš„å®šä¹‰ï¼‰
        P = np.zeros((n_states, n_actions, n_states))
        R = np.zeros((n_states, n_actions, n_states))
        
        transitions = {
            0: {0: 0, 1: 1, 2: 2, 3: 0},
            1: {0: 1, 1: 1, 2: 3, 3: 0},
            2: {0: 0, 1: 3, 2: 2, 3: 2},
            3: {0: 3, 1: 3, 2: 3, 3: 3}
        }
        
        for s in range(n_states):
            for a in range(n_actions):
                next_s = transitions[s][a]
                P[s, a, next_s] = 1.0
                R[s, a, next_s] = 10.0 if next_s == 3 else -0.1
        
        # åˆå§‹åŒ–éšæœºç­–ç•¥
        policy = np.random.randint(0, n_actions, n_states)
        policy[3] = 0  # ç»ˆæ­¢çŠ¶æ€åŠ¨ä½œæ— å…³ç´§è¦
        
        policy_stable = False
        iteration = 0
        policy_history = [policy.copy()]
        value_history = []
        
        while not policy_stable:
            iteration += 1
            
            # ç­–ç•¥è¯„ä¼°
            V = np.zeros(n_states)
            theta = 1e-6
            
            while True:
                delta = 0
                V_new = np.zeros(n_states)
                
                for s in range(n_states):
                    if s == 3:
                        V_new[s] = 0
                        continue
                    
                    a = policy[s]
                    v = 0
                    for s_prime in range(n_states):
                        v += P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])
                    
                    V_new[s] = v
                    delta = max(delta, abs(V_new[s] - V[s]))
                
                V = V_new
                if delta < theta:
                    break
            
            value_history.append(V.copy())
            
            # ç­–ç•¥æ”¹è¿›
            policy_stable = True
            new_policy = np.zeros(n_states, dtype=int)
            
            for s in range(n_states):
                if s == 3:
                    new_policy[s] = policy[s]
                    continue
                
                # è®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„ä»·å€¼
                action_values = np.zeros(n_actions)
                for a in range(n_actions):
                    for s_prime in range(n_states):
                        action_values[a] += P[s, a, s_prime] * \
                                          (R[s, a, s_prime] + gamma * V[s_prime])
                
                new_policy[s] = np.argmax(action_values)
                
                if new_policy[s] != policy[s]:
                    policy_stable = False
            
            policy = new_policy
            policy_history.append(policy.copy())
        
        print(f"ç­–ç•¥è¿­ä»£æ”¶æ•›ï¼Œè¿­ä»£æ¬¡æ•°: {iteration}")
        print(f"æœ€ä¼˜ç­–ç•¥: {policy}")
        print(f"æœ€ä¼˜ä»·å€¼å‡½æ•°: {V}")
        
        # å¯è§†åŒ–ç­–ç•¥è¿­ä»£è¿‡ç¨‹
        self.visualize_policy_iteration(policy_history, value_history)
        
        return policy, V, iteration
    
    def visualize_policy_iteration(self, policy_history, value_history):
        """å¯è§†åŒ–ç­–ç•¥è¿­ä»£è¿‡ç¨‹"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # ç­–ç•¥å˜åŒ–
        policy_array = np.array(policy_history)
        action_names = ['â†‘', 'â†’', 'â†“', 'â†']
        
        axes[0, 0].set_title('ç­–ç•¥æ¼”åŒ–è¿‡ç¨‹')
        for s in range(3):  # åªæ˜¾ç¤ºéç»ˆæ­¢çŠ¶æ€
            policy_s = policy_array[:, s]
            axes[0, 0].plot(policy_s, 'o-', label=f'State {s}', linewidth=2, markersize=8)
        
        axes[0, 0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 0].set_ylabel('é€‰æ‹©çš„åŠ¨ä½œ')
        axes[0, 0].set_yticks(range(4))
        axes[0, 0].set_yticklabels(action_names)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # ä»·å€¼å‡½æ•°å˜åŒ–
        if value_history:
            value_array = np.array(value_history)
            axes[0, 1].set_title('ä»·å€¼å‡½æ•°æ¼”åŒ–')
            for s in range(3):
                axes[0, 1].plot(value_array[:, s], 'o-', label=f'State {s}', linewidth=2)
            
            axes[0, 1].set_xlabel('è¿­ä»£æ¬¡æ•°')
            axes[0, 1].set_ylabel('çŠ¶æ€ä»·å€¼')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # æœ€ç»ˆç­–ç•¥å¯è§†åŒ–
        final_policy = policy_history[-1]
        policy_grid = np.array([final_policy[0], final_policy[1], 
                               final_policy[2], final_policy[3]]).reshape(2, 2)
        
        axes[1, 0].set_title('æœ€ç»ˆç­–ç•¥')
        for i in range(2):
            for j in range(2):
                state_idx = i * 2 + j
                if state_idx == 3:
                    axes[1, 0].text(j, i, 'GOAL', ha='center', va='center', 
                                   fontsize=12, color='red')
                else:
                    action = final_policy[state_idx]
                    axes[1, 0].text(j, i, action_names[action], ha='center', va='center', 
                                   fontsize=16, color='blue')
        
        axes[1, 0].set_xlim(-0.5, 1.5)
        axes[1, 0].set_ylim(-0.5, 1.5)
        axes[1, 0].set_xticks([0, 1])
        axes[1, 0].set_yticks([0, 1])
        axes[1, 0].grid(True)
        
        # æœ€ç»ˆä»·å€¼å‡½æ•°
        if value_history:
            final_values = value_history[-1]
            value_grid = final_values.reshape(2, 2)
            
            im = axes[1, 1].imshow(value_grid, cmap='Blues', interpolation='nearest')
            axes[1, 1].set_title('æœ€ç»ˆä»·å€¼å‡½æ•°')
            
            for i in range(2):
                for j in range(2):
                    axes[1, 1].text(j, i, f'{value_grid[i, j]:.2f}', 
                                   ha='center', va='center', color='white')
            
            plt.colorbar(im, ax=axes[1, 1])
        
        plt.tight_layout()
        plt.show()

class TemporalDifferenceLearning:
    """æ—¶åºå·®åˆ†å­¦ä¹ """
    
    def __init__(self):
        pass
    
    def td_learning_concept(self):
        """TDå­¦ä¹ æ¦‚å¿µ"""
        print("=== æ—¶åºå·®åˆ†å­¦ä¹  (Temporal Difference Learning) ===")
        
        print("æ ¸å¿ƒæ€æƒ³:")
        print("- ç»“åˆè’™ç‰¹å¡æ´›æ–¹æ³•å’ŒåŠ¨æ€è§„åˆ’çš„ä¼˜ç‚¹")
        print("- æ— éœ€ç­‰å¾…episodeç»“æŸå³å¯æ›´æ–°")
        print("- åˆ©ç”¨bootstrapï¼ˆè‡ªä¸¾ï¼‰æ›´æ–°ä»·å€¼ä¼°è®¡")
        print()
        
        print("TDè¯¯å·®:")
        print("Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)")
        print("å…¶ä¸­:")
        print("- R_{t+1} + Î³V(S_{t+1}): TDç›®æ ‡")
        print("- V(S_t): å½“å‰ä¼°è®¡")
        print("- Î´_t: TDè¯¯å·®ï¼ˆæ—¶åºå·®åˆ†è¯¯å·®ï¼‰")
        print()
        
        print("TD(0)æ›´æ–°è§„åˆ™:")
        print("V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]")
        print("V(S_t) â† V(S_t) + Î±Î´_t")
        
        # TDå­¦ä¹ vså…¶ä»–æ–¹æ³•æ¯”è¾ƒ
        self.compare_learning_methods()
        
        return self.implement_td_zero()
    
    def compare_learning_methods(self):
        """æ¯”è¾ƒä¸åŒå­¦ä¹ æ–¹æ³•"""
        print("\n=== å­¦ä¹ æ–¹æ³•æ¯”è¾ƒ ===")
        
        methods = {
            "è’™ç‰¹å¡æ´›": {
                "æ›´æ–°æ—¶æœº": "episodeç»“æŸå",
                "ç›®æ ‡": "å®é™…å›æŠ¥G_t",
                "æ–¹å·®": "é«˜ï¼ˆçœŸå®å›æŠ¥æœ‰å™ªå£°ï¼‰",
                "åå·®": "æ— å",
                "æ”¶æ•›": "ä¿è¯æ”¶æ•›åˆ°V^Ï€"
            },
            "TD(0)": {
                "æ›´æ–°æ—¶æœº": "æ¯æ­¥æ›´æ–°",
                "ç›®æ ‡": "R_{t+1} + Î³V(S_{t+1})",
                "æ–¹å·®": "ä½ï¼ˆåªæœ‰ä¸€æ­¥å™ªå£°ï¼‰",
                "åå·®": "æœ‰åï¼ˆbootstrapï¼‰",
                "æ”¶æ•›": "æ”¶æ•›åˆ°V^Ï€"
            },
            "åŠ¨æ€è§„åˆ’": {
                "æ›´æ–°æ—¶æœº": "å…¨çŠ¶æ€æ‰«æ",
                "ç›®æ ‡": "æœŸæœ›å›æŠ¥ï¼ˆéœ€è¦æ¨¡å‹ï¼‰",
                "æ–¹å·®": "æ— ",
                "åå·®": "æ— åï¼ˆæœ‰æ¨¡å‹ï¼‰",
                "æ”¶æ•›": "å¿«é€Ÿæ”¶æ•›"
            }
        }
        
        for method, details in methods.items():
            print(f"{method}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # å¯è§†åŒ–æ¯”è¾ƒ
        self.visualize_method_comparison()
        
        return methods
    
    def visualize_method_comparison(self):
        """å¯è§†åŒ–æ–¹æ³•æ¯”è¾ƒ"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # æ¨¡æ‹Ÿå­¦ä¹ æ›²çº¿
        episodes = np.arange(1, 101)
        
        # è’™ç‰¹å¡æ´›ï¼šé«˜æ–¹å·®ï¼Œæ— å
        mc_curve = 1 - 0.5 * np.exp(-episodes/30) + np.random.normal(0, 0.1, 100)
        
        # TDå­¦ä¹ ï¼šä½æ–¹å·®ï¼ŒåˆæœŸæœ‰å
        td_curve = 1 - 0.7 * np.exp(-episodes/20) + np.random.normal(0, 0.05, 100)
        
        # DPï¼šå¿«é€Ÿæ”¶æ•›ï¼ˆå¦‚æœæœ‰æ¨¡å‹ï¼‰
        dp_curve = 1 - 0.9 * np.exp(-episodes/10) + np.random.normal(0, 0.02, 100)
        
        # å­¦ä¹ æ›²çº¿
        axes[0].plot(episodes, mc_curve, label='Monte Carlo', alpha=0.7, linewidth=2)
        axes[0].plot(episodes, td_curve, label='TD(0)', alpha=0.7, linewidth=2)
        axes[0].plot(episodes, dp_curve, label='Dynamic Programming', alpha=0.7, linewidth=2)
        axes[0].axhline(y=1, color='black', linestyle='--', alpha=0.5, label='True Value')
        
        axes[0].set_xlabel('Episodes')
        axes[0].set_ylabel('Value Estimate')
        axes[0].set_title('Learning Curves Comparison')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # æ–¹å·®å¯¹æ¯”
        methods = ['Monte Carlo', 'TD(0)', 'DP']
        variances = [0.1, 0.05, 0.02]
        
        bars = axes[1].bar(methods, variances, color=['orange', 'blue', 'green'], alpha=0.7)
        axes[1].set_ylabel('Variance')
        axes[1].set_title('Variance Comparison')
        
        for bar, var in zip(bars, variances):
            height = bar.get_height()
            axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.005,
                        f'{var:.3f}', ha='center', va='bottom')
        
        # æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        convergence_rates = [30, 20, 10]  # æ—¶é—´å¸¸æ•°
        
        bars = axes[2].bar(methods, convergence_rates, color=['orange', 'blue', 'green'], alpha=0.7)
        axes[2].set_ylabel('Convergence Time Constant')
        axes[2].set_title('Convergence Speed')
        
        for bar, rate in zip(bars, convergence_rates):
            height = bar.get_height()
            axes[2].text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{rate}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
    
    def implement_td_zero(self):
        """å®ç°TD(0)ç®—æ³•"""
        print("\n=== TD(0)ç®—æ³•å®ç° ===")
        
        # éšæœºæ¸¸èµ°é—®é¢˜
        # çŠ¶æ€: 0, 1, 2, 3, 4, 5, 6
        # èµ·å§‹çŠ¶æ€: 3, ç»ˆæ­¢çŠ¶æ€: 0, 6
        # åŠ¨ä½œ: å·¦(-1), å³(+1)
        # å¥–åŠ±: åˆ°è¾¾çŠ¶æ€6å¾—+1ï¼Œåˆ°è¾¾çŠ¶æ€0å¾—0ï¼Œå…¶ä»–ä¸º0
        
        n_states = 7
        start_state = 3
        alpha = 0.1  # å­¦ä¹ ç‡
        gamma = 1.0  # æ— æŠ˜æ‰£
        
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = np.zeros(n_states)
        V[0] = 0  # ç»ˆæ­¢çŠ¶æ€
        V[6] = 0  # ç»ˆæ­¢çŠ¶æ€
        
        # çœŸå®ä»·å€¼å‡½æ•°ï¼ˆè§£æè§£ï¼‰
        true_V = np.array([0, 1/6, 2/6, 3/6, 4/6, 5/6, 0])
        
        n_episodes = 1000
        V_history = []
        
        for episode in range(n_episodes):
            state = start_state
            
            while state != 0 and state != 6:
                # éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆå·¦æˆ–å³ï¼‰
                action = np.random.choice([-1, 1])
                next_state = state + action
                
                # è®¡ç®—å¥–åŠ±
                if next_state == 6:
                    reward = 1
                else:
                    reward = 0
                
                # TDæ›´æ–°
                td_error = reward + gamma * V[next_state] - V[state]
                V[state] += alpha * td_error
                
                state = next_state
            
            # è®°å½•ä»·å€¼å‡½æ•°
            if episode % 100 == 0:
                V_history.append(V.copy())
        
        print(f"TD(0)å­¦ä¹ å®Œæˆï¼Œepisodes: {n_episodes}")
        print(f"çœŸå®ä»·å€¼å‡½æ•°: {true_V}")
        print(f"å­¦ä¹ ä»·å€¼å‡½æ•°: {V}")
        print(f"å‡æ–¹è¯¯å·®: {np.mean((V - true_V)**2):.4f}")
        
        # å¯è§†åŒ–TDå­¦ä¹ è¿‡ç¨‹
        self.visualize_td_learning(V_history, true_V)
        
        return V, V_history
    
    def visualize_td_learning(self, V_history, true_V):
        """å¯è§†åŒ–TDå­¦ä¹ è¿‡ç¨‹"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # ä»·å€¼å‡½æ•°å­¦ä¹ è¿‡ç¨‹
        states = range(len(true_V))
        
        for i, V in enumerate(V_history):
            alpha_val = 0.3 + 0.7 * i / len(V_history)
            axes[0].plot(states, V, 'b-', alpha=alpha_val, linewidth=1)
        
        axes[0].plot(states, true_V, 'r-', linewidth=3, label='True Value Function')
        axes[0].plot(states, V_history[-1], 'g--', linewidth=2, label='Learned Value Function')
        
        axes[0].set_xlabel('State')
        axes[0].set_ylabel('Value')
        axes[0].set_title('TD(0) Learning Process')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # è¯¯å·®æ”¶æ•›
        errors = [np.mean((V - true_V)**2) for V in V_history]
        episodes = np.arange(0, len(V_history) * 100, 100)
        
        axes[1].plot(episodes, errors, 'b-', linewidth=2)
        axes[1].set_xlabel('Episodes')
        axes[1].set_ylabel('Mean Squared Error')
        axes[1].set_title('Learning Error')
        axes[1].set_yscale('log')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def comprehensive_rl_theory_summary():
    """å¼ºåŒ–å­¦ä¹ ç†è®ºç»¼åˆæ€»ç»“"""
    print("=== å¼ºåŒ–å­¦ä¹ ç†è®ºç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "ç†è®ºåŸºç¡€": {
            "MDPæ¡†æ¶": "çŠ¶æ€ã€åŠ¨ä½œã€è½¬ç§»ã€å¥–åŠ±ã€æŠ˜æ‰£",
            "ä»·å€¼å‡½æ•°": "çŠ¶æ€ä»·å€¼V(s)ã€åŠ¨ä½œä»·å€¼Q(s,a)",
            "Bellmanæ–¹ç¨‹": "é€’å½’å…³ç³»ã€æœ€ä¼˜æ€§æ–¹ç¨‹",
            "ç­–ç•¥": "ç¡®å®šæ€§ã€éšæœºæ€§ã€æœ€ä¼˜ç­–ç•¥"
        },
        
        "æ±‚è§£æ–¹æ³•": {
            "åŠ¨æ€è§„åˆ’": "éœ€è¦æ¨¡å‹ã€ç²¾ç¡®è§£ã€è®¡ç®—å¤æ‚",
            "è’™ç‰¹å¡æ´›": "å…æ¨¡å‹ã€é«˜æ–¹å·®ã€éœ€è¦å®Œæ•´episode",
            "æ—¶åºå·®åˆ†": "å…æ¨¡å‹ã€ä½æ–¹å·®ã€åœ¨çº¿å­¦ä¹ ",
            "å‡½æ•°è¿‘ä¼¼": "å¤„ç†å¤§çŠ¶æ€ç©ºé—´ã€ç¥ç»ç½‘ç»œ"
        },
        
        "æ ¸å¿ƒç®—æ³•": {
            "ä»·å€¼è¿­ä»£": "V_{k+1} = max_a E[R + Î³V_k]",
            "ç­–ç•¥è¿­ä»£": "è¯„ä¼°+æ”¹è¿›ã€ä¿è¯æ”¶æ•›",
            "TD(0)": "V(s) â† V(s) + Î±[R + Î³V(s') - V(s)]",
            "Q-Learning": "Q(s,a) â† Q(s,a) + Î±[R + Î³max Q(s',a') - Q(s,a)]"
        },
        
        "æ¢ç´¢ç­–ç•¥": {
            "Îµ-è´ªå¿ƒ": "å¹³è¡¡åˆ©ç”¨å’Œæ¢ç´¢",
            "Softmax": "åŸºäºä»·å€¼çš„æ¦‚ç‡é€‰æ‹©",
            "UCB": "ç½®ä¿¡åº¦ä¸Šç•Œã€ç†è®ºä¿è¯",
            "Thompsoné‡‡æ ·": "è´å¶æ–¯æ–¹æ³•ã€è‡ªç„¶æ¢ç´¢"
        },
        
        "ç°ä»£å‘å±•": {
            "æ·±åº¦å¼ºåŒ–å­¦ä¹ ": "DQNã€Policy Gradientã€Actor-Critic",
            "å¤šæ™ºèƒ½ä½“": "åˆä½œã€ç«äº‰ã€é€šä¿¡",
            "å…ƒå­¦ä¹ ": "å­¦ä¼šå­¦ä¹ ã€å¿«é€Ÿé€‚åº”",
            "å®‰å…¨å¼ºåŒ–å­¦ä¹ ": "çº¦æŸã€é£é™©æ„ŸçŸ¥"
        },
        
        "åº”ç”¨é¢†åŸŸ": {
            "æ¸¸æˆAI": "Atariã€å›´æ£‹ã€æ˜Ÿé™…äº‰éœ¸",
            "æœºå™¨äººæ§åˆ¶": "å¯¼èˆªã€æ“ä½œã€æ­¥æ€",
            "æ¨èç³»ç»Ÿ": "ä¸ªæ€§åŒ–ã€é•¿æœŸä¼˜åŒ–",
            "é‡‘èäº¤æ˜“": "ç®—æ³•äº¤æ˜“ã€é£é™©ç®¡ç†",
            "è‡ªåŠ¨é©¾é©¶": "è·¯å¾„è§„åˆ’ã€å†³ç­–åˆ¶å®š"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("å¼ºåŒ–å­¦ä¹ ç†è®ºåŸºç¡€æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Sutton & Barto (2018): "Reinforcement Learning: An Introduction"
- Bellman (1957): "Dynamic Programming"
- Watkins (1989): "Learning from Delayed Rewards"
- Kaelbling et al. (1996): "Reinforcement Learning: A Survey"
- Mnih et al. (2015): "Human-level control through deep reinforcement learning"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [æ·±åº¦å¼ºåŒ–å­¦ä¹ ](deep_rl_theory.md) - DQNã€Policy Gradient
- [å¤šæ™ºèƒ½ä½“ç†è®º](multi_agent_theory.md) - åšå¼ˆè®ºä¸åˆä½œ
- [å…ƒå­¦ä¹ ç†è®º](meta_learning_theory.md) - å­¦ä¼šå­¦ä¹ 