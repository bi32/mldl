# è‡ªç„¶è¯­è¨€å¤„ç†ç†è®ºåŸºç¡€ ğŸ“

æ·±å…¥ç†è§£è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¸å¿ƒç†è®ºã€æ–¹æ³•å’Œæœ€æ–°å‘å±•ã€‚

## 1. è¯­è¨€å­¦åŸºç¡€ ğŸ”¤

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import re
import math
from scipy.spatial.distance import cosine
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import nltk
from wordcloud import WordCloud

class LinguisticFoundations:
    """è¯­è¨€å­¦åŸºç¡€ç†è®º"""
    
    def __init__(self):
        self.linguistic_levels = {
            "è¯­éŸ³å­¦/éŸ³éŸµå­¦": "ç ”ç©¶è¯­è¨€çš„å£°éŸ³ç³»ç»Ÿ",
            "å½¢æ€å­¦": "ç ”ç©¶è¯æ±‡çš„å†…éƒ¨ç»“æ„",
            "å¥æ³•å­¦": "ç ”ç©¶å¥å­çš„ç»“æ„è§„åˆ™",
            "è¯­ä¹‰å­¦": "ç ”ç©¶æ„ä¹‰å’Œå«ä¹‰",
            "è¯­ç”¨å­¦": "ç ”ç©¶è¯­è¨€çš„ä½¿ç”¨æ–¹å¼",
            "è¯è¯­åˆ†æ": "ç ”ç©¶æ–‡æœ¬å’Œå¯¹è¯çš„ç»“æ„"
        }
    
    def morphology_analysis(self):
        """å½¢æ€å­¦åˆ†æ"""
        print("=== å½¢æ€å­¦åˆ†æ ===")
        
        morphology_concepts = {
            "è¯ç´ (Morpheme)": {
                "å®šä¹‰": "æœ€å°çš„æœ‰æ„ä¹‰è¯­è¨€å•ä½",
                "ç±»å‹": ["è‡ªç”±è¯ç´ ", "ç»‘å®šè¯ç´ "],
                "ç¤ºä¾‹": "un-happy-ness (3ä¸ªè¯ç´ )"
            },
            "è¯æ ¹(Root)": {
                "å®šä¹‰": "è¯æ±‡çš„æ ¸å¿ƒéƒ¨åˆ†",
                "åŠŸèƒ½": "æ‰¿è½½ä¸»è¦æ„ä¹‰",
                "ç¤ºä¾‹": "walk in walking, walked"
            },
            "è¯ç¼€(Affix)": {
                "å®šä¹‰": "é™„åŠ åˆ°è¯æ ¹çš„ç»‘å®šè¯ç´ ",
                "ç±»å‹": ["å‰ç¼€", "åç¼€", "ä¸­ç¼€"],
                "ç¤ºä¾‹": "un-(å‰ç¼€), -ing(åç¼€)"
            }
        }
        
        for concept, details in morphology_concepts.items():
            print(f"\n{concept}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # å®é™…åˆ†æç¤ºä¾‹
        words = ["unhappiness", "reconstruction", "uncomfortable"]
        print("\nå½¢æ€å­¦åˆ†è§£ç¤ºä¾‹:")
        
        morpheme_analysis = {
            "unhappiness": ["un-", "happy", "-ness"],
            "reconstruction": ["re-", "construct", "-ion"], 
            "uncomfortable": ["un-", "comfort", "-able"]
        }
        
        for word, morphemes in morpheme_analysis.items():
            print(f"{word}: {' + '.join(morphemes)}")
        
        return morphology_concepts
    
    def syntax_analysis(self):
        """å¥æ³•åˆ†æ"""
        print("=== å¥æ³•å­¦ç†è®º ===")
        
        syntax_theories = {
            "çŸ­è¯­ç»“æ„è¯­æ³•": {
                "æ ¸å¿ƒæ€æƒ³": "å¥å­ç”±åµŒå¥—çš„çŸ­è¯­æ„æˆ",
                "è§„åˆ™å½¢å¼": "S â†’ NP VP",
                "ä¼˜ç‚¹": "å½¢å¼åŒ–ç¨‹åº¦é«˜",
                "ç¼ºç‚¹": "éš¾ä»¥å¤„ç†å¤æ‚ç°è±¡"
            },
            "ä¾å­˜è¯­æ³•": {
                "æ ¸å¿ƒæ€æƒ³": "è¯æ±‡ä¹‹é—´å­˜åœ¨ä¾å­˜å…³ç³»",
                "è¡¨ç¤ºæ–¹å¼": "æœ‰å‘å›¾ç»“æ„",
                "ä¼˜ç‚¹": "è·¨è¯­è¨€é€‚ç”¨æ€§å¼º",
                "ç¼ºç‚¹": "ç¼ºä¹å±‚æ¬¡ç»“æ„ä¿¡æ¯"
            },
            "è½¬æ¢ç”Ÿæˆè¯­æ³•": {
                "æ ¸å¿ƒæ€æƒ³": "æ·±å±‚ç»“æ„é€šè¿‡è½¬æ¢å¾—åˆ°è¡¨å±‚ç»“æ„",
                "å…³é”®æ¦‚å¿µ": "æ·±å±‚ç»“æ„ã€è¡¨å±‚ç»“æ„ã€è½¬æ¢è§„åˆ™",
                "ä¼˜ç‚¹": "è§£é‡Šè¯­è¨€åˆ›é€ æ€§",
                "ç¼ºç‚¹": "è§„åˆ™å¤æ‚ï¼Œéš¾ä»¥è®¡ç®—"
            }
        }
        
        for theory, details in syntax_theories.items():
            print(f"\n{theory}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # å¯è§†åŒ–å¥æ³•æ ‘
        self.visualize_syntax_tree()
        
        return syntax_theories
    
    def visualize_syntax_tree(self):
        """å¯è§†åŒ–å¥æ³•æ ‘"""
        print("\nå¥æ³•æ ‘ç¤ºä¾‹: 'The cat sat on the mat'")
        
        # ç®€å•çš„å¥æ³•æ ‘å¯è§†åŒ–
        tree_structure = """
                    S
                   / \\
                  NP  VP
                  |   /|\\
                 Det V PP
                  |  |  /|\\
                The cat P NP
                       |  /|\\
                      on Det N
                          |   |
                         the mat
        """
        print(tree_structure)
        
        # ä¾å­˜å…³ç³»ç¤ºä¾‹
        print("\nä¾å­˜å…³ç³»ç¤ºä¾‹:")
        dependencies = [
            ("sat", "root", "ROOT"),
            ("cat", "nsubj", "sat"),
            ("The", "det", "cat"),
            ("on", "prep", "sat"),
            ("mat", "pobj", "on"),
            ("the", "det", "mat")
        ]
        
        for word, relation, head in dependencies:
            print(f"{word} --{relation}--> {head}")
    
    def semantics_analysis(self):
        """è¯­ä¹‰å­¦åˆ†æ"""
        print("=== è¯­ä¹‰å­¦ç†è®º ===")
        
        semantic_theories = {
            "å½¢å¼è¯­ä¹‰å­¦": {
                "æ–¹æ³•": "é€»è¾‘å’Œæ•°å­¦å·¥å…·",
                "æ ¸å¿ƒæ¦‚å¿µ": "çœŸå€¼æ¡ä»¶ã€å¯èƒ½ä¸–ç•Œ",
                "åº”ç”¨": "é€»è¾‘æ¨ç†ã€é—®ç­”ç³»ç»Ÿ"
            },
            "åˆ†å¸ƒå¼è¯­ä¹‰å­¦": {
                "æ–¹æ³•": "ç»Ÿè®¡å­¦ä¹ ",
                "æ ¸å¿ƒå‡è®¾": "ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„è¯æ±‡æœ‰ç›¸ä¼¼å«ä¹‰",
                "åº”ç”¨": "è¯å‘é‡ã€è¯­è¨€æ¨¡å‹"
            },
            "è®¤çŸ¥è¯­ä¹‰å­¦": {
                "æ–¹æ³•": "è®¤çŸ¥ç§‘å­¦",
                "æ ¸å¿ƒæ¦‚å¿µ": "æ¦‚å¿µéšå–»ã€æ„è±¡å›¾å¼",
                "åº”ç”¨": "å¸¸è¯†æ¨ç†ã€æ¦‚å¿µè¡¨ç¤º"
            }
        }
        
        for theory, details in semantic_theories.items():
            print(f"\n{theory}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        return semantic_theories

class LanguageModeling:
    """è¯­è¨€å»ºæ¨¡ç†è®º"""
    
    def __init__(self):
        self.models = {}
    
    def ngram_theory(self):
        """N-gramè¯­è¨€æ¨¡å‹ç†è®º"""
        print("=== N-gramè¯­è¨€æ¨¡å‹ ===")
        
        print("åŸºæœ¬æ€æƒ³:")
        print("P(wâ‚, wâ‚‚, ..., wâ‚™) â‰ˆ âˆáµ¢ P(wáµ¢|wáµ¢â‚‹â‚™â‚Šâ‚, ..., wáµ¢â‚‹â‚)")
        print()
        
        # N-gramæ¨¡å‹æ¯”è¾ƒ
        ngram_types = {
            "Unigram": {
                "å…¬å¼": "P(wâ‚, wâ‚‚, ..., wâ‚™) = âˆáµ¢ P(wáµ¢)",
                "å‡è®¾": "è¯æ±‡ç‹¬ç«‹",
                "ä¼˜ç‚¹": "ç®€å•ï¼Œè®¡ç®—å¿«",
                "ç¼ºç‚¹": "å¿½ç•¥ä¸Šä¸‹æ–‡"
            },
            "Bigram": {
                "å…¬å¼": "P(wâ‚, wâ‚‚, ..., wâ‚™) = âˆáµ¢ P(wáµ¢|wáµ¢â‚‹â‚)",
                "å‡è®¾": "é©¬å°”ç§‘å¤«å‡è®¾(n=1)",
                "ä¼˜ç‚¹": "è€ƒè™‘å±€éƒ¨ä¸Šä¸‹æ–‡",
                "ç¼ºç‚¹": "ä¸Šä¸‹æ–‡çª—å£å°"
            },
            "Trigram": {
                "å…¬å¼": "P(wâ‚, wâ‚‚, ..., wâ‚™) = âˆáµ¢ P(wáµ¢|wáµ¢â‚‹â‚‚, wáµ¢â‚‹â‚)",
                "å‡è®¾": "é©¬å°”ç§‘å¤«å‡è®¾(n=2)",
                "ä¼˜ç‚¹": "æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯",
                "ç¼ºç‚¹": "æ•°æ®ç¨€ç–æ€§"
            }
        }
        
        for model_type, details in ngram_types.items():
            print(f"\n{model_type}æ¨¡å‹:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # å®ç°ç®€å•çš„bigramæ¨¡å‹
        self.implement_bigram_model()
        
        return ngram_types
    
    def implement_bigram_model(self):
        """å®ç°bigramæ¨¡å‹"""
        print("\n=== Bigramæ¨¡å‹å®ç°ç¤ºä¾‹ ===")
        
        # ç¤ºä¾‹è¯­æ–™
        corpus = [
            "the cat sat on the mat",
            "the dog ran in the park",
            "a cat and a dog played together"
        ]
        
        # æ„å»ºbigramè®¡æ•°
        bigram_counts = defaultdict(lambda: defaultdict(int))
        unigram_counts = defaultdict(int)
        
        for sentence in corpus:
            words = ["<s>"] + sentence.lower().split() + ["</s>"]
            
            for i in range(len(words)):
                unigram_counts[words[i]] += 1
                if i > 0:
                    bigram_counts[words[i-1]][words[i]] += 1
        
        # è®¡ç®—æ¦‚ç‡
        def bigram_probability(w1, w2):
            if unigram_counts[w1] == 0:
                return 0
            return bigram_counts[w1][w2] / unigram_counts[w1]
        
        # ç¤ºä¾‹è®¡ç®—
        print("Bigramæ¦‚ç‡ç¤ºä¾‹:")
        test_pairs = [("the", "cat"), ("cat", "sat"), ("<s>", "the")]
        for w1, w2 in test_pairs:
            prob = bigram_probability(w1, w2)
            print(f"P({w2}|{w1}) = {prob:.3f}")
        
        # å¥å­æ¦‚ç‡è®¡ç®—
        def sentence_probability(sentence):
            words = ["<s>"] + sentence.lower().split() + ["</s>"]
            prob = 1.0
            
            for i in range(1, len(words)):
                prob *= bigram_probability(words[i-1], words[i])
            
            return prob
        
        test_sentences = ["the cat sat", "a dog ran"]
        print("\nå¥å­æ¦‚ç‡:")
        for sentence in test_sentences:
            prob = sentence_probability(sentence)
            print(f"P('{sentence}') = {prob:.6f}")
        
        return bigram_counts, unigram_counts
    
    def smoothing_techniques(self):
        """å¹³æ»‘æŠ€æœ¯"""
        print("=== å¹³æ»‘æŠ€æœ¯ ===")
        
        print("æ•°æ®ç¨€ç–æ€§é—®é¢˜:")
        print("- è®­ç»ƒè¯­æ–™ä¸­æœªå‡ºç°çš„n-gramæ¦‚ç‡ä¸º0")
        print("- å¯¼è‡´æ•´ä¸ªå¥å­æ¦‚ç‡ä¸º0")
        print("- éœ€è¦å¹³æ»‘æŠ€æœ¯åˆ†é…æ¦‚ç‡è´¨é‡")
        print()
        
        smoothing_methods = {
            "åŠ æ³•å¹³æ»‘ (Add-k)": {
                "å…¬å¼": "P(wáµ¢|wáµ¢â‚‹â‚) = (C(wáµ¢â‚‹â‚,wáµ¢) + k) / (C(wáµ¢â‚‹â‚) + kÂ·|V|)",
                "å‚æ•°": "k (é€šå¸¸k=1, ç§°ä¸ºæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘)",
                "ä¼˜ç‚¹": "ç®€å•ç›´è§‚",
                "ç¼ºç‚¹": "å¯¹ä½é¢‘äº‹ä»¶è¿‡åº¦å¹³æ»‘"
            },
            "Good-Turingå¹³æ»‘": {
                "æ€æƒ³": "ç”¨é¢‘ç‡r+1çš„äº‹ä»¶æ•°ä¼°è®¡é¢‘ç‡rçš„æ¦‚ç‡",
                "é€‚ç”¨": "å¤„ç†é›¶é¢‘å’Œä½é¢‘äº‹ä»¶",
                "ä¼˜ç‚¹": "ç†è®ºåŸºç¡€å¼º",
                "ç¼ºç‚¹": "å®ç°å¤æ‚"
            },
            "Kneser-Neyå¹³æ»‘": {
                "æ€æƒ³": "åŸºäºcontinuation countè€Œéé¢‘ç‡",
                "ç‰¹ç‚¹": "è€ƒè™‘è¯æ±‡åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„åˆ†å¸ƒ",
                "ä¼˜ç‚¹": "æ•ˆæœæœ€å¥½",
                "ç¼ºç‚¹": "è®¡ç®—å¤æ‚"
            },
            "å›é€€ (Back-off)": {
                "æ€æƒ³": "é«˜é˜¶n-gramä¸å¯é æ—¶å›é€€åˆ°ä½é˜¶",
                "å®ç°": "Katzå›é€€ã€æ’å€¼å¹³æ»‘",
                "ä¼˜ç‚¹": "å……åˆ†åˆ©ç”¨å„é˜¶ä¿¡æ¯",
                "ç¼ºç‚¹": "éœ€è¦å¤šä¸ªæ¨¡å‹"
            }
        }
        
        for method, details in smoothing_methods.items():
            print(f"\n{method}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # å®ç°æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
        self.implement_laplace_smoothing()
        
        return smoothing_methods
    
    def implement_laplace_smoothing(self):
        """å®ç°æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘"""
        print("\n=== æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç¤ºä¾‹ ===")
        
        # ä½¿ç”¨å‰é¢çš„bigramè®¡æ•°
        corpus = ["the cat sat", "the dog ran"]
        
        bigram_counts = defaultdict(lambda: defaultdict(int))
        unigram_counts = defaultdict(int)
        vocabulary = set()
        
        for sentence in corpus:
            words = ["<s>"] + sentence.split() + ["</s>"]
            vocabulary.update(words)
            
            for i in range(len(words)):
                unigram_counts[words[i]] += 1
                if i > 0:
                    bigram_counts[words[i-1]][words[i]] += 1
        
        V = len(vocabulary)
        
        def laplace_probability(w1, w2, k=1):
            """æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ¦‚ç‡"""
            return (bigram_counts[w1][w2] + k) / (unigram_counts[w1] + k * V)
        
        def unsmoothed_probability(w1, w2):
            """æœªå¹³æ»‘æ¦‚ç‡"""
            if unigram_counts[w1] == 0:
                return 0
            return bigram_counts[w1][w2] / unigram_counts[w1]
        
        # æ¯”è¾ƒå¹³æ»‘å‰åçš„æ¦‚ç‡
        test_pairs = [("the", "cat"), ("cat", "dog"), ("dog", "elephant")]
        
        print("å¹³æ»‘å‰åæ¦‚ç‡æ¯”è¾ƒ:")
        print("Bigram\t\tUnsmoothed\tLaplace")
        print("-" * 40)
        
        for w1, w2 in test_pairs:
            unsmooth = unsmoothed_probability(w1, w2)
            laplace = laplace_probability(w1, w2)
            print(f"{w1},{w2}\t\t{unsmooth:.4f}\t\t{laplace:.4f}")
        
        return vocabulary, bigram_counts
```

## 2. è¯æ±‡è¡¨ç¤ºç†è®º ğŸ“Š

```python
class WordRepresentationTheory:
    """è¯æ±‡è¡¨ç¤ºç†è®º"""
    
    def __init__(self):
        self.representation_methods = {}
    
    def distributional_hypothesis(self):
        """åˆ†å¸ƒå‡è®¾ç†è®º"""
        print("=== åˆ†å¸ƒå‡è®¾ ===")
        
        print("Firth (1957): 'You shall know a word by the company it keeps'")
        print()
        print("æ ¸å¿ƒæ€æƒ³:")
        print("- è¯æ±‡çš„å«ä¹‰ç”±å…¶ä¸Šä¸‹æ–‡å†³å®š")
        print("- ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„è¯æ±‡æœ‰ç›¸ä¼¼å«ä¹‰")
        print("- å¯ä»¥é€šè¿‡ç»Ÿè®¡ä¸Šä¸‹æ–‡ä¿¡æ¯å­¦ä¹ è¯æ±‡è¡¨ç¤º")
        print()
        
        # ä¸Šä¸‹æ–‡ç±»å‹
        context_types = {
            "è¯è¢‹ä¸Šä¸‹æ–‡": {
                "å®šä¹‰": "å›ºå®šçª—å£å†…çš„è¯æ±‡é›†åˆ",
                "ç‰¹ç‚¹": "å¿½ç•¥è¯åº",
                "åº”ç”¨": "ä¼ ç»Ÿè¯å‘é‡æ¨¡å‹"
            },
            "å¥æ³•ä¸Šä¸‹æ–‡": {
                "å®šä¹‰": "é€šè¿‡å¥æ³•å…³ç³»å®šä¹‰çš„ä¸Šä¸‹æ–‡",
                "ç‰¹ç‚¹": "è€ƒè™‘è¯­æ³•ç»“æ„",
                "åº”ç”¨": "å¥æ³•è¯å‘é‡"
            },
            "æ–‡æ¡£ä¸Šä¸‹æ–‡": {
                "å®šä¹‰": "æ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡",
                "ç‰¹ç‚¹": "é•¿è·ç¦»ä¾èµ–",
                "åº”ç”¨": "æ–‡æ¡£è¡¨ç¤ºå­¦ä¹ "
            }
        }
        
        for context_type, details in context_types.items():
            print(f"{context_type}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        return context_types
    
    def vector_space_models(self):
        """å‘é‡ç©ºé—´æ¨¡å‹"""
        print("=== å‘é‡ç©ºé—´æ¨¡å‹ ===")
        
        # TF-IDFåŸç†
        print("TF-IDF (Term Frequency - Inverse Document Frequency):")
        print("TF-IDF(t,d) = TF(t,d) Ã— IDF(t)")
        print("å…¶ä¸­:")
        print("- TF(t,d) = count(t,d) / |d|")
        print("- IDF(t) = log(|D| / |{d: t âˆˆ d}|)")
        print()
        
        # å®ç°ç®€å•çš„TF-IDF
        documents = [
            "the cat sat on the mat",
            "the dog ran in the park", 
            "cats and dogs are pets"
        ]
        
        # è®¡ç®—TF-IDF
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(documents)
        feature_names = vectorizer.get_feature_names_out()
        
        print("TF-IDFçŸ©é˜µç¤ºä¾‹:")
        print("è¯æ±‡:", feature_names[:10])  # æ˜¾ç¤ºå‰10ä¸ªè¯
        print("æ–‡æ¡£0å‘é‡:", tfidf_matrix[0].toarray().flatten()[:10])
        print()
        
        # è¯æ±‡ç›¸ä¼¼åº¦è®¡ç®—
        def cosine_similarity_manual(vec1, vec2):
            """æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0
            return dot_product / (norm1 * norm2)
        
        # è®¡ç®—æ–‡æ¡£é—´ç›¸ä¼¼åº¦
        print("æ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µ:")
        n_docs = tfidf_matrix.shape[0]
        similarity_matrix = np.zeros((n_docs, n_docs))
        
        for i in range(n_docs):
            for j in range(n_docs):
                vec1 = tfidf_matrix[i].toarray().flatten()
                vec2 = tfidf_matrix[j].toarray().flatten()
                similarity_matrix[i, j] = cosine_similarity_manual(vec1, vec2)
        
        print(similarity_matrix)
        
        # å¯è§†åŒ–TF-IDF
        self.visualize_tfidf(tfidf_matrix, feature_names, documents)
        
        return tfidf_matrix, feature_names
    
    def visualize_tfidf(self, tfidf_matrix, feature_names, documents):
        """å¯è§†åŒ–TF-IDF"""
        # åˆ›å»ºçƒ­å›¾
        plt.figure(figsize=(12, 8))
        
        # é€‰æ‹©å‰15ä¸ªç‰¹å¾è¿›è¡Œå¯è§†åŒ–
        n_features = min(15, len(feature_names))
        tfidf_dense = tfidf_matrix.toarray()[:, :n_features]
        
        plt.subplot(2, 2, 1)
        sns.heatmap(tfidf_dense, 
                   xticklabels=feature_names[:n_features],
                   yticklabels=[f'Doc {i}' for i in range(len(documents))],
                   annot=True, fmt='.2f', cmap='YlOrRd')
        plt.title('TF-IDF çƒ­å›¾')
        plt.xlabel('è¯æ±‡')
        plt.ylabel('æ–‡æ¡£')
        
        # PCAé™ç»´å¯è§†åŒ–
        if tfidf_matrix.shape[1] > 2:
            pca = PCA(n_components=2)
            tfidf_2d = pca.fit_transform(tfidf_matrix.toarray())
            
            plt.subplot(2, 2, 2)
            plt.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1], s=100, alpha=0.7)
            for i, doc in enumerate(documents):
                plt.annotate(f'Doc {i}', (tfidf_2d[i, 0], tfidf_2d[i, 1]), 
                           xytext=(5, 5), textcoords='offset points')
            plt.title('æ–‡æ¡£åœ¨TF-IDFç©ºé—´çš„åˆ†å¸ƒ')
            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        
        # è¯æ±‡é¢‘ç‡åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        word_freqs = np.sum(tfidf_matrix.toarray(), axis=0)
        top_words_idx = np.argsort(word_freqs)[-10:]
        top_words = [feature_names[i] for i in top_words_idx]
        top_freqs = word_freqs[top_words_idx]
        
        plt.barh(range(len(top_words)), top_freqs)
        plt.yticks(range(len(top_words)), top_words)
        plt.title('Top 10 è¯æ±‡ TF-IDF æƒé‡')
        plt.xlabel('TF-IDF Score')
        
        # è¯äº‘
        plt.subplot(2, 2, 4)
        all_text = ' '.join(documents)
        if all_text.strip():  # æ£€æŸ¥æ–‡æœ¬ä¸ä¸ºç©º
            try:
                wordcloud = WordCloud(width=400, height=300, 
                                    background_color='white').generate(all_text)
                plt.imshow(wordcloud, interpolation='bilinear')
            except:
                plt.text(0.5, 0.5, 'WordCloud\nNot Available', 
                        ha='center', va='center', transform=plt.gca().transAxes)
        plt.title('è¯äº‘')
        plt.axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def word_embeddings_theory(self):
        """è¯å‘é‡ç†è®º"""
        print("=== è¯å‘é‡ç†è®º ===")
        
        embedding_models = {
            "Word2Vec": {
                "æ¶æ„": ["Skip-gram", "CBOW"],
                "ç›®æ ‡": "é¢„æµ‹ä¸Šä¸‹æ–‡è¯æ±‡",
                "ä¼˜ç‚¹": ["é«˜æ•ˆè®­ç»ƒ", "æ•è·è¯­ä¹‰ç›¸ä¼¼æ€§"],
                "ç¼ºç‚¹": ["é™æ€è¡¨ç¤º", "å¤šä¹‰è¯é—®é¢˜"]
            },
            "GloVe": {
                "æ¶æ„": "å…¨å±€å‘é‡",
                "ç›®æ ‡": "åˆ†è§£è¯æ±‡å…±ç°çŸ©é˜µ",
                "ä¼˜ç‚¹": ["ç»“åˆå…¨å±€å’Œå±€éƒ¨ä¿¡æ¯"],
                "ç¼ºç‚¹": ["è®¡ç®—å¤æ‚", "å†…å­˜éœ€æ±‚å¤§"]
            },
            "FastText": {
                "æ¶æ„": "å­è¯åµŒå…¥",
                "ç›®æ ‡": "è€ƒè™‘è¯æ±‡å†…éƒ¨ç»“æ„",
                "ä¼˜ç‚¹": ["å¤„ç†OOVè¯æ±‡", "å½¢æ€ä¿¡æ¯"],
                "ç¼ºç‚¹": ["å‘é‡ç»´åº¦é«˜", "å™ªå£°æ•æ„Ÿ"]
            }
        }
        
        for model, details in embedding_models.items():
            print(f"\n{model}:")
            for key, value in details.items():
                if isinstance(value, list):
                    print(f"  {key}: {', '.join(value)}")
                else:
                    print(f"  {key}: {value}")
        
        # Word2Vecæ•°å­¦åŸç†
        print("\n=== Word2Vecæ•°å­¦åŸç† ===")
        print("Skip-gramç›®æ ‡å‡½æ•°:")
        print("L = Î£_{wâˆˆC} Î£_{câˆˆcontext(w)} log P(c|w)")
        print("å…¶ä¸­:")
        print("P(c|w) = exp(u_c^T v_w) / Î£_{w'} exp(u_{w'}^T v_w)")
        print()
        
        print("è´Ÿé‡‡æ ·ä¼˜åŒ–:")
        print("L = log Ïƒ(u_c^T v_w) + Î£_{i=1}^k E_{w_i~P_n(w)} [log Ïƒ(-u_{w_i}^T v_w)]")
        print("å…¶ä¸­ Ïƒ(x) = 1/(1+exp(-x))")
        
        return embedding_models
    
    def contextual_embeddings(self):
        """ä¸Šä¸‹æ–‡è¯å‘é‡"""
        print("=== ä¸Šä¸‹æ–‡è¯å‘é‡ ===")
        
        print("ä¼ ç»Ÿè¯å‘é‡é—®é¢˜:")
        print("- ä¸€è¯ä¸€å‘é‡ï¼Œæ— æ³•å¤„ç†å¤šä¹‰è¯")
        print("- é™æ€è¡¨ç¤ºï¼Œå¿½ç•¥ä¸Šä¸‹æ–‡å˜åŒ–")
        print("- ä¾‹å¦‚ï¼š'bank'åœ¨ä¸åŒè¯­å¢ƒä¸‹å«ä¹‰ä¸åŒ")
        print()
        
        contextual_models = {
            "ELMo": {
                "æ¶æ„": "åŒå‘LSTM",
                "ç‰¹ç‚¹": "å±‚æ¬¡åŒ–è¡¨ç¤º",
                "åˆ›æ–°": "å­—ç¬¦çº§è¾“å…¥",
                "è¡¨ç¤º": "åŠ æƒå¹³å‡å„å±‚è¾“å‡º"
            },
            "GPT": {
                "æ¶æ„": "Transformerè§£ç å™¨",
                "ç‰¹ç‚¹": "å•å‘è¯­è¨€æ¨¡å‹",
                "åˆ›æ–°": "å¤§è§„æ¨¡é¢„è®­ç»ƒ",
                "è¡¨ç¤º": "æœ€åä¸€å±‚éšè—çŠ¶æ€"
            },
            "BERT": {
                "æ¶æ„": "Transformerç¼–ç å™¨",
                "ç‰¹ç‚¹": "åŒå‘ä¸Šä¸‹æ–‡",
                "åˆ›æ–°": "æ©ç è¯­è¨€æ¨¡å‹",
                "è¡¨ç¤º": "æ‰€æœ‰å±‚çš„åŠ æƒç»„åˆ"
            }
        }
        
        for model, details in contextual_models.items():
            print(f"{model}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„æ•°å­¦æè¿°
        print("ä¸Šä¸‹æ–‡è¡¨ç¤ºå…¬å¼:")
        print("h_i^(l) = Transformer_layer^(l)(h_i^(l-1), context)")
        print("å…¶ä¸­ context åŒ…å«æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯")
        
        return contextual_models

class SemanticComposition:
    """è¯­ä¹‰ç»„åˆç†è®º"""
    
    def __init__(self):
        pass
    
    def compositionality_principle(self):
        """ç»„åˆæ€§åŸç†"""
        print("=== è¯­ä¹‰ç»„åˆæ€§åŸç† ===")
        
        print("FregeåŸç†:")
        print("'å¥å­çš„å«ä¹‰æ˜¯å…¶ç»„æˆéƒ¨åˆ†å«ä¹‰çš„å‡½æ•°'")
        print()
        
        composition_methods = {
            "åŠ æ³•ç»„åˆ": {
                "å…¬å¼": "v(A B) = v(A) + v(B)",
                "ä¼˜ç‚¹": "ç®€å•é«˜æ•ˆ",
                "ç¼ºç‚¹": "å¿½ç•¥è¯­æ³•ç»“æ„",
                "é€‚ç”¨": "è¯è¢‹æ¨¡å‹"
            },
            "ä¹˜æ³•ç»„åˆ": {
                "å…¬å¼": "v(A B) = v(A) âŠ™ v(B) (é€å…ƒç´ ç›¸ä¹˜)",
                "ä¼˜ç‚¹": "ä¿ç•™ç‰¹å¾äº¤äº’",
                "ç¼ºç‚¹": "ä¿¡æ¯å¯èƒ½ä¸¢å¤±",
                "é€‚ç”¨": "ç®€å•ç»„åˆ"
            },
            "å¼ é‡ç§¯": {
                "å…¬å¼": "v(A B) = v(A) âŠ— v(B)",
                "ä¼˜ç‚¹": "ä¿ç•™å®Œæ•´ä¿¡æ¯",
                "ç¼ºç‚¹": "ç»´åº¦çˆ†ç‚¸",
                "é€‚ç”¨": "ç†è®ºåˆ†æ"
            },
            "å¾ªç¯å·ç§¯": {
                "å…¬å¼": "v(A B) = v(A) âŠ› v(B)",
                "ä¼˜ç‚¹": "ç»´åº¦ä¸å˜",
                "ç¼ºç‚¹": "ä¸å¯äº¤æ¢",
                "é€‚ç”¨": "åºåˆ—å»ºæ¨¡"
            }
        }
        
        for method, details in composition_methods.items():
            print(f"{method}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # å®ç°ç®€å•çš„ç»„åˆç¤ºä¾‹
        self.demonstrate_composition()
        
        return composition_methods
    
    def demonstrate_composition(self):
        """æ¼”ç¤ºç»„åˆæ–¹æ³•"""
        print("=== ç»„åˆæ–¹æ³•ç¤ºä¾‹ ===")
        
        # å‡è®¾è¯å‘é‡
        np.random.seed(42)
        word_vectors = {
            "red": np.random.randn(5),
            "car": np.random.randn(5),
            "fast": np.random.randn(5),
            "blue": np.random.randn(5)
        }
        
        # ä¸åŒç»„åˆæ–¹æ³•
        def additive_composition(v1, v2):
            return v1 + v2
        
        def multiplicative_composition(v1, v2):
            return v1 * v2
        
        def weighted_composition(v1, v2, alpha=0.5):
            return alpha * v1 + (1 - alpha) * v2
        
        # è®¡ç®—çŸ­è¯­è¡¨ç¤º
        phrases = [("red", "car"), ("fast", "car"), ("blue", "car")]
        
        print("çŸ­è¯­è¡¨ç¤ºæ¯”è¾ƒ:")
        print("Phrase\t\tAdditive\tMultiplicative\tWeighted")
        print("-" * 60)
        
        for w1, w2 in phrases:
            v1, v2 = word_vectors[w1], word_vectors[w2]
            
            add_comp = additive_composition(v1, v2)
            mult_comp = multiplicative_composition(v1, v2)
            weighted_comp = weighted_composition(v1, v2)
            
            print(f"{w1} {w2}\t{add_comp[:2]}\t{mult_comp[:2]}\t{weighted_comp[:2]}")
        
        # è®¡ç®—ç»„åˆåçš„ç›¸ä¼¼åº¦
        red_car = additive_composition(word_vectors["red"], word_vectors["car"])
        blue_car = additive_composition(word_vectors["blue"], word_vectors["car"])
        fast_car = additive_composition(word_vectors["fast"], word_vectors["car"])
        
        def cosine_sim(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        
        print(f"\nç›¸ä¼¼åº¦åˆ†æ:")
        print(f"sim(red car, blue car) = {cosine_sim(red_car, blue_car):.3f}")
        print(f"sim(red car, fast car) = {cosine_sim(red_car, fast_car):.3f}")
        
        return word_vectors

class DiscourseAnalysis:
    """è¯è¯­åˆ†æç†è®º"""
    
    def __init__(self):
        pass
    
    def coherence_and_cohesion(self):
        """è¿è´¯æ€§å’Œå‡èšæ€§"""
        print("=== è¿è´¯æ€§å’Œå‡èšæ€§ ===")
        
        print("è¿è´¯æ€§ (Coherence):")
        print("- æ–‡æœ¬çš„æ•´ä½“æ„ä¹‰ç»Ÿä¸€æ€§")
        print("- è¯»è€…èƒ½å¤Ÿç†è§£æ–‡æœ¬çš„é€»è¾‘å…³ç³»")
        print("- åŸºäºè¯­ä¹‰å’Œè¯­ç”¨å±‚é¢")
        print()
        
        print("å‡èšæ€§ (Cohesion):")
        print("- æ–‡æœ¬è¡¨é¢çš„è¯­è¨€è¿æ¥")
        print("- é€šè¿‡è¯æ±‡å’Œè¯­æ³•æ‰‹æ®µå®ç°")
        print("- åŒ…æ‹¬æŒ‡ä»£ã€æ›¿æ¢ã€çœç•¥ç­‰")
        print()
        
        cohesion_devices = {
            "è¯æ±‡å‡èš": {
                "é‡å¤": "åŒä¸€è¯æ±‡çš„é‡å¤ä½¿ç”¨",
                "åŒä¹‰": "ä½¿ç”¨åŒä¹‰è¯æˆ–è¿‘ä¹‰è¯", 
                "ä¸Šä¸‹ä¹‰": "ä½¿ç”¨ä¸Šä½è¯æˆ–ä¸‹ä½è¯",
                "æ­é…": "è¯æ±‡ä¹‹é—´çš„ä¹ æƒ¯æ­é…"
            },
            "è¯­æ³•å‡èš": {
                "æŒ‡ä»£": "äººç§°ä»£è¯ã€æŒ‡ç¤ºä»£è¯",
                "æ›¿ä»£": "ç”¨å…¶ä»–è¯æ±‡æ›¿ä»£å‰æ–‡å†…å®¹",
                "çœç•¥": "çœç•¥å¯ä»ä¸Šä¸‹æ–‡æ¨æ–­çš„å†…å®¹",
                "è¿æ¥": "è¿æ¥è¯è¡¨ç¤ºé€»è¾‘å…³ç³»"
            }
        }
        
        for device_type, devices in cohesion_devices.items():
            print(f"{device_type}:")
            for device, description in devices.items():
                print(f"  {device}: {description}")
            print()
        
        return cohesion_devices
    
    def discourse_relations(self):
        """è¯è¯­å…³ç³»"""
        print("=== è¯è¯­å…³ç³»ç†è®º ===")
        
        discourse_relations = {
            "å› æœå…³ç³»": {
                "æ ‡è®°è¯": ["å› ä¸º", "æ‰€ä»¥", "å› æ­¤", "ç”±äº"],
                "ç¤ºä¾‹": "å› ä¸ºä¸‹é›¨ï¼Œæ‰€ä»¥æˆ‘å¸¦äº†ä¼",
                "é€»è¾‘": "P â†’ Q"
            },
            "å¯¹æ¯”å…³ç³»": {
                "æ ‡è®°è¯": ["ä½†æ˜¯", "ç„¶è€Œ", "ç›¸å", "ä¸è¿‡"],
                "ç¤ºä¾‹": "ä»–å¾ˆèªæ˜ï¼Œä½†æ˜¯ä¸å¤ŸåŠªåŠ›",
                "é€»è¾‘": "P âˆ§ Â¬Q"
            },
            "é€’è¿›å…³ç³»": {
                "æ ‡è®°è¯": ["è€Œä¸”", "å¹¶ä¸”", "ä¸ä»…...è¿˜", "æ›´"],
                "ç¤ºä¾‹": "ä»–ä¸ä»…èªæ˜ï¼Œè€Œä¸”åŠªåŠ›",
                "é€»è¾‘": "P âˆ§ Q (Q > P)"
            },
            "æ¡ä»¶å…³ç³»": {
                "æ ‡è®°è¯": ["å¦‚æœ", "å‡å¦‚", "åªè¦", "é™¤é"],
                "ç¤ºä¾‹": "å¦‚æœæ˜å¤©æ™´å¤©ï¼Œæˆ‘ä»¬å°±å»éƒŠæ¸¸",
                "é€»è¾‘": "P â†’ Q"
            }
        }
        
        for relation, details in discourse_relations.items():
            print(f"{relation}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        return discourse_relations
```

## 3. è¯­è¨€ç†è§£ä¸ç”Ÿæˆ ğŸ’­

```python
class LanguageUnderstandingGeneration:
    """è¯­è¨€ç†è§£ä¸ç”Ÿæˆç†è®º"""
    
    def __init__(self):
        pass
    
    def parsing_theories(self):
        """å¥æ³•è§£æç†è®º"""
        print("=== å¥æ³•è§£æç†è®º ===")
        
        parsing_approaches = {
            "è‡ªé¡¶å‘ä¸‹è§£æ": {
                "ç­–ç•¥": "ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œé€æ­¥å±•å¼€",
                "ä¼˜ç‚¹": "ç›®æ ‡å¯¼å‘ï¼Œæ˜“äºå®ç°",
                "ç¼ºç‚¹": "å¯èƒ½äº§ç”Ÿæ— ç”¨åˆ†æ”¯",
                "ç®—æ³•": "é€’å½’ä¸‹é™ã€é¢„æµ‹è§£æ"
            },
            "è‡ªåº•å‘ä¸Šè§£æ": {
                "ç­–ç•¥": "ä»è¯æ±‡å¼€å§‹ï¼Œé€æ­¥æ„å»ºæ ‘",
                "ä¼˜ç‚¹": "åªæ„å»ºæœ‰ç”¨çš„ç»“æ„",
                "ç¼ºç‚¹": "å¯èƒ½äº§ç”Ÿæ­§ä¹‰",
                "ç®—æ³•": "ç§»ä½-å½’çº¦ã€CKYç®—æ³•"
            },
            "Chartè§£æ": {
                "ç­–ç•¥": "åŠ¨æ€è§„åˆ’ï¼Œé¿å…é‡å¤è®¡ç®—",
                "ä¼˜ç‚¹": "é«˜æ•ˆå¤„ç†æ­§ä¹‰",
                "ç¼ºç‚¹": "å†…å­˜éœ€æ±‚å¤§",
                "ç®—æ³•": "Earleyç®—æ³•ã€CKYç®—æ³•"
            },
            "ä¾å­˜è§£æ": {
                "ç­–ç•¥": "æ„å»ºè¯æ±‡é—´çš„ä¾å­˜å…³ç³»",
                "ä¼˜ç‚¹": "è·¨è¯­è¨€é€‚ç”¨æ€§å¼º",
                "ç¼ºç‚¹": "ç¼ºä¹å±‚æ¬¡ç»“æ„",
                "ç®—æ³•": "åŸºäºè½¬ç§»ã€åŸºäºå›¾"
            }
        }
        
        for approach, details in parsing_approaches.items():
            print(f"{approach}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # CKYç®—æ³•ç¤ºä¾‹
        self.demonstrate_cky_parsing()
        
        return parsing_approaches
    
    def demonstrate_cky_parsing(self):
        """æ¼”ç¤ºCKYè§£æç®—æ³•"""
        print("=== CKYç®—æ³•ç¤ºä¾‹ ===")
        
        # CNFæ–‡æ³•è§„åˆ™
        grammar_rules = {
            # ç»ˆç»“ç¬¦è§„åˆ™ A -> a
            "Det": ["the", "a"],
            "N": ["cat", "dog", "mat"],
            "V": ["sat", "ran"],
            "P": ["on", "in"],
            # éç»ˆç»“ç¬¦è§„åˆ™ A -> BC
            "NP": [("Det", "N")],
            "PP": [("P", "NP")],
            "VP": [("V", "PP")],
            "S": [("NP", "VP")]
        }
        
        sentence = ["the", "cat", "sat", "on", "the", "mat"]
        n = len(sentence)
        
        # åˆå§‹åŒ–CKYè¡¨
        table = [[set() for _ in range(n)] for _ in range(n)]
        
        # å¡«å……å¯¹è§’çº¿ï¼ˆé•¿åº¦ä¸º1çš„å­ä¸²ï¼‰
        for i in range(n):
            word = sentence[i]
            for lhs, rhs_list in grammar_rules.items():
                if word in rhs_list:
                    table[i][i].add(lhs)
        
        # å¡«å……è¡¨æ ¼ï¼ˆé•¿åº¦ä»2åˆ°nï¼‰
        for length in range(2, n + 1):
            for i in range(n - length + 1):
                j = i + length - 1
                
                for k in range(i, j):
                    # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
                    left_set = table[i][k]
                    right_set = table[k+1][j]
                    
                    for lhs, rhs_list in grammar_rules.items():
                        for rhs in rhs_list:
                            if isinstance(rhs, tuple) and len(rhs) == 2:
                                if rhs[0] in left_set and rhs[1] in right_set:
                                    table[i][j].add(lhs)
        
        # è¾“å‡ºè§£æè¡¨
        print("CKYè§£æè¡¨:")
        print("å¥å­:", ' '.join(sentence))
        print()
        
        for i in range(n):
            for j in range(i, n):
                if table[i][j]:
                    span = ' '.join(sentence[i:j+1])
                    categories = ', '.join(table[i][j])
                    print(f"[{i},{j}] '{span}': {categories}")
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥è§£æä¸ºå®Œæ•´å¥å­
        if "S" in table[0][n-1]:
            print(f"\nâœ“ å¥å­å¯ä»¥è§£æä¸ºS (sentence)")
        else:
            print(f"\nâœ— å¥å­æ— æ³•è§£æ")
        
        return table
    
    def semantic_parsing(self):
        """è¯­ä¹‰è§£æ"""
        print("=== è¯­ä¹‰è§£æ ===")
        
        print("è¯­ä¹‰è§£æç›®æ ‡:")
        print("å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºå½¢å¼åŒ–çš„è¯­ä¹‰è¡¨ç¤º")
        print()
        
        semantic_representations = {
            "ä¸€é˜¶é€»è¾‘": {
                "å½¢å¼": "âˆƒx (cat(x) âˆ§ sat(x, mat))",
                "ä¼˜ç‚¹": "è¡¨è¾¾èƒ½åŠ›å¼ºï¼Œæ¨ç†èƒ½åŠ›å¼º",
                "ç¼ºç‚¹": "å¤æ‚ï¼Œéš¾ä»¥å­¦ä¹ ",
                "åº”ç”¨": "é—®ç­”ç³»ç»Ÿã€æ¨ç†"
            },
            "Lambdaæ¼”ç®—": {
                "å½¢å¼": "Î»x.cat(x) âˆ§ sat(x, mat)",
                "ä¼˜ç‚¹": "ç»„åˆæ€§å¼º",
                "ç¼ºç‚¹": "æŠ½è±¡ç¨‹åº¦é«˜",
                "åº”ç”¨": "ç»„åˆè¯­ä¹‰å­¦"
            },
            "SQLæŸ¥è¯¢": {
                "å½¢å¼": "SELECT * FROM table WHERE cat='true'",
                "ä¼˜ç‚¹": "ç›´æ¥å¯æ‰§è¡Œ",
                "ç¼ºç‚¹": "è¡¨è¾¾èƒ½åŠ›æœ‰é™",
                "åº”ç”¨": "æ•°æ®åº“æŸ¥è¯¢"
            },
            "æŠ½è±¡è¯­æ³•æ ‘": {
                "å½¢å¼": "æ ‘çŠ¶ç»“æ„è¡¨ç¤º",
                "ä¼˜ç‚¹": "ç»“æ„æ¸…æ™°",
                "ç¼ºç‚¹": "é¢†åŸŸç‰¹å®š",
                "åº”ç”¨": "ä»£ç ç”Ÿæˆã€APIè°ƒç”¨"
            }
        }
        
        for repr_type, details in semantic_representations.items():
            print(f"{repr_type}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        return semantic_representations
    
    def text_generation_theories(self):
        """æ–‡æœ¬ç”Ÿæˆç†è®º"""
        print("=== æ–‡æœ¬ç”Ÿæˆç†è®º ===")
        
        generation_approaches = {
            "åŸºäºè§„åˆ™": {
                "æ–¹æ³•": "é¢„å®šä¹‰æ¨¡æ¿å’Œè§„åˆ™",
                "ä¼˜ç‚¹": "å¯æ§æ€§å¼ºï¼Œè´¨é‡ç¨³å®š",
                "ç¼ºç‚¹": "ç¼ºä¹å¤šæ ·æ€§ï¼Œæ‰©å±•æ€§å·®",
                "åº”ç”¨": "æŠ¥å‘Šç”Ÿæˆã€æ•°æ®åˆ°æ–‡æœ¬"
            },
            "åŸºäºæ£€ç´¢": {
                "æ–¹æ³•": "ä»è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬",
                "ä¼˜ç‚¹": "è¯­æ³•æ­£ç¡®æ€§é«˜",
                "ç¼ºç‚¹": "åˆ›æ–°æ€§ä¸è¶³",
                "åº”ç”¨": "èŠå¤©æœºå™¨äººã€é—®ç­”ç³»ç»Ÿ"
            },
            "åŸºäºç»Ÿè®¡": {
                "æ–¹æ³•": "n-gramè¯­è¨€æ¨¡å‹",
                "ä¼˜ç‚¹": "å¯ä»¥ç”Ÿæˆæ–°å†…å®¹",
                "ç¼ºç‚¹": "é•¿è·ç¦»ä¸€è‡´æ€§å·®",
                "åº”ç”¨": "æ–‡æœ¬è¡¥å…¨ã€æ‘˜è¦"
            },
            "åŸºäºç¥ç»ç½‘ç»œ": {
                "æ–¹æ³•": "RNNã€Transformerç­‰",
                "ä¼˜ç‚¹": "æµç•…åº¦é«˜ï¼Œå¤šæ ·æ€§å¥½",
                "ç¼ºç‚¹": "å¯èƒ½äº§ç”Ÿå¹»è§‰",
                "åº”ç”¨": "å¯¹è¯ã€åˆ›ä½œã€ç¿»è¯‘"
            }
        }
        
        for approach, details in generation_approaches.items():
            print(f"{approach}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # ç”Ÿæˆç­–ç•¥
        print("=== ç”Ÿæˆç­–ç•¥ ===")
        
        generation_strategies = {
            "è´ªå¿ƒè§£ç ": {
                "ç­–ç•¥": "æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯",
                "ä¼˜ç‚¹": "ç®€å•å¿«é€Ÿ",
                "ç¼ºç‚¹": "å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜",
                "æ•°å­¦": "w_t = argmax P(w|w_<t)"
            },
            "æŸæœç´¢": {
                "ç­–ç•¥": "ä¿æŒkä¸ªæœ€ä¼˜å€™é€‰åºåˆ—",
                "ä¼˜ç‚¹": "è´¨é‡è¾ƒé«˜",
                "ç¼ºç‚¹": "è®¡ç®—å¤æ‚åº¦é«˜",
                "æ•°å­¦": "ä¿æŒtop-kåºåˆ—"
            },
            "éšæœºé‡‡æ ·": {
                "ç­–ç•¥": "æŒ‰æ¦‚ç‡åˆ†å¸ƒéšæœºé‡‡æ ·",
                "ä¼˜ç‚¹": "å¤šæ ·æ€§å¥½",
                "ç¼ºç‚¹": "è´¨é‡ä¸ç¨³å®š",
                "æ•°å­¦": "w_t ~ P(w|w_<t)"
            },
            "Top-ké‡‡æ ·": {
                "ç­–ç•¥": "åœ¨top-kè¯æ±‡ä¸­éšæœºé‡‡æ ·",
                "ä¼˜ç‚¹": "å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§",
                "ç¼ºç‚¹": "kå€¼éœ€è¦è°ƒä¼˜",
                "æ•°å­¦": "é‡æ–°å½’ä¸€åŒ–top-kæ¦‚ç‡"
            },
            "Top-pé‡‡æ ·": {
                "ç­–ç•¥": "ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pæ—¶æˆªæ–­",
                "ä¼˜ç‚¹": "è‡ªé€‚åº”è¯æ±‡é›†åˆå¤§å°",
                "ç¼ºç‚¹": "på€¼éœ€è¦è°ƒä¼˜",
                "æ•°å­¦": "P_cumsum â‰¤ p"
            }
        }
        
        for strategy, details in generation_strategies.items():
            print(f"{strategy}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        return generation_approaches, generation_strategies

def comprehensive_nlp_theory_summary():
    """NLPç†è®ºç»¼åˆæ€»ç»“"""
    print("=== NLPç†è®ºç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "ç†è®ºåŸºç¡€": {
            "è¯­è¨€å­¦åŸºç¡€": "å½¢æ€å­¦ã€å¥æ³•å­¦ã€è¯­ä¹‰å­¦ã€è¯­ç”¨å­¦",
            "æ•°å­¦åŸºç¡€": "æ¦‚ç‡è®ºã€ä¿¡æ¯è®ºã€çº¿æ€§ä»£æ•°",
            "è®¤çŸ¥åŸºç¡€": "å¿ƒç†è¯­è¨€å­¦ã€è®¤çŸ¥ç§‘å­¦"
        },
        
        "æ ¸å¿ƒæ¦‚å¿µ": {
            "åˆ†å¸ƒå‡è®¾": "è¯æ±‡å«ä¹‰ç”±ä¸Šä¸‹æ–‡å†³å®š",
            "ç»„åˆæ€§åŸç†": "æ•´ä½“å«ä¹‰ç”±éƒ¨åˆ†å«ä¹‰ç»„åˆ",
            "ç¨€ç–æ€§é—®é¢˜": "è‡ªç„¶è¯­è¨€çš„é«˜ç»´ç¨€ç–ç‰¹æ€§"
        },
        
        "è¡¨ç¤ºå­¦ä¹ ": {
            "ç¬¦å·è¡¨ç¤º": "ç¦»æ•£ç¬¦å·ã€è¯­æ³•æ ‘ã€é€»è¾‘å½¢å¼",
            "åˆ†å¸ƒè¡¨ç¤º": "è¯å‘é‡ã€å¥å‘é‡ã€æ–‡æ¡£å‘é‡",
            "ä¸Šä¸‹æ–‡è¡¨ç¤º": "åŠ¨æ€è¡¨ç¤ºã€å¤šä¹‰è¯æ¶ˆæ­§"
        },
        
        "æ¨¡å‹æ¼”è¿›": {
            "ç»Ÿè®¡æ–¹æ³•": "n-gramã€HMMã€CRF",
            "ç¥ç»æ–¹æ³•": "RNNã€CNNã€Attention",
            "é¢„è®­ç»ƒæ¨¡å‹": "BERTã€GPTã€T5"
        },
        
        "åº”ç”¨ä»»åŠ¡": {
            "ç†è§£ä»»åŠ¡": "åˆ†ç±»ã€æ ‡æ³¨ã€è§£æã€é—®ç­”",
            "ç”Ÿæˆä»»åŠ¡": "ç¿»è¯‘ã€æ‘˜è¦ã€å¯¹è¯ã€åˆ›ä½œ",
            "å¤šæ¨¡æ€": "å›¾æ–‡åŒ¹é…ã€è§†é¢‘æè¿°"
        },
        
        "æœªæ¥æ–¹å‘": {
            "å¤§è¯­è¨€æ¨¡å‹": "è§„æ¨¡åŒ–ã€æ¶Œç°èƒ½åŠ›ã€å¯¹é½",
            "å¤šæ¨¡æ€èåˆ": "è§†è§‰-è¯­è¨€ã€è¯­éŸ³-è¯­è¨€",
            "çŸ¥è¯†å¢å¼º": "å¸¸è¯†æ¨ç†ã€çŸ¥è¯†å›¾è°±",
            "å¯è§£é‡Šæ€§": "æ³¨æ„åŠ›å¯è§†åŒ–ã€æ¢æµ‹å®éªŒ"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("è‡ªç„¶è¯­è¨€å¤„ç†ç†è®ºæŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Manning & SchÃ¼tze (1999): "Foundations of Statistical Natural Language Processing"
- Jurafsky & Martin (2020): "Speech and Language Processing"
- Goldberg (2017): "Neural Network Methods for Natural Language Processing"
- Koehn (2020): "Neural Machine Translation"
- Rogers et al. (2020): "A Primer on Neural Network Models for Natural Language Processing"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [NLPæ¨¡å‹å®ç°](../nlp_models.md) - å®é™…æ¨¡å‹æ„å»º
- [Transformeræ¶æ„](transformer_architecture.md) - æ·±å…¥ç†è§£Transformer
- [å¤§è¯­è¨€æ¨¡å‹ç†è®º](llm_architecture_theory.md) - LLMæ¶æ„åˆ†æ