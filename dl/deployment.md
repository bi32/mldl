# PyTorchæ¨¡å‹éƒ¨ç½²å®Œå…¨æŒ‡å— ğŸš€

ä»è®­ç»ƒåˆ°ç”Ÿäº§ï¼Œè®©ä½ çš„æ¨¡å‹çœŸæ­£å‘æŒ¥ä»·å€¼ã€‚æœ¬ç« æ¶µç›–ONNXã€TorchScriptã€é‡åŒ–ã€å‰ªæç­‰æ‰€æœ‰éƒ¨ç½²æŠ€æœ¯ã€‚

## 1. TorchScript - PyTorchåŸç”Ÿéƒ¨ç½² ğŸ“¦

```python
import torch
import torch.nn as nn
import time

# ç¤ºä¾‹æ¨¡å‹
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3, padding=1)
        self.bn = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(64 * 32 * 32, 10)
    
    def forward(self, x):
        x = self.relu(self.bn(self.conv(x)))
        x = x.view(x.size(0), -1)
        return self.fc(x)

# æ–¹æ³•1: Tracing
def torch_script_trace(model, example_input):
    """é€šè¿‡è¿½è¸ªè½¬æ¢ä¸ºTorchScript"""
    model.eval()
    
    # è¿½è¸ªæ¨¡å‹
    traced_model = torch.jit.trace(model, example_input)
    
    # ä¿å­˜æ¨¡å‹
    traced_model.save("model_traced.pt")
    
    # åŠ è½½æ¨¡å‹
    loaded_model = torch.jit.load("model_traced.pt")
    
    # æ¨ç†
    with torch.no_grad():
        output = loaded_model(example_input)
    
    return traced_model

# æ–¹æ³•2: Scripting
def torch_script_script(model):
    """é€šè¿‡è„šæœ¬åŒ–è½¬æ¢ä¸ºTorchScript"""
    model.eval()
    
    # è„šæœ¬åŒ–æ¨¡å‹
    scripted_model = torch.jit.script(model)
    
    # ä¿å­˜
    scripted_model.save("model_scripted.pt")
    
    return scripted_model

# å¸¦æ§åˆ¶æµçš„æ¨¡å‹ï¼ˆéœ€è¦scriptï¼‰
class ModelWithControlFlow(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 10)
    
    def forward(self, x, use_dropout: bool = False):
        x = self.fc1(x)
        if use_dropout:  # æ§åˆ¶æµ
            x = torch.dropout(x, 0.5, self.training)
        x = self.fc2(x)
        return x

# æ€§èƒ½å¯¹æ¯”
def benchmark_torchscript():
    model = SimpleModel()
    model.eval()
    example_input = torch.randn(1, 3, 32, 32)
    
    # åŸå§‹PyTorch
    start = time.time()
    for _ in range(1000):
        with torch.no_grad():
            _ = model(example_input)
    pytorch_time = time.time() - start
    
    # TorchScript
    traced_model = torch.jit.trace(model, example_input)
    start = time.time()
    for _ in range(1000):
        with torch.no_grad():
            _ = traced_model(example_input)
    torchscript_time = time.time() - start
    
    print(f"PyTorch: {pytorch_time:.3f}s")
    print(f"TorchScript: {torchscript_time:.3f}s")
    print(f"åŠ é€Ÿæ¯”: {pytorch_time/torchscript_time:.2f}x")
```

## 2. ONNXå¯¼å‡ºå’Œä¼˜åŒ– ğŸ”„

```python
import torch
import onnx
import onnxruntime as ort
import numpy as np

def export_to_onnx(model, dummy_input, onnx_path="model.onnx"):
    """å¯¼å‡ºæ¨¡å‹åˆ°ONNX"""
    model.eval()
    
    # å¯¼å‡º
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    # éªŒè¯ONNXæ¨¡å‹
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    
    print(f"æ¨¡å‹å·²å¯¼å‡ºåˆ° {onnx_path}")
    return onnx_model

def optimize_onnx(onnx_path):
    """ä¼˜åŒ–ONNXæ¨¡å‹"""
    from onnxruntime.transformers import optimizer
    
    # ä¼˜åŒ–é€‰é¡¹
    opt_options = optimizer.OptimizeOptions(
        enable_gelu=True,
        enable_layer_norm=True,
        enable_attention=True,
        use_gpu=torch.cuda.is_available(),
        only_fusion=False,
        enable_gemm_fast=True
    )
    
    # æ‰§è¡Œä¼˜åŒ–
    optimized_model = optimizer.optimize_model(
        onnx_path,
        model_type='bert',  # æˆ– 'gpt2', 'vit' ç­‰
        optimization_options=opt_options
    )
    
    # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
    optimized_path = onnx_path.replace('.onnx', '_optimized.onnx')
    optimized_model.save_model_to_file(optimized_path)
    
    return optimized_path

def onnx_inference(onnx_path, input_data):
    """ä½¿ç”¨ONNX Runtimeæ¨ç†"""
    # åˆ›å»ºæ¨ç†ä¼šè¯
    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
    session = ort.InferenceSession(onnx_path, providers=providers)
    
    # å‡†å¤‡è¾“å…¥
    input_name = session.get_inputs()[0].name
    
    # æ¨ç†
    result = session.run(None, {input_name: input_data})
    
    return result[0]

# å®Œæ•´ONNXå·¥ä½œæµ
def complete_onnx_workflow():
    # 1. åˆ›å»ºæ¨¡å‹
    model = SimpleModel()
    model.eval()
    
    # 2. å¯¼å‡ºåˆ°ONNX
    dummy_input = torch.randn(1, 3, 32, 32)
    onnx_model = export_to_onnx(model, dummy_input)
    
    # 3. ONNXæ¨ç†
    input_numpy = dummy_input.numpy()
    onnx_output = onnx_inference("model.onnx", input_numpy)
    
    # 4. éªŒè¯ç»“æœ
    torch_output = model(dummy_input).detach().numpy()
    
    print(f"è¾“å‡ºå·®å¼‚: {np.mean(np.abs(onnx_output - torch_output)):.6f}")
    
    # 5. æ€§èƒ½æµ‹è¯•
    import time
    
    # PyTorchæ¨ç†
    start = time.time()
    for _ in range(100):
        _ = model(dummy_input)
    pytorch_time = time.time() - start
    
    # ONNXæ¨ç†
    start = time.time()
    for _ in range(100):
        _ = onnx_inference("model.onnx", input_numpy)
    onnx_time = time.time() - start
    
    print(f"PyTorchæ—¶é—´: {pytorch_time:.3f}s")
    print(f"ONNXæ—¶é—´: {onnx_time:.3f}s")
    print(f"åŠ é€Ÿæ¯”: {pytorch_time/onnx_time:.2f}x")
```

## 3. é‡åŒ–æŠ€æœ¯ âš¡

```python
import torch.quantization as quant

# åŠ¨æ€é‡åŒ–ï¼ˆæœ€ç®€å•ï¼‰
def dynamic_quantization(model):
    """åŠ¨æ€é‡åŒ–ï¼šæ¨ç†æ—¶é‡åŒ–"""
    quantized_model = quant.quantize_dynamic(
        model,
        {nn.Linear, nn.Conv2d},  # è¦é‡åŒ–çš„å±‚ç±»å‹
        dtype=torch.qint8
    )
    return quantized_model

# é™æ€é‡åŒ–ï¼ˆéœ€è¦æ ¡å‡†ï¼‰
def static_quantization(model, calibration_loader):
    """é™æ€é‡åŒ–ï¼šéœ€è¦æ ¡å‡†æ•°æ®"""
    # 1. å‡†å¤‡æ¨¡å‹
    model.eval()
    model.qconfig = quant.get_default_qconfig('fbgemm')
    
    # 2. èåˆå±‚ï¼ˆConv+BN+ReLUï¼‰
    model_fused = quant.fuse_modules(model, [['conv', 'bn', 'relu']])
    
    # 3. å‡†å¤‡é‡åŒ–
    model_prepared = quant.prepare(model_fused)
    
    # 4. æ ¡å‡†
    with torch.no_grad():
        for data, _ in calibration_loader:
            model_prepared(data)
    
    # 5. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    model_quantized = quant.convert(model_prepared)
    
    return model_quantized

# QATï¼ˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼‰
class QATModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.quant = quant.QuantStub()
        self.conv = nn.Conv2d(3, 64, 3)
        self.bn = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(64 * 30 * 30, 10)
        self.dequant = quant.DeQuantStub()
    
    def forward(self, x):
        x = self.quant(x)
        x = self.relu(self.bn(self.conv(x)))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        x = self.dequant(x)
        return x

def quantization_aware_training(model, train_loader, epochs=10):
    """é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ"""
    # 1. å‡†å¤‡QAT
    model.qconfig = quant.get_default_qat_qconfig('fbgemm')
    model_prepared = quant.prepare_qat(model)
    
    # 2. è®­ç»ƒ
    optimizer = torch.optim.Adam(model_prepared.parameters())
    criterion = nn.CrossEntropyLoss()
    
    model_prepared.train()
    for epoch in range(epochs):
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model_prepared(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
    
    # 3. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    model_prepared.eval()
    model_quantized = quant.convert(model_prepared)
    
    return model_quantized

# é‡åŒ–æ•ˆæœè¯„ä¼°
def evaluate_quantization(original_model, quantized_model, test_loader):
    """è¯„ä¼°é‡åŒ–æ•ˆæœ"""
    import os
    
    # æ¨¡å‹å¤§å°
    def get_model_size(model):
        torch.save(model.state_dict(), "temp.pt")
        size = os.path.getsize("temp.pt") / 1e6  # MB
        os.remove("temp.pt")
        return size
    
    original_size = get_model_size(original_model)
    quantized_size = get_model_size(quantized_model)
    
    print(f"åŸå§‹æ¨¡å‹å¤§å°: {original_size:.2f} MB")
    print(f"é‡åŒ–æ¨¡å‹å¤§å°: {quantized_size:.2f} MB")
    print(f"å‹ç¼©æ¯”: {original_size/quantized_size:.2f}x")
    
    # æ¨ç†é€Ÿåº¦
    import time
    
    def benchmark_speed(model, test_loader, num_batches=100):
        model.eval()
        start = time.time()
        with torch.no_grad():
            for i, (data, _) in enumerate(test_loader):
                if i >= num_batches:
                    break
                _ = model(data)
        return time.time() - start
    
    original_time = benchmark_speed(original_model, test_loader)
    quantized_time = benchmark_speed(quantized_model, test_loader)
    
    print(f"åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´: {original_time:.3f}s")
    print(f"é‡åŒ–æ¨¡å‹æ¨ç†æ—¶é—´: {quantized_time:.3f}s")
    print(f"åŠ é€Ÿæ¯”: {original_time/quantized_time:.2f}x")
```

## 4. æ¨¡å‹å‰ªæ âœ‚ï¸

```python
import torch.nn.utils.prune as prune

def structured_pruning(model, pruning_rate=0.3):
    """ç»“æ„åŒ–å‰ªæ"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            # L1ç»“æ„åŒ–å‰ªæ
            prune.ln_structured(module, name='weight', amount=pruning_rate, n=1, dim=0)
        elif isinstance(module, nn.Linear):
            # L2éç»“æ„åŒ–å‰ªæ
            prune.l2_unstructured(module, name='weight', amount=pruning_rate)
    
    return model

def iterative_pruning(model, train_loader, test_loader, pruning_rates=[0.1, 0.2, 0.3]):
    """è¿­ä»£å‰ªæ+å¾®è°ƒ"""
    best_accuracy = evaluate_model(model, test_loader)
    
    for rate in pruning_rates:
        print(f"å‰ªæç‡: {rate}")
        
        # å‰ªæ
        model = structured_pruning(model, rate)
        
        # å¾®è°ƒ
        finetune_model(model, train_loader, epochs=5)
        
        # è¯„ä¼°
        accuracy = evaluate_model(model, test_loader)
        print(f"ç²¾åº¦: {accuracy:.2f}%")
        
        if accuracy < best_accuracy * 0.95:  # ç²¾åº¦ä¸‹é™è¶…è¿‡5%
            print("ç²¾åº¦ä¸‹é™è¿‡å¤šï¼Œåœæ­¢å‰ªæ")
            break
    
    # æ°¸ä¹…ç§»é™¤å‰ªæçš„æƒé‡
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            prune.remove(module, 'weight')
    
    return model

def evaluate_model(model, test_loader):
    """è¯„ä¼°æ¨¡å‹ç²¾åº¦"""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    return 100 * correct / total

def finetune_model(model, train_loader, epochs=5):
    """å¾®è°ƒæ¨¡å‹"""
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(epochs):
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
```

## 5. TensorRTéƒ¨ç½² ğŸï¸

```python
def tensorrt_deployment(onnx_path):
    """TensorRTéƒ¨ç½²ï¼ˆNVIDIA GPUï¼‰"""
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    
    # åˆ›å»ºbuilder
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(
        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    )
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # è§£æONNX
    with open(onnx_path, 'rb') as f:
        if not parser.parse(f.read()):
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            return None
    
    # é…ç½®
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB
    
    # INT8é‡åŒ–
    if builder.platform_has_fast_int8:
        config.set_flag(trt.BuilderFlag.INT8)
        # éœ€è¦æ ¡å‡†æ•°æ®é›†
        # config.int8_calibrator = create_calibrator(calibration_data)
    
    # FP16
    if builder.platform_has_fast_fp16:
        config.set_flag(trt.BuilderFlag.FP16)
    
    # æ„å»ºå¼•æ“
    engine = builder.build_engine(network, config)
    
    # ä¿å­˜å¼•æ“
    with open("model.trt", "wb") as f:
        f.write(engine.serialize())
    
    return engine

class TensorRTInference:
    def __init__(self, engine_path):
        import tensorrt as trt
        import pycuda.driver as cuda
        
        # åŠ è½½å¼•æ“
        with open(engine_path, "rb") as f:
            self.engine = trt.Runtime(trt.Logger()).deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
        
        # åˆ†é…ç¼“å†²åŒº
        self.inputs = []
        self.outputs = []
        self.bindings = []
        
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding))
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))
            
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            
            self.bindings.append(int(device_mem))
            
            if self.engine.binding_is_input(binding):
                self.inputs.append({'host': host_mem, 'device': device_mem})
            else:
                self.outputs.append({'host': host_mem, 'device': device_mem})
    
    def infer(self, input_data):
        import pycuda.driver as cuda
        
        # å¤åˆ¶è¾“å…¥æ•°æ®
        np.copyto(self.inputs[0]['host'], input_data.ravel())
        
        # ä¼ è¾“åˆ°GPU
        stream = cuda.Stream()
        for inp in self.inputs:
            cuda.memcpy_htod_async(inp['device'], inp['host'], stream)
        
        # æ‰§è¡Œæ¨ç†
        self.context.execute_async_v2(bindings=self.bindings, stream_handle=stream.handle)
        
        # ä¼ è¾“å›CPU
        for out in self.outputs:
            cuda.memcpy_dtoh_async(out['host'], out['device'], stream)
        
        stream.synchronize()
        
        return [out['host'] for out in self.outputs]
```

## 6. ç§»åŠ¨ç«¯éƒ¨ç½² ğŸ“±

```python
# PyTorch Mobile
def prepare_mobile_model(model):
    """å‡†å¤‡ç§»åŠ¨ç«¯æ¨¡å‹"""
    model.eval()
    example = torch.rand(1, 3, 224, 224)
    
    # ä¼˜åŒ–ä¸ºç§»åŠ¨ç«¯
    from torch.utils.mobile_optimizer import optimize_for_mobile
    
    traced_model = torch.jit.trace(model, example)
    optimized_model = optimize_for_mobile(traced_model)
    
    # ä¿å­˜
    optimized_model._save_for_lite_interpreter("model_mobile.ptl")
    
    print("æ¨¡å‹å·²ä¼˜åŒ–å¹¶ä¿å­˜ä¸ºç§»åŠ¨ç«¯æ ¼å¼")
    return optimized_model

# Core MLï¼ˆiOSï¼‰
def export_to_coreml(model, example_input):
    """å¯¼å‡ºåˆ°Core ML"""
    import coremltools as ct
    
    model.eval()
    traced_model = torch.jit.trace(model, example_input)
    
    # è½¬æ¢
    ml_model = ct.convert(
        traced_model,
        inputs=[ct.TensorType(shape=example_input.shape)],
        minimum_deployment_target=ct.target.iOS14
    )
    
    # ä¿å­˜
    ml_model.save("model.mlmodel")
    
    return ml_model

# TFLiteï¼ˆAndroidï¼‰
def export_to_tflite(onnx_path):
    """é€šè¿‡ONNXå¯¼å‡ºåˆ°TFLite"""
    import tf2onnx
    import tensorflow as tf
    
    # ONNX -> TensorFlow
    tf_model = tf2onnx.convert.from_onnx(onnx_path)
    
    # TensorFlow -> TFLite
    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    
    tflite_model = converter.convert()
    
    # ä¿å­˜
    with open('model.tflite', 'wb') as f:
        f.write(tflite_model)
    
    return tflite_model
```

## 7. æœåŠ¡åŒ–éƒ¨ç½² ğŸŒ

```python
# FastAPIæœåŠ¡
from fastapi import FastAPI, File, UploadFile
from PIL import Image
import io

app = FastAPI()

# å…¨å±€æ¨¡å‹
model = torch.jit.load("model_scripted.pt")
model.eval()

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """æ¨¡å‹æ¨ç†API"""
    # è¯»å–å›¾åƒ
    contents = await file.read()
    image = Image.open(io.BytesIO(contents))
    
    # é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    input_tensor = transform(image).unsqueeze(0)
    
    # æ¨ç†
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = torch.softmax(output, dim=1)
        top5_prob, top5_idx = torch.topk(probabilities, 5)
    
    # è¿”å›ç»“æœ
    results = []
    for i in range(5):
        results.append({
            "class": int(top5_idx[0][i]),
            "probability": float(top5_prob[0][i])
        })
    
    return {"predictions": results}

# Dockeréƒ¨ç½²
dockerfile_content = """
FROM python:3.8-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
"""

# Triton Inference Serveré…ç½®
triton_config = """
name: "resnet50"
platform: "pytorch_libtorch"
max_batch_size: 8
input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [3, 224, 224]
  }
]
output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [1000]
  }
]
instance_group [
  {
    count: 2
    kind: KIND_GPU
  }
]
"""
```

## éƒ¨ç½²æ£€æŸ¥æ¸…å• âœ…

```python
def deployment_checklist(model, test_loader):
    """éƒ¨ç½²å‰çš„å®Œæ•´æ£€æŸ¥"""
    print("=== éƒ¨ç½²æ£€æŸ¥æ¸…å• ===\n")
    
    checks = {
        "æ¨¡å‹è¯„ä¼°æ¨¡å¼": model.training == False,
        "æ‰¹å½’ä¸€åŒ–å›ºå®š": all(not m.training for m in model.modules() 
                           if isinstance(m, nn.BatchNorm2d)),
        "æ¢¯åº¦è®¡ç®—å…³é—­": not any(p.requires_grad for p in model.parameters()),
        "è¾“å…¥å½¢çŠ¶éªŒè¯": True,  # éœ€è¦å®é™…æµ‹è¯•
        "è¾“å‡ºèŒƒå›´æ£€æŸ¥": True,  # éœ€è¦å®é™…æµ‹è¯•
        "æ€§èƒ½åŸºå‡†æµ‹è¯•": True,  # å·²å®Œæˆ
        "é‡åŒ–ç²¾åº¦æŸå¤±": True,  # <5%
        "æ¨¡å‹å¤§å°ä¼˜åŒ–": True,  # å·²å‹ç¼©
    }
    
    for check, status in checks.items():
        status_str = "âœ…" if status else "âŒ"
        print(f"{status_str} {check}")
    
    print("\n=== æ¨èéƒ¨ç½²æ–¹æ¡ˆ ===")
    print("â€¢ æœåŠ¡å™¨GPU: TensorRT + Triton")
    print("â€¢ æœåŠ¡å™¨CPU: ONNX Runtime + Docker")
    print("â€¢ ç§»åŠ¨ç«¯iOS: Core ML")
    print("â€¢ ç§»åŠ¨ç«¯Android: TFLite")
    print("â€¢ è¾¹ç¼˜è®¾å¤‡: OpenVINO")
    print("â€¢ Webæµè§ˆå™¨: ONNX.js / TensorFlow.js")
```

## æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„éƒ¨ç½²æ–¹æ¡ˆ**
   - å»¶è¿Ÿæ•æ„Ÿï¼šTensorRT/OpenVINO
   - é€šç”¨æ€§ï¼šONNX Runtime
   - ç§»åŠ¨ç«¯ï¼šTFLite/Core ML

2. **ä¼˜åŒ–ç­–ç•¥ç»„åˆ**
   - é‡åŒ– + å‰ªæ + çŸ¥è¯†è’¸é¦
   - å…ˆå‰ªæï¼Œåé‡åŒ–
   - ä¿æŒç²¾åº¦æŸå¤±<5%

3. **æ€§èƒ½ç›‘æ§**
   - å»¶è¿Ÿã€ååé‡ã€å†…å­˜å ç”¨
   - A/Bæµ‹è¯•
   - æŒç»­ä¼˜åŒ–

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [NLPæ¨¡å‹](nlp_models.md) - BERTã€GPTã€T5
- [LLMéƒ¨ç½²](llm_deployment.md) - å¤§æ¨¡å‹éƒ¨ç½²æŠ€æœ¯