# Vision Transformer (ViT) å®Œå…¨æŒ‡å— ğŸ‘ï¸

Transformeråœ¨NLPé¢†åŸŸå–å¾—å·¨å¤§æˆåŠŸåï¼Œç»ˆäºæ¥åˆ°äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚ViTè¯æ˜äº†ï¼šå›¾åƒä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼

## 1. ViT - å¼€åˆ›æ€§å·¥ä½œ ğŸ¨

### æ ¸å¿ƒæ€æƒ³
å°†å›¾åƒåˆ‡åˆ†æˆå°å—ï¼ˆpatchesï¼‰ï¼ŒæŠŠæ¯ä¸ªå—å½“ä½œä¸€ä¸ªtokenï¼Œç„¶åç”¨æ ‡å‡†çš„Transformerå¤„ç†ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from einops import rearrange, repeat
from einops.layers.torch import Rearrange

# Multi-Head Self-Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):
        super().__init__()
        inner_dim = dim_head * heads
        self.heads = heads
        self.scale = dim_head ** -0.5
        
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        b, n, _ = x.shape
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)
        
        # Attention
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = F.softmax(dots, dim=-1)
        
        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

# Feed Forward Network
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        return self.net(x)

# Transformer Block
class TransformerBlock(nn.Module):
    def __init__(self, dim, heads, dim_head, mlp_dim, dropout=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = MultiHeadAttention(dim, heads, dim_head, dropout)
        self.norm2 = nn.LayerNorm(dim)
        self.ff = FeedForward(dim, mlp_dim, dropout)
    
    def forward(self, x):
        x = self.attn(self.norm1(x)) + x
        x = self.ff(self.norm2(x)) + x
        return x

# Vision Transformer
class VisionTransformer(nn.Module):
    def __init__(self, image_size=224, patch_size=16, num_classes=1000, dim=768,
                 depth=12, heads=12, mlp_dim=3072, dropout=0.1, emb_dropout=0.1):
        super().__init__()
        assert image_size % patch_size == 0
        
        num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size ** 2
        
        self.patch_size = patch_size
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout = nn.Dropout(emb_dropout)
        
        # Patch embedding
        self.patch_to_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', 
                     p1=patch_size, p2=patch_size),
            nn.Linear(patch_dim, dim)
        )
        
        # Transformer blocks
        self.transformer = nn.Sequential(
            *[TransformerBlock(dim, heads, dim // heads, mlp_dim, dropout)
              for _ in range(depth)]
        )
        
        self.norm = nn.LayerNorm(dim)
        self.to_cls_token = nn.Identity()
        
        # Classification head
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )
    
    def forward(self, img):
        # Patch embedding
        x = self.patch_to_embedding(img)
        b, n, _ = x.shape
        
        # Add CLS token
        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)
        x = torch.cat((cls_tokens, x), dim=1)
        
        # Add positional embedding
        x += self.pos_embedding[:, :(n + 1)]
        x = self.dropout(x)
        
        # Transformer
        x = self.transformer(x)
        x = self.norm(x)
        
        # Classification
        cls_token = x[:, 0]
        return self.mlp_head(cls_token)

# åˆ›å»ºViTæ¨¡å‹
def create_vit(variant='base'):
    """åˆ›å»ºä¸åŒè§„æ¨¡çš„ViTæ¨¡å‹"""
    configs = {
        'tiny': dict(dim=192, depth=12, heads=3, mlp_dim=768),
        'small': dict(dim=384, depth=12, heads=6, mlp_dim=1536),
        'base': dict(dim=768, depth=12, heads=12, mlp_dim=3072),
        'large': dict(dim=1024, depth=24, heads=16, mlp_dim=4096),
        'huge': dict(dim=1280, depth=32, heads=16, mlp_dim=5120),
    }
    
    return VisionTransformer(**configs[variant])

# æµ‹è¯•
model = create_vit('base')
x = torch.randn(2, 3, 224, 224)
output = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
```

## 2. CLIP - è§†è§‰-è¯­è¨€å¯¹é½ ğŸ”—

### æ ¸å¿ƒæ€æƒ³
é€šè¿‡å¯¹æ¯”å­¦ä¹ åŒæ—¶è®­ç»ƒè§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œå®ç°é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚

```python
import clip  # pip install git+https://github.com/openai/CLIP.git

class CLIP(nn.Module):
    def __init__(self, vision_encoder, text_encoder, embed_dim=512):
        super().__init__()
        self.vision_encoder = vision_encoder
        self.text_encoder = text_encoder
        
        # æŠ•å½±å±‚
        self.vision_proj = nn.Linear(vision_encoder.output_dim, embed_dim)
        self.text_proj = nn.Linear(text_encoder.output_dim, embed_dim)
        
        # æ¸©åº¦å‚æ•°
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def encode_image(self, image):
        features = self.vision_encoder(image)
        features = self.vision_proj(features)
        return F.normalize(features, dim=-1)
    
    def encode_text(self, text):
        features = self.text_encoder(text)
        features = self.text_proj(features)
        return F.normalize(features, dim=-1)
    
    def forward(self, image, text):
        image_features = self.encode_image(image)
        text_features = self.encode_text(text)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()
        
        return logits_per_image, logits_per_text

# CLIPè®­ç»ƒ
def train_clip(model, dataloader, epochs=10):
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    
    for epoch in range(epochs):
        for batch in dataloader:
            images, texts = batch
            
            # å‰å‘ä¼ æ’­
            logits_per_image, logits_per_text = model(images, texts)
            
            # å¯¹æ¯”å­¦ä¹ æŸå¤±
            batch_size = images.shape[0]
            labels = torch.arange(batch_size).to(images.device)
            
            loss_i = F.cross_entropy(logits_per_image, labels)
            loss_t = F.cross_entropy(logits_per_text, labels)
            loss = (loss_i + loss_t) / 2
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

# é›¶æ ·æœ¬åˆ†ç±»
def zero_shot_classification(model, image, text_prompts):
    """ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»"""
    with torch.no_grad():
        image_features = model.encode_image(image)
        text_features = torch.stack([
            model.encode_text(prompt) for prompt in text_prompts
        ])
        
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    
    return similarity
```

## 3. MAE - è‡ªç›‘ç£é¢„è®­ç»ƒ ğŸ­

### æ ¸å¿ƒæ€æƒ³
é®ç›–å›¾åƒçš„å¤§éƒ¨åˆ†patchesï¼ˆ75%ï¼‰ï¼Œè®©æ¨¡å‹é‡å»ºè¢«é®ç›–çš„éƒ¨åˆ†ã€‚

```python
class MAE(nn.Module):
    def __init__(self, encoder, decoder, mask_ratio=0.75):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.mask_ratio = mask_ratio
    
    def random_masking(self, x, mask_ratio):
        """éšæœºé®ç›–patches"""
        N, L, D = x.shape  # batch, length, dim
        len_keep = int(L * (1 - mask_ratio))
        
        # ç”Ÿæˆéšæœºå™ªå£°
        noise = torch.rand(N, L, device=x.device)
        
        # æ’åºè·å–è¦ä¿ç•™çš„ç´¢å¼•
        ids_shuffle = torch.argsort(noise, dim=1)
        ids_restore = torch.argsort(ids_shuffle, dim=1)
        
        # ä¿ç•™æœªé®ç›–çš„patches
        ids_keep = ids_shuffle[:, :len_keep]
        x_masked = torch.gather(x, dim=1, 
                               index=ids_keep.unsqueeze(-1).repeat(1, 1, D))
        
        # ç”Ÿæˆmask
        mask = torch.ones([N, L], device=x.device)
        mask[:, :len_keep] = 0
        mask = torch.gather(mask, dim=1, index=ids_restore)
        
        return x_masked, mask, ids_restore
    
    def forward(self, imgs):
        # Encoder
        latent, mask, ids_restore = self.encoder(imgs, self.mask_ratio)
        
        # Decoder
        pred = self.decoder(latent, ids_restore)
        
        # è®¡ç®—æŸå¤±ï¼ˆåªåœ¨è¢«é®ç›–çš„éƒ¨åˆ†ï¼‰
        target = self.patchify(imgs)
        loss = (pred - target) ** 2
        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch
        
        loss = (loss * mask).sum() / mask.sum()
        
        return loss, pred, mask
    
    def patchify(self, imgs):
        """å°†å›¾åƒè½¬æ¢ä¸ºpatches"""
        p = self.encoder.patch_size
        h = w = imgs.shape[2] // p
        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))
        x = torch.einsum('nchpwq->nhwpqc', x)
        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))
        return x
```

## 4. DINO - è‡ªè’¸é¦ ğŸ¦•

```python
class DINO(nn.Module):
    def __init__(self, student, teacher, center_momentum=0.9):
        super().__init__()
        self.student = student
        self.teacher = teacher
        self.center = nn.Parameter(torch.zeros(1, student.output_dim))
        self.center_momentum = center_momentum
        
        # Teacherä½¿ç”¨EMAæ›´æ–°
        for p in self.teacher.parameters():
            p.requires_grad = False
    
    @torch.no_grad()
    def update_teacher(self, momentum=0.996):
        """EMAæ›´æ–°teacherç½‘ç»œ"""
        for param_s, param_t in zip(self.student.parameters(), 
                                    self.teacher.parameters()):
            param_t.data = momentum * param_t.data + (1 - momentum) * param_s.data
    
    @torch.no_grad()
    def update_center(self, teacher_output):
        """æ›´æ–°center"""
        batch_center = torch.mean(teacher_output, dim=0, keepdim=True)
        self.center = self.center * self.center_momentum + \
                     batch_center * (1 - self.center_momentum)
    
    def forward(self, images):
        # å¤šå°ºåº¦å¢å¼º
        student_views = images[:2]  # 2ä¸ªglobal views
        teacher_views = images[2:]  # å¤šä¸ªlocal views
        
        # Studentå‰å‘
        student_output = torch.cat([
            self.student(view) for view in student_views
        ])
        
        # Teacherå‰å‘
        with torch.no_grad():
            teacher_output = torch.cat([
                self.teacher(view) for view in teacher_views
            ])
            teacher_output = F.softmax((teacher_output - self.center) / 0.04, dim=-1)
        
        # è®¡ç®—æŸå¤±
        student_output = student_output / 0.1
        student_log_sm = F.log_softmax(student_output, dim=-1)
        
        loss = -torch.mean(torch.sum(teacher_output * student_log_sm, dim=-1))
        
        self.update_center(teacher_output)
        
        return loss
```

## 5. å®æˆ˜åº”ç”¨

```python
# ä½¿ç”¨é¢„è®­ç»ƒViT
from transformers import ViTForImageClassification, ViTImageProcessor

def use_pretrained_vit():
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
    
    # å¤„ç†å›¾åƒ
    from PIL import Image
    image = Image.open("test.jpg")
    inputs = processor(images=image, return_tensors="pt")
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = logits.argmax(-1).item()
    
    print(f"é¢„æµ‹ç±»åˆ«: {model.config.id2label[predicted_class]}")

# å¾®è°ƒViT
def finetune_vit(train_dataset, val_dataset, num_classes=10):
    from transformers import TrainingArguments, Trainer
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = ViTForImageClassification.from_pretrained(
        'google/vit-base-patch16-224',
        num_labels=num_classes,
        ignore_mismatched_sizes=True
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir='./vit-finetuned',
        num_train_epochs=10,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
    )
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )
    
    # è®­ç»ƒ
    trainer.train()
    
    return model

# å¯è§†åŒ–æ³¨æ„åŠ›
def visualize_attention(model, image):
    """å¯è§†åŒ–ViTçš„æ³¨æ„åŠ›å›¾"""
    import matplotlib.pyplot as plt
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    model.eval()
    with torch.no_grad():
        outputs = model(image, output_attentions=True)
        attentions = outputs.attentions  # List of attention matrices
    
    # å–æœ€åä¸€å±‚çš„æ³¨æ„åŠ›
    attention = attentions[-1]  # [batch, heads, seq, seq]
    attention = attention.mean(dim=1)  # å¹³å‡æ‰€æœ‰heads
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # åŸå›¾
    axes[0].imshow(image[0].permute(1, 2, 0))
    axes[0].set_title('åŸå›¾')
    axes[0].axis('off')
    
    # æ³¨æ„åŠ›å›¾
    axes[1].imshow(attention[0, 0, 1:].reshape(14, 14), cmap='hot')
    axes[1].set_title('æ³¨æ„åŠ›å›¾')
    axes[1].axis('off')
    
    plt.show()
```

## 6. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# Flash Attention
class FlashMultiHeadAttention(nn.Module):
    """ä½¿ç”¨Flash AttentionåŠ é€Ÿ"""
    def __init__(self, dim, heads=8):
        super().__init__()
        from flash_attn import flash_attn_func
        self.flash_attn = flash_attn_func
        # ... å…¶ä»–åˆå§‹åŒ–
    
    def forward(self, x):
        # ä½¿ç”¨Flash Attention
        out = self.flash_attn(q, k, v)
        return out

# æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

def train_with_amp(model, dataloader):
    scaler = GradScaler()
    optimizer = torch.optim.AdamW(model.parameters())
    
    for batch in dataloader:
        with autocast():
            loss = model(batch)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

# Token Pruning
def token_pruning(x, keep_ratio=0.5):
    """åŠ¨æ€å‰ªæä¸é‡è¦çš„tokens"""
    # è®¡ç®—tokené‡è¦æ€§
    cls_attn = x[:, 0, 1:]  # CLS tokenå¯¹å…¶ä»–tokensçš„æ³¨æ„åŠ›
    
    # ä¿ç•™æœ€é‡è¦çš„tokens
    _, idx = torch.topk(cls_attn, int(keep_ratio * cls_attn.size(-1)))
    
    return torch.gather(x, dim=1, index=idx.unsqueeze(-1).expand(-1, -1, x.size(-1)))
```

## æ¨¡å‹é€‰æ‹©æŒ‡å—

| æ¨¡å‹ | å‚æ•°é‡ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|--------|------|----------|
| ViT-B/16 | 86M | æ ‡å‡†ViT | é€šç”¨å›¾åƒåˆ†ç±» |
| ViT-L/14 | 304M | å¤§æ¨¡å‹ | é«˜ç²¾åº¦éœ€æ±‚ |
| DeiT | 86M | æ•°æ®é«˜æ•ˆ | å°æ•°æ®é›† |
| Swin Transformer | 88M | å±‚çº§ç»“æ„ | æ£€æµ‹ã€åˆ†å‰² |
| CLIP | 428M | å¤šæ¨¡æ€ | é›¶æ ·æœ¬å­¦ä¹  |
| MAE | 86M | è‡ªç›‘ç£ | é¢„è®­ç»ƒ |

## æœ€ä½³å®è·µ

1. **æ•°æ®å¢å¼ºå¾ˆé‡è¦**ï¼šViTéœ€è¦å¤§é‡æ•°æ®ï¼Œå¼ºå¢å¼ºå¿…ä¸å¯å°‘
2. **é¢„è®­ç»ƒæƒé‡**ï¼šå°½é‡ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
3. **ä½ç½®ç¼–ç **ï¼šå¯å­¦ä¹ ä½ç½®ç¼–ç é€šå¸¸æ›´å¥½
4. **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼šç”¨äºæ¨¡å‹è§£é‡Š
5. **æ•ˆç‡ä¼˜åŒ–**ï¼šè€ƒè™‘Token pruningã€Flash Attention

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [ç›®æ ‡æ£€æµ‹](object_detection.md) - YOLOç³»åˆ—å’ŒDETR
- [PyTorchéƒ¨ç½²](deployment.md) - æ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²
- [NLPæ¨¡å‹](nlp_models.md) - BERTã€GPTç­‰