# æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§å¤§å…¨ ğŸš€

æŒæ¡è®©æ¨¡å‹è®­ç»ƒæ›´å¿«ã€æ›´ç¨³å®šã€æ›´å‡†ç¡®çš„æ‰€æœ‰æŠ€å·§ã€‚

## 1. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ ğŸ“ˆ

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import *
import numpy as np
import matplotlib.pyplot as plt

class LearningRateSchedulers:
    """å­¦ä¹ ç‡è°ƒåº¦å™¨å®ç°"""
    
    def __init__(self, optimizer, initial_lr=0.1):
        self.optimizer = optimizer
        self.initial_lr = initial_lr
        self.schedulers = {}
    
    def create_schedulers(self, epochs=100):
        """åˆ›å»ºå„ç§å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # é˜¶æ¢¯è¡°å‡
        self.schedulers['StepLR'] = StepLR(
            self.optimizer, step_size=30, gamma=0.1
        )
        
        # å¤šé˜¶æ¢¯è¡°å‡
        self.schedulers['MultiStepLR'] = MultiStepLR(
            self.optimizer, milestones=[30, 60, 90], gamma=0.1
        )
        
        # æŒ‡æ•°è¡°å‡
        self.schedulers['ExponentialLR'] = ExponentialLR(
            self.optimizer, gamma=0.95
        )
        
        # ä½™å¼¦é€€ç«
        self.schedulers['CosineAnnealingLR'] = CosineAnnealingLR(
            self.optimizer, T_max=epochs
        )
        
        # å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«
        self.schedulers['CosineAnnealingWarmRestarts'] = \
            CosineAnnealingWarmRestarts(
                self.optimizer, T_0=10, T_mult=2
            )
        
        # ReduceLROnPlateauï¼ˆéœ€è¦éªŒè¯æŸå¤±ï¼‰
        self.schedulers['ReduceLROnPlateau'] = ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=10
        )
        
        # OneCycleLR
        self.schedulers['OneCycleLR'] = OneCycleLR(
            self.optimizer, max_lr=0.1, 
            steps_per_epoch=100, epochs=epochs
        )
        
        return self.schedulers
    
    def visualize_schedules(self, epochs=100):
        """å¯è§†åŒ–ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦"""
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        axes = axes.flatten()
        
        for idx, (name, scheduler) in enumerate(self.schedulers.items()):
            if idx >= len(axes):
                break
            
            lrs = []
            optimizer_temp = optim.SGD([torch.randn(2, 2)], lr=self.initial_lr)
            
            if name == 'OneCycleLR':
                scheduler_temp = OneCycleLR(
                    optimizer_temp, max_lr=0.1,
                    steps_per_epoch=1, epochs=epochs
                )
            elif name == 'ReduceLROnPlateau':
                scheduler_temp = ReduceLROnPlateau(
                    optimizer_temp, mode='min', factor=0.5, patience=10
                )
            elif name == 'CosineAnnealingWarmRestarts':
                scheduler_temp = CosineAnnealingWarmRestarts(
                    optimizer_temp, T_0=10, T_mult=2
                )
            elif name == 'CosineAnnealingLR':
                scheduler_temp = CosineAnnealingLR(
                    optimizer_temp, T_max=epochs
                )
            elif name == 'StepLR':
                scheduler_temp = StepLR(
                    optimizer_temp, step_size=30, gamma=0.1
                )
            elif name == 'MultiStepLR':
                scheduler_temp = MultiStepLR(
                    optimizer_temp, milestones=[30, 60, 90], gamma=0.1
                )
            else:  # ExponentialLR
                scheduler_temp = ExponentialLR(
                    optimizer_temp, gamma=0.95
                )
            
            for epoch in range(epochs):
                lrs.append(optimizer_temp.param_groups[0]['lr'])
                if name == 'ReduceLROnPlateau':
                    # æ¨¡æ‹ŸéªŒè¯æŸå¤±
                    val_loss = np.random.random()
                    scheduler_temp.step(val_loss)
                else:
                    scheduler_temp.step()
            
            axes[idx].plot(lrs)
            axes[idx].set_title(name)
            axes[idx].set_xlabel('Epoch')
            axes[idx].set_ylabel('Learning Rate')
            axes[idx].grid(True, alpha=0.3)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(self.schedulers), len(axes)):
            axes[idx].set_visible(False)
        
        plt.tight_layout()
        plt.show()

# Warmupå®ç°
class WarmupScheduler:
    """å¸¦Warmupçš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer, warmup_epochs, initial_lr, target_lr):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.initial_lr = initial_lr
        self.target_lr = target_lr
        self.current_epoch = 0
    
    def step(self):
        """æ›´æ–°å­¦ä¹ ç‡"""
        if self.current_epoch < self.warmup_epochs:
            # çº¿æ€§warmup
            lr = self.initial_lr + (self.target_lr - self.initial_lr) * \
                 (self.current_epoch / self.warmup_epochs)
        else:
            # warmupåçš„è°ƒåº¦ç­–ç•¥
            lr = self.target_lr
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        self.current_epoch += 1
        return lr
```

## 2. æ¢¯åº¦å¤„ç†æŠ€å·§ ğŸ¯

```python
class GradientTechniques:
    """æ¢¯åº¦å¤„ç†æŠ€å·§"""
    
    @staticmethod
    def gradient_clipping(model, max_norm=1.0):
        """æ¢¯åº¦è£å‰ª"""
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
        
        # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        
        print(f"æ¢¯åº¦èŒƒæ•°: {total_norm:.4f}")
        if total_norm > max_norm:
            print(f"æ¢¯åº¦è¢«è£å‰ª: {total_norm:.4f} -> {max_norm}")
        
        return total_norm
    
    @staticmethod
    def gradient_accumulation(model, data_loader, optimizer, 
                            accumulation_steps=4):
        """æ¢¯åº¦ç´¯ç§¯"""
        model.train()
        optimizer.zero_grad()
        
        for i, (inputs, targets) in enumerate(data_loader):
            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            
            # æ ‡å‡†åŒ–æŸå¤±
            loss = loss / accumulation_steps
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                print(f"Step {i+1}: Updated weights")
    
    @staticmethod
    def gradient_penalty(discriminator, real_data, fake_data, lambda_gp=10):
        """æ¢¯åº¦æƒ©ç½šï¼ˆç”¨äºWGAN-GPï¼‰"""
        batch_size = real_data.size(0)
        epsilon = torch.rand(batch_size, 1, 1, 1).to(real_data.device)
        
        # æ’å€¼
        interpolated = epsilon * real_data + (1 - epsilon) * fake_data
        interpolated.requires_grad_(True)
        
        # è®¡ç®—åˆ¤åˆ«å™¨è¾“å‡º
        d_interpolated = discriminator(interpolated)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = torch.autograd.grad(
            outputs=d_interpolated,
            inputs=interpolated,
            grad_outputs=torch.ones_like(d_interpolated),
            create_graph=True,
            retain_graph=True
        )[0]
        
        # è®¡ç®—æ¢¯åº¦æƒ©ç½š
        gradients = gradients.view(batch_size, -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        
        return lambda_gp * gradient_penalty
    
    @staticmethod
    def gradient_centralization(optimizer):
        """æ¢¯åº¦ä¸­å¿ƒåŒ–"""
        for group in optimizer.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    # è®¡ç®—æ¢¯åº¦å‡å€¼
                    grad = p.grad.data
                    if len(grad.shape) > 1:
                        # å¯¹æ¯ä¸ªè¾“å‡ºé€šé“ä¸­å¿ƒåŒ–
                        grad -= grad.mean(dim=tuple(range(1, len(grad.shape))), 
                                        keepdim=True)
```

## 3. æ··åˆç²¾åº¦è®­ç»ƒ âš¡

```python
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTraining:
    """æ··åˆç²¾åº¦è®­ç»ƒ"""
    
    def __init__(self):
        self.scaler = GradScaler()
    
    def train_with_amp(self, model, train_loader, optimizer, epochs=5):
        """ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ"""
        model.cuda()
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.cuda(), target.cuda()
                
                optimizer.zero_grad()
                
                # è‡ªåŠ¨æ··åˆç²¾åº¦
                with autocast():
                    output = model(data)
                    loss = criterion(output, target)
                
                # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                # æ›´æ–°æƒé‡
                self.scaler.step(optimizer)
                self.scaler.update()
                
                if batch_idx % 100 == 0:
                    print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
    
    def manual_mixed_precision(self, model, x):
        """æ‰‹åŠ¨æ··åˆç²¾åº¦"""
        # å°†è¾“å…¥è½¬æ¢ä¸ºFP16
        x_fp16 = x.half()
        
        # æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆFP16ï¼‰
        with torch.cuda.amp.autocast():
            output_fp16 = model(x_fp16)
        
        # æŸå¤±è®¡ç®—ï¼ˆFP32ï¼‰
        output_fp32 = output_fp16.float()
        loss = output_fp32.mean()
        
        return loss
    
    def compare_precision(self):
        """æ¯”è¾ƒä¸åŒç²¾åº¦çš„å½±å“"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(100, 100).cuda()
        
        # FP32
        x_fp32 = x.float()
        result_fp32 = torch.matmul(x_fp32, x_fp32.T)
        
        # FP16
        x_fp16 = x.half()
        result_fp16 = torch.matmul(x_fp16, x_fp16.T)
        
        # BF16 (å¦‚æœæ”¯æŒ)
        if torch.cuda.is_bf16_supported():
            x_bf16 = x.bfloat16()
            result_bf16 = torch.matmul(x_bf16, x_bf16.T)
        
        # æ¯”è¾ƒç»“æœ
        print("ç²¾åº¦æ¯”è¾ƒ:")
        print(f"FP32 èŒƒå›´: [{result_fp32.min():.6f}, {result_fp32.max():.6f}]")
        print(f"FP16 èŒƒå›´: [{result_fp16.min():.6f}, {result_fp16.max():.6f}]")
        
        # è¯¯å·®åˆ†æ
        error = (result_fp32 - result_fp16.float()).abs().mean()
        print(f"FP32 vs FP16 å¹³å‡è¯¯å·®: {error:.6f}")
```

## 4. æ•°æ®å¢å¼ºæŠ€å·§ ğŸ–¼ï¸

```python
import torchvision.transforms as transforms

class DataAugmentation:
    """æ•°æ®å¢å¼ºæŠ€å·§"""
    
    @staticmethod
    def create_augmentation_pipeline(task='classification'):
        """åˆ›å»ºæ•°æ®å¢å¼ºç®¡é“"""
        if task == 'classification':
            train_transform = transforms.Compose([
                transforms.RandomResizedCrop(224),
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(15),
                transforms.ColorJitter(brightness=0.4, contrast=0.4, 
                                      saturation=0.4, hue=0.1),
                transforms.RandomGrayscale(p=0.1),
                transforms.RandomErasing(p=0.5, scale=(0.02, 0.33)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            val_transform = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
        
        return train_transform, val_transform
    
    @staticmethod
    def mixup(x, y, alpha=1.0):
        """Mixupæ•°æ®å¢å¼º"""
        batch_size = x.size(0)
        
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        
        index = torch.randperm(batch_size).to(x.device)
        mixed_x = lam * x + (1 - lam) * x[index]
        y_a, y_b = y, y[index]
        
        return mixed_x, y_a, y_b, lam
    
    @staticmethod
    def cutmix(x, y, alpha=1.0):
        """CutMixæ•°æ®å¢å¼º"""
        batch_size = x.size(0)
        
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        
        index = torch.randperm(batch_size).to(x.device)
        
        # ç”Ÿæˆè£å‰ªåŒºåŸŸ
        W = x.size(2)
        H = x.size(3)
        cut_rat = np.sqrt(1. - lam)
        cut_w = np.int(W * cut_rat)
        cut_h = np.int(H * cut_rat)
        
        cx = np.random.randint(W)
        cy = np.random.randint(H)
        
        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)
        
        # åº”ç”¨CutMix
        x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
        
        # è°ƒæ•´lambda
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
        
        return x, y, y[index], lam
    
    @staticmethod
    def auto_augment():
        """AutoAugmentç­–ç•¥"""
        from torchvision.transforms import autoaugment
        
        # ä½¿ç”¨ImageNetç­–ç•¥
        augmenter = autoaugment.AutoAugment(
            autoaugment.AutoAugmentPolicy.IMAGENET
        )
        
        transform = transforms.Compose([
            augmenter,
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform
```

## 5. è®­ç»ƒç¨³å®šæ€§æŠ€å·§ ğŸ”§

```python
class TrainingStability:
    """è®­ç»ƒç¨³å®šæ€§æŠ€å·§"""
    
    @staticmethod
    def label_smoothing(targets, n_classes, epsilon=0.1):
        """æ ‡ç­¾å¹³æ»‘"""
        with torch.no_grad():
            targets = targets * (1 - epsilon) + epsilon / n_classes
        return targets
    
    @staticmethod
    def stochastic_depth(x, survival_prob=0.8, training=True):
        """éšæœºæ·±åº¦ï¼ˆç”¨äºResNetç­‰ï¼‰"""
        if not training:
            return x
        
        # äºŒé¡¹åˆ†å¸ƒé‡‡æ ·
        survival = torch.bernoulli(torch.tensor(survival_prob))
        
        if survival:
            return x / survival_prob
        else:
            return torch.zeros_like(x)
    
    @staticmethod
    def gradient_checkpointing_example():
        """æ¢¯åº¦æ£€æŸ¥ç‚¹ç¤ºä¾‹"""
        import torch.utils.checkpoint as checkpoint
        
        class CheckpointedModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.layer1 = nn.Linear(1000, 1000)
                self.layer2 = nn.Linear(1000, 1000)
                self.layer3 = nn.Linear(1000, 10)
            
            def forward(self, x):
                # ä½¿ç”¨checkpointå‡å°‘å†…å­˜ä½¿ç”¨
                x = checkpoint.checkpoint(self.layer1, x)
                x = checkpoint.checkpoint(self.layer2, x)
                x = self.layer3(x)
                return x
        
        return CheckpointedModel()
    
    @staticmethod
    def ema_model(model, ema_decay=0.999):
        """æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¨¡å‹"""
        class EMA:
            def __init__(self, model, decay=0.999):
                self.model = model
                self.decay = decay
                self.shadow = {}
                self.backup = {}
                
                # åˆå§‹åŒ–shadowå‚æ•°
                for name, param in model.named_parameters():
                    if param.requires_grad:
                        self.shadow[name] = param.data.clone()
            
            def update(self):
                """æ›´æ–°EMAå‚æ•°"""
                for name, param in self.model.named_parameters():
                    if param.requires_grad:
                        new_average = (1.0 - self.decay) * param.data + \
                                     self.decay * self.shadow[name]
                        self.shadow[name] = new_average.clone()
            
            def apply_shadow(self):
                """åº”ç”¨EMAå‚æ•°"""
                for name, param in self.model.named_parameters():
                    if param.requires_grad:
                        self.backup[name] = param.data
                        param.data = self.shadow[name]
            
            def restore(self):
                """æ¢å¤åŸå§‹å‚æ•°"""
                for name, param in self.model.named_parameters():
                    if param.requires_grad:
                        param.data = self.backup[name]
                self.backup = {}
        
        return EMA(model, ema_decay)
```

## 6. åˆ†å¸ƒå¼è®­ç»ƒ ğŸŒ

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class DistributedTraining:
    """åˆ†å¸ƒå¼è®­ç»ƒæŠ€å·§"""
    
    @staticmethod
    def setup_distributed(rank, world_size):
        """è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ"""
        import os
        
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        
        # è®¾ç½®è®¾å¤‡
        torch.cuda.set_device(rank)
    
    @staticmethod
    def cleanup_distributed():
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        dist.destroy_process_group()
    
    @staticmethod
    def distributed_training_loop(rank, world_size, model, dataset):
        """åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯"""
        # è®¾ç½®åˆ†å¸ƒå¼
        DistributedTraining.setup_distributed(rank, world_size)
        
        # å°†æ¨¡å‹ç§»åˆ°GPU
        model = model.to(rank)
        
        # åŒ…è£…ä¸ºDDPæ¨¡å‹
        ddp_model = DDP(model, device_ids=[rank])
        
        # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
        from torch.utils.data.distributed import DistributedSampler
        sampler = DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank
        )
        
        # æ•°æ®åŠ è½½å™¨
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=32,
            sampler=sampler,
            num_workers=4
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)
        
        # è®­ç»ƒ
        for epoch in range(10):
            sampler.set_epoch(epoch)  # é‡è¦ï¼šæ¯ä¸ªepochéƒ½è¦è®¾ç½®
            
            for batch_idx, (data, target) in enumerate(dataloader):
                data, target = data.to(rank), target.to(rank)
                
                optimizer.zero_grad()
                output = ddp_model(data)
                loss = nn.CrossEntropyLoss()(output, target)
                loss.backward()
                optimizer.step()
                
                if batch_idx % 10 == 0 and rank == 0:
                    print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        # æ¸…ç†
        DistributedTraining.cleanup_distributed()
    
    @staticmethod
    def all_reduce_example():
        """All-Reduceç¤ºä¾‹"""
        # å‡è®¾åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­
        tensor = torch.ones(1).cuda()
        
        # All-reduceæ“ä½œ
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        
        # ç°åœ¨tensoråŒ…å«æ‰€æœ‰è¿›ç¨‹çš„å’Œ
        world_size = dist.get_world_size()
        tensor = tensor / world_size  # å¹³å‡
        
        return tensor
```

## 7. å†…å­˜ä¼˜åŒ–æŠ€å·§ ğŸ’¾

```python
class MemoryOptimization:
    """å†…å­˜ä¼˜åŒ–æŠ€å·§"""
    
    @staticmethod
    def optimize_dataloader(dataset):
        """ä¼˜åŒ–æ•°æ®åŠ è½½å™¨"""
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=32,
            shuffle=True,
            num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½
            pin_memory=True,  # å›ºå®šå†…å­˜
            persistent_workers=True,  # æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
            prefetch_factor=2  # é¢„å–å› å­
        )
        return dataloader
    
    @staticmethod
    def clear_cache():
        """æ¸…ç†GPUç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ
            print(f"GPUå†…å­˜å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
            print(f"GPUå†…å­˜å·²ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
    
    @staticmethod
    def inplace_operations():
        """åŸåœ°æ“ä½œç¤ºä¾‹"""
        x = torch.randn(1000, 1000)
        
        # éåŸåœ°æ“ä½œï¼ˆä½¿ç”¨æ›´å¤šå†…å­˜ï¼‰
        y = x + 1
        y = F.relu(y)
        
        # åŸåœ°æ“ä½œï¼ˆèŠ‚çœå†…å­˜ï¼‰
        x.add_(1)  # åŸåœ°åŠ æ³•
        x.relu_()  # åŸåœ°ReLU
        
        return x
    
    @staticmethod
    def memory_efficient_attention(Q, K, V, chunk_size=1024):
        """å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶"""
        batch_size, seq_len, d_model = Q.shape
        
        # åˆ†å—è®¡ç®—æ³¨æ„åŠ›
        attn_output = torch.zeros_like(Q)
        
        for i in range(0, seq_len, chunk_size):
            end_i = min(i + chunk_size, seq_len)
            
            # è®¡ç®—å½“å‰å—çš„æ³¨æ„åŠ›
            Q_chunk = Q[:, i:end_i]
            scores = torch.matmul(Q_chunk, K.transpose(-2, -1)) / np.sqrt(d_model)
            attn_weights = F.softmax(scores, dim=-1)
            attn_output[:, i:end_i] = torch.matmul(attn_weights, V)
        
        return attn_output
```

## 8. è°ƒè¯•å’Œç›‘æ§ ğŸ“Š

```python
class DebuggingMonitoring:
    """è°ƒè¯•å’Œç›‘æ§æŠ€å·§"""
    
    @staticmethod
    def hook_example(model):
        """ä½¿ç”¨é’©å­è°ƒè¯•"""
        activations = {}
        gradients = {}
        
        def forward_hook(module, input, output):
            activations[module.__class__.__name__] = output.detach()
        
        def backward_hook(module, grad_input, grad_output):
            gradients[module.__class__.__name__] = grad_output[0].detach()
        
        # æ³¨å†Œé’©å­
        for name, module in model.named_modules():
            module.register_forward_hook(forward_hook)
            module.register_backward_hook(backward_hook)
        
        return activations, gradients
    
    @staticmethod
    def gradient_flow_check(model):
        """æ£€æŸ¥æ¢¯åº¦æµ"""
        # ä¸€æ¬¡å‰å‘å’Œåå‘ä¼ æ’­
        x = torch.randn(1, 3, 224, 224)
        y = model(x)
        loss = y.mean()
        loss.backward()
        
        # åˆ†ææ¢¯åº¦
        ave_grads = []
        max_grads = []
        layers = []
        
        for n, p in model.named_parameters():
            if p.requires_grad and p.grad is not None:
                layers.append(n)
                ave_grads.append(p.grad.abs().mean().item())
                max_grads.append(p.grad.abs().max().item())
        
        # å¯è§†åŒ–
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.bar(np.arange(len(ave_grads)), ave_grads, alpha=0.7)
        plt.hlines(0, 0, len(ave_grads)+1, lw=2, color="k")
        plt.xticks(range(0, len(ave_grads), 1), layers, rotation="vertical")
        plt.xlim(left=0, right=len(ave_grads))
        plt.ylim(bottom=-0.001)
        plt.xlabel("Layers")
        plt.ylabel("Average gradient")
        plt.title("Gradient flow")
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 2, 2)
        plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.7)
        plt.hlines(0, 0, len(max_grads)+1, lw=2, color="k")
        plt.xticks(range(0, len(max_grads), 1), layers, rotation="vertical")
        plt.xlim(left=0, right=len(max_grads))
        plt.ylim(bottom=-0.001)
        plt.xlabel("Layers")
        plt.ylabel("Max gradient")
        plt.title("Gradient flow")
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def profile_model(model, input_shape=(1, 3, 224, 224)):
        """æ€§èƒ½åˆ†æ"""
        from torch.profiler import profile, record_function, ProfilerActivity
        
        inputs = torch.randn(input_shape)
        
        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                    record_shapes=True,
                    profile_memory=True,
                    with_stack=True) as prof:
            with record_function("model_inference"):
                model(inputs)
        
        # æ‰“å°ç»“æœ
        print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
        
        # ç”ŸæˆChromeè·Ÿè¸ªæ–‡ä»¶
        prof.export_chrome_trace("trace.json")
        
        return prof
```

## 9. é«˜çº§è®­ç»ƒæŠ€å·§ ğŸ“

```python
class AdvancedTrainingTricks:
    """é«˜çº§è®­ç»ƒæŠ€å·§"""
    
    @staticmethod
    def sam_optimizer(model, base_optimizer, rho=0.05):
        """Sharpness Aware Minimization (SAM)"""
        class SAM:
            def __init__(self, params, base_optimizer, rho=0.05):
                self.base_optimizer = base_optimizer
                self.rho = rho
                self.param_groups = self.base_optimizer.param_groups
            
            def first_step(self, zero_grad=False):
                grad_norm = self._grad_norm()
                for group in self.param_groups:
                    scale = self.rho / (grad_norm + 1e-12)
                    
                    for p in group["params"]:
                        if p.grad is None:
                            continue
                        e_w = p.grad * scale
                        p.add_(e_w)  # çˆ¬åˆ°æŸå¤±æ›´é«˜çš„åœ°æ–¹
                        self.state[p]["e_w"] = e_w
                
                if zero_grad:
                    self.zero_grad()
            
            def second_step(self, zero_grad=False):
                for group in self.param_groups:
                    for p in group["params"]:
                        if p.grad is None:
                            continue
                        p.sub_(self.state[p]["e_w"])  # æ¢å¤å‚æ•°
                
                self.base_optimizer.step()
                
                if zero_grad:
                    self.zero_grad()
            
            def _grad_norm(self):
                norm = torch.norm(
                    torch.stack([
                        p.grad.norm(p=2)
                        for group in self.param_groups
                        for p in group["params"]
                        if p.grad is not None
                    ]),
                    p=2
                )
                return norm
        
        return SAM(model.parameters(), base_optimizer, rho)
    
    @staticmethod
    def lookahead_optimizer(base_optimizer, k=5, alpha=0.5):
        """Lookaheadä¼˜åŒ–å™¨"""
        class Lookahead:
            def __init__(self, optimizer, k=5, alpha=0.5):
                self.optimizer = optimizer
                self.k = k
                self.alpha = alpha
                self.step_count = 0
                
                # ä¿å­˜æ…¢æƒé‡
                self.slow_weights = [[p.clone().detach() 
                                     for p in group['params']]
                                    for group in optimizer.param_groups]
            
            def step(self, closure=None):
                loss = self.optimizer.step(closure)
                self.step_count += 1
                
                if self.step_count % self.k == 0:
                    # æ›´æ–°æ…¢æƒé‡
                    for group_idx, group in enumerate(self.optimizer.param_groups):
                        for p_idx, p in enumerate(group['params']):
                            slow = self.slow_weights[group_idx][p_idx]
                            slow.add_(self.alpha * (p.data - slow))
                            p.data.copy_(slow)
                
                return loss
        
        return Lookahead(base_optimizer, k, alpha)
    
    @staticmethod
    def progressive_resizing(initial_size=64, final_size=224, stages=4):
        """æ¸è¿›å¼è°ƒæ•´å›¾åƒå¤§å°"""
        sizes = np.linspace(initial_size, final_size, stages).astype(int)
        
        transforms_list = []
        for size in sizes:
            transform = transforms.Compose([
                transforms.Resize(size),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            transforms_list.append(transform)
        
        return transforms_list
```

## 10. å®æˆ˜æ¡ˆä¾‹ ğŸ’¼

```python
class TrainingPipeline:
    """å®Œæ•´çš„è®­ç»ƒç®¡é“"""
    
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.model.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # è°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # æ··åˆç²¾åº¦
        self.scaler = GradScaler()
        
        # EMA
        self.ema = None
        if config.get('use_ema', False):
            self.ema = self._create_ema()
        
        # æœ€ä½³æ¨¡å‹
        self.best_acc = 0
        self.best_model_state = None
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.config['optimizer'] == 'adam':
            return optim.Adam(
                self.model.parameters(),
                lr=self.config['lr'],
                weight_decay=self.config.get('weight_decay', 0)
            )
        elif self.config['optimizer'] == 'sgd':
            return optim.SGD(
                self.model.parameters(),
                lr=self.config['lr'],
                momentum=0.9,
                weight_decay=self.config.get('weight_decay', 0)
            )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.config['scheduler'] == 'cosine':
            return CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['epochs']
            )
        elif self.config['scheduler'] == 'onecycle':
            return OneCycleLR(
                self.optimizer,
                max_lr=self.config['lr'],
                epochs=self.config['epochs'],
                steps_per_epoch=len(self.train_loader)
            )
    
    def _create_ema(self):
        """åˆ›å»ºEMAæ¨¡å‹"""
        ema_model = copy.deepcopy(self.model)
        ema_model.eval()
        return ema_model
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            # Mixup/CutMix
            if self.config.get('mixup', False):
                inputs, targets_a, targets_b, lam = DataAugmentation.mixup(
                    inputs, targets, alpha=1.0
                )
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with autocast():
                outputs = self.model(inputs)
                
                if self.config.get('mixup', False):
                    loss = lam * self.criterion(outputs, targets_a) + \
                           (1 - lam) * self.criterion(outputs, targets_b)
                else:
                    loss = self.criterion(outputs, targets)
            
            # æ¢¯åº¦ç¼©æ”¾
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.get('gradient_clip', 0) > 0:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['gradient_clip']
                )
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # æ›´æ–°EMA
            if self.ema is not None:
                self._update_ema()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            if not self.config.get('mixup', False):
                correct += predicted.eq(targets).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': total_loss / (batch_idx + 1),
                'acc': 100. * correct / total if not self.config.get('mixup', False) else 0
            })
        
        return total_loss / len(self.train_loader)
    
    def validate(self):
        """éªŒè¯"""
        model = self.ema if self.ema is not None else self.model
        model.eval()
        
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in tqdm(self.val_loader, desc='Validating'):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                outputs = model(inputs)
                loss = self.criterion(outputs, targets)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        acc = 100. * correct / total
        avg_loss = total_loss / len(self.val_loader)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if acc > self.best_acc:
            self.best_acc = acc
            self.best_model_state = copy.deepcopy(model.state_dict())
            print(f'Best model saved with accuracy: {acc:.2f}%')
        
        return avg_loss, acc
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"Training on {self.device}")
        
        for epoch in range(self.config['epochs']):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                self.scheduler.step()
            
            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, '
                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        print(f'Best validation accuracy: {self.best_acc:.2f}%')
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
        
        return self.model
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“‹

```python
def training_best_practices():
    """è®­ç»ƒæœ€ä½³å®è·µ"""
    
    practices = {
        "åˆå§‹åŒ–": [
            "ä½¿ç”¨åˆé€‚çš„æƒé‡åˆå§‹åŒ–ï¼ˆHe, Xavierï¼‰",
            "BatchNormåçš„å±‚åˆå§‹åŒ–ä¸º0",
            "æ®‹å·®è¿æ¥çš„æœ€åä¸€å±‚åˆå§‹åŒ–ä¸º0",
            "æ£€æŸ¥åˆå§‹æŸå¤±æ˜¯å¦åˆç†"
        ],
        
        "å­¦ä¹ ç‡": [
            "ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨æ‰¾åˆ°æœ€ä½³èŒƒå›´",
            "ä½¿ç”¨warmupé¿å…åˆæœŸä¸ç¨³å®š",
            "ä½™å¼¦é€€ç«æˆ–OneCycleè°ƒåº¦",
            "ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡"
        ],
        
        "æ­£åˆ™åŒ–": [
            "Dropouté€šå¸¸0.1-0.5",
            "æƒé‡è¡°å‡é€šå¸¸1e-4åˆ°1e-2",
            "æ ‡ç­¾å¹³æ»‘0.1æ•ˆæœå¥½",
            "æ•°æ®å¢å¼ºæ˜¯æœ€å¥½çš„æ­£åˆ™åŒ–"
        ],
        
        "ä¼˜åŒ–æŠ€å·§": [
            "æ··åˆç²¾åº¦è®­ç»ƒåŠ é€Ÿ50%+",
            "æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§batch",
            "æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸",
            "EMAæå‡æµ‹è¯•æ€§èƒ½"
        ],
        
        "è°ƒè¯•": [
            "å…ˆåœ¨å°æ•°æ®é›†è¿‡æ‹Ÿåˆ",
            "å¯è§†åŒ–æ¢¯åº¦æµ",
            "ç›‘æ§æ¿€æ´»å€¼åˆ†å¸ƒ",
            "ä½¿ç”¨TensorBoardè®°å½•"
        ],
        
        "æ•ˆç‡": [
            "num_workers=4*num_gpu",
            "pin_memory=TrueåŠ é€Ÿ",
            "æŒä¹…åŒ–workerså‡å°‘å¼€é”€",
            "ä½¿ç”¨torch.compileåŠ é€Ÿ"
        ]
    }
    
    return practices

# å¸¸è§é—®é¢˜è§£å†³
troubleshooting = """
é—®é¢˜1: æŸå¤±ä¸ä¸‹é™
- æ£€æŸ¥å­¦ä¹ ç‡ï¼ˆå¤ªå¤§æˆ–å¤ªå°ï¼‰
- æ£€æŸ¥æ•°æ®å’Œæ ‡ç­¾
- æ£€æŸ¥æŸå¤±å‡½æ•°
- å°è¯•æ›´ç®€å•çš„æ¨¡å‹

é—®é¢˜2: æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
- ä½¿ç”¨æ¢¯åº¦è£å‰ª
- æ£€æŸ¥åˆå§‹åŒ–
- ä½¿ç”¨BatchNorm
- å‡å°‘ç½‘ç»œæ·±åº¦

é—®é¢˜3: è¿‡æ‹Ÿåˆä¸¥é‡
- å¢åŠ æ•°æ®å¢å¼º
- å¢åŠ Dropout
- å‡å°‘æ¨¡å‹å®¹é‡
- ä½¿ç”¨æ—©åœ

é—®é¢˜4: è®­ç»ƒä¸ç¨³å®š
- å‡å°å­¦ä¹ ç‡
- ä½¿ç”¨æ¢¯åº¦è£å‰ª
- å¢åŠ æ‰¹æ¬¡å¤§å°
- ä½¿ç”¨æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨ï¼ˆå¦‚AdamWï¼‰

é—®é¢˜5: GPUå†…å­˜ä¸è¶³
- å‡å°æ‰¹æ¬¡å¤§å°
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- ä½¿ç”¨æ··åˆç²¾åº¦
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
"""

print("è®­ç»ƒæŠ€å·§æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [æ¨¡å‹å¾®è°ƒ](finetuning.md) - é«˜æ•ˆå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
- [NLPæ¨¡å‹](nlp_models.md) - è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯
- [æ¨¡å‹éƒ¨ç½²](deployment.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²