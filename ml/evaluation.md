# æ¨¡å‹è¯„ä¼°å®Œå…¨æŒ‡å— ğŸ“Š

æ·±å…¥ç†è§£å¦‚ä½•æ­£ç¡®è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚

## 1. è¯„ä¼°åŸºç¡€ ğŸ¯

```python
import numpy as np
import pandas as pd
from sklearn.metrics import (
    # åˆ†ç±»æŒ‡æ ‡
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, classification_report,
    # å›å½’æŒ‡æ ‡
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error, explained_variance_score,
    # èšç±»æŒ‡æ ‡
    silhouette_score, davies_bouldin_score, calinski_harabasz_score
)
from sklearn.model_selection import (
    train_test_split, cross_val_score, cross_validate,
    KFold, StratifiedKFold, TimeSeriesSplit,
    GridSearchCV, RandomizedSearchCV
)
import matplotlib.pyplot as plt
import seaborn as sns

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, task_type='classification'):
        """
        task_type: 'classification', 'regression', 'clustering'
        """
        self.task_type = task_type
        self.results = {}
        
    def evaluate_classification(self, y_true, y_pred, y_proba=None):
        """åˆ†ç±»æ¨¡å‹è¯„ä¼°"""
        metrics = {}
        
        # åŸºç¡€æŒ‡æ ‡
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision'] = precision_score(y_true, y_pred, average='weighted')
        metrics['recall'] = recall_score(y_true, y_pred, average='weighted')
        metrics['f1'] = f1_score(y_true, y_pred, average='weighted')
        
        # å¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹
        if y_proba is not None:
            if len(np.unique(y_true)) == 2:  # äºŒåˆ†ç±»
                metrics['roc_auc'] = roc_auc_score(y_true, y_proba[:, 1])
            else:  # å¤šåˆ†ç±»
                metrics['roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr')
        
        # æ··æ·†çŸ©é˜µ
        metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)
        
        # åˆ†ç±»æŠ¥å‘Š
        metrics['classification_report'] = classification_report(y_true, y_pred)
        
        self.results = metrics
        return metrics
    
    def evaluate_regression(self, y_true, y_pred):
        """å›å½’æ¨¡å‹è¯„ä¼°"""
        metrics = {}
        
        # åŸºç¡€æŒ‡æ ‡
        metrics['mse'] = mean_squared_error(y_true, y_pred)
        metrics['rmse'] = np.sqrt(metrics['mse'])
        metrics['mae'] = mean_absolute_error(y_true, y_pred)
        metrics['r2'] = r2_score(y_true, y_pred)
        
        # é«˜çº§æŒ‡æ ‡
        metrics['mape'] = mean_absolute_percentage_error(y_true, y_pred)
        metrics['explained_variance'] = explained_variance_score(y_true, y_pred)
        
        # æ®‹å·®åˆ†æ
        residuals = y_true - y_pred
        metrics['residual_mean'] = np.mean(residuals)
        metrics['residual_std'] = np.std(residuals)
        metrics['residual_skew'] = self._calculate_skewness(residuals)
        
        self.results = metrics
        return metrics
    
    def evaluate_clustering(self, X, labels):
        """èšç±»æ¨¡å‹è¯„ä¼°"""
        metrics = {}
        
        # å†…éƒ¨æŒ‡æ ‡
        metrics['silhouette'] = silhouette_score(X, labels)
        metrics['davies_bouldin'] = davies_bouldin_score(X, labels)
        metrics['calinski_harabasz'] = calinski_harabasz_score(X, labels)
        
        # èšç±»ç»Ÿè®¡
        unique_labels = np.unique(labels)
        metrics['n_clusters'] = len(unique_labels)
        metrics['cluster_sizes'] = {
            f'cluster_{i}': np.sum(labels == i) 
            for i in unique_labels
        }
        
        self.results = metrics
        return metrics
    
    def _calculate_skewness(self, data):
        """è®¡ç®—ååº¦"""
        from scipy.stats import skew
        return skew(data)
    
    def plot_results(self):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
        if self.task_type == 'classification':
            self._plot_classification_results()
        elif self.task_type == 'regression':
            self._plot_regression_results()
        elif self.task_type == 'clustering':
            self._plot_clustering_results()
    
    def _plot_classification_results(self):
        """åˆ†ç±»ç»“æœå¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # æ··æ·†çŸ©é˜µ
        if 'confusion_matrix' in self.results:
            sns.heatmap(self.results['confusion_matrix'], 
                       annot=True, fmt='d', cmap='Blues',
                       ax=axes[0, 0])
            axes[0, 0].set_title('Confusion Matrix')
            axes[0, 0].set_xlabel('Predicted')
            axes[0, 0].set_ylabel('Actual')
        
        # æŒ‡æ ‡æ¡å½¢å›¾
        if 'accuracy' in self.results:
            metrics = ['accuracy', 'precision', 'recall', 'f1']
            values = [self.results.get(m, 0) for m in metrics]
            axes[0, 1].bar(metrics, values)
            axes[0, 1].set_title('Classification Metrics')
            axes[0, 1].set_ylim(0, 1)
        
        plt.tight_layout()
        plt.show()
```

## 2. äº¤å‰éªŒè¯ç­–ç•¥ ğŸ”„

```python
class CrossValidationStrategy:
    """äº¤å‰éªŒè¯ç­–ç•¥"""
    
    def __init__(self, cv_type='kfold', n_splits=5, random_state=42):
        self.cv_type = cv_type
        self.n_splits = n_splits
        self.random_state = random_state
        self.cv = self._get_cv_splitter()
    
    def _get_cv_splitter(self):
        """è·å–äº¤å‰éªŒè¯åˆ†å‰²å™¨"""
        if self.cv_type == 'kfold':
            return KFold(n_splits=self.n_splits, 
                        shuffle=True, 
                        random_state=self.random_state)
        
        elif self.cv_type == 'stratified':
            return StratifiedKFold(n_splits=self.n_splits,
                                  shuffle=True,
                                  random_state=self.random_state)
        
        elif self.cv_type == 'timeseries':
            return TimeSeriesSplit(n_splits=self.n_splits)
        
        elif self.cv_type == 'leave_one_out':
            from sklearn.model_selection import LeaveOneOut
            return LeaveOneOut()
        
        elif self.cv_type == 'group':
            from sklearn.model_selection import GroupKFold
            return GroupKFold(n_splits=self.n_splits)
    
    def evaluate_model(self, model, X, y, scoring='accuracy'):
        """ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹"""
        scores = cross_val_score(model, X, y, cv=self.cv, scoring=scoring)
        
        results = {
            'scores': scores,
            'mean': np.mean(scores),
            'std': np.std(scores),
            'min': np.min(scores),
            'max': np.max(scores),
            '95_ci': (np.mean(scores) - 1.96 * np.std(scores),
                     np.mean(scores) + 1.96 * np.std(scores))
        }
        
        return results
    
    def evaluate_multiple_metrics(self, model, X, y, metrics_dict):
        """è¯„ä¼°å¤šä¸ªæŒ‡æ ‡"""
        results = cross_validate(model, X, y, cv=self.cv, 
                               scoring=metrics_dict,
                               return_train_score=True)
        
        # æ•´ç†ç»“æœ
        summary = {}
        for key in results:
            if key.startswith('test_') or key.startswith('train_'):
                summary[key] = {
                    'mean': np.mean(results[key]),
                    'std': np.std(results[key])
                }
        
        return summary
    
    def nested_cross_validation(self, model, param_grid, X, y, 
                              scoring='accuracy'):
        """åµŒå¥—äº¤å‰éªŒè¯"""
        outer_cv = self.cv
        inner_cv = KFold(n_splits=3, shuffle=True, 
                        random_state=self.random_state)
        
        # å¤–å±‚äº¤å‰éªŒè¯
        nested_scores = []
        
        for train_idx, test_idx in outer_cv.split(X, y):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # å†…å±‚äº¤å‰éªŒè¯ç”¨äºè¶…å‚æ•°è°ƒä¼˜
            grid_search = GridSearchCV(model, param_grid, cv=inner_cv,
                                      scoring=scoring)
            grid_search.fit(X_train, y_train)
            
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            score = grid_search.score(X_test, y_test)
            nested_scores.append(score)
        
        return {
            'nested_scores': nested_scores,
            'mean': np.mean(nested_scores),
            'std': np.std(nested_scores)
        }

# è‡ªå®šä¹‰äº¤å‰éªŒè¯
class CustomCrossValidation:
    """è‡ªå®šä¹‰äº¤å‰éªŒè¯ç­–ç•¥"""
    
    @staticmethod
    def time_based_split(df, date_column, n_splits=5):
        """åŸºäºæ—¶é—´çš„åˆ†å‰²"""
        df = df.sort_values(date_column)
        split_size = len(df) // n_splits
        
        for i in range(n_splits):
            train_end = (i + 1) * split_size
            test_start = train_end
            test_end = min(test_start + split_size, len(df))
            
            train_idx = df.index[:train_end]
            test_idx = df.index[test_start:test_end]
            
            yield train_idx, test_idx
    
    @staticmethod
    def blocked_time_series_split(df, block_size=30):
        """å—æ—¶é—´åºåˆ—åˆ†å‰²"""
        n_samples = len(df)
        n_blocks = n_samples // block_size
        
        for i in range(2, n_blocks):
            train_blocks = i
            test_blocks = 1
            
            train_end = train_blocks * block_size
            test_start = train_end
            test_end = test_start + test_blocks * block_size
            
            if test_end > n_samples:
                break
            
            yield range(train_end), range(test_start, test_end)
    
    @staticmethod
    def stratified_group_split(X, y, groups, n_splits=5):
        """åˆ†å±‚åˆ†ç»„åˆ†å‰²"""
        from sklearn.model_selection import StratifiedGroupKFold
        
        sgkf = StratifiedGroupKFold(n_splits=n_splits)
        return sgkf.split(X, y, groups)
```

## 3. é«˜çº§è¯„ä¼°æŒ‡æ ‡ ğŸ“ˆ

```python
class AdvancedMetrics:
    """é«˜çº§è¯„ä¼°æŒ‡æ ‡"""
    
    @staticmethod
    def cohen_kappa_score(y_true, y_pred):
        """Cohen's Kappaç³»æ•°"""
        from sklearn.metrics import cohen_kappa_score
        return cohen_kappa_score(y_true, y_pred)
    
    @staticmethod
    def matthews_corrcoef(y_true, y_pred):
        """Matthewsç›¸å…³ç³»æ•°"""
        from sklearn.metrics import matthews_corrcoef
        return matthews_corrcoef(y_true, y_pred)
    
    @staticmethod
    def log_loss(y_true, y_proba):
        """å¯¹æ•°æŸå¤±"""
        from sklearn.metrics import log_loss
        return log_loss(y_true, y_proba)
    
    @staticmethod
    def brier_score(y_true, y_proba):
        """Brieråˆ†æ•°"""
        from sklearn.metrics import brier_score_loss
        return brier_score_loss(y_true, y_proba)
    
    @staticmethod
    def average_precision_score(y_true, y_score):
        """å¹³å‡ç²¾åº¦åˆ†æ•°"""
        from sklearn.metrics import average_precision_score
        return average_precision_score(y_true, y_score)
    
    @staticmethod
    def balanced_accuracy(y_true, y_pred):
        """å¹³è¡¡å‡†ç¡®ç‡"""
        from sklearn.metrics import balanced_accuracy_score
        return balanced_accuracy_score(y_true, y_pred)
    
    @staticmethod
    def hamming_loss(y_true, y_pred):
        """æ±‰æ˜æŸå¤±ï¼ˆå¤šæ ‡ç­¾åˆ†ç±»ï¼‰"""
        from sklearn.metrics import hamming_loss
        return hamming_loss(y_true, y_pred)
    
    @staticmethod
    def jaccard_score(y_true, y_pred):
        """Jaccardç›¸ä¼¼ç³»æ•°"""
        from sklearn.metrics import jaccard_score
        return jaccard_score(y_true, y_pred, average='weighted')

class RegressionMetrics:
    """å›å½’è¯„ä¼°æŒ‡æ ‡"""
    
    @staticmethod
    def mean_squared_log_error(y_true, y_pred):
        """å‡æ–¹å¯¹æ•°è¯¯å·®"""
        from sklearn.metrics import mean_squared_log_error
        return mean_squared_log_error(y_true, y_pred)
    
    @staticmethod
    def median_absolute_error(y_true, y_pred):
        """ä¸­ä½æ•°ç»å¯¹è¯¯å·®"""
        from sklearn.metrics import median_absolute_error
        return median_absolute_error(y_true, y_pred)
    
    @staticmethod
    def max_error(y_true, y_pred):
        """æœ€å¤§è¯¯å·®"""
        from sklearn.metrics import max_error
        return max_error(y_true, y_pred)
    
    @staticmethod
    def mean_tweedie_deviance(y_true, y_pred, power=0):
        """Tweedieåå·®"""
        from sklearn.metrics import mean_tweedie_deviance
        return mean_tweedie_deviance(y_true, y_pred, power=power)
    
    @staticmethod
    def mean_pinball_loss(y_true, y_pred, alpha=0.5):
        """PinballæŸå¤±ï¼ˆåˆ†ä½æ•°æŸå¤±ï¼‰"""
        from sklearn.metrics import mean_pinball_loss
        return mean_pinball_loss(y_true, y_pred, alpha=alpha)
    
    @staticmethod
    def adjusted_r2_score(y_true, y_pred, n_features):
        """è°ƒæ•´RÂ²åˆ†æ•°"""
        n = len(y_true)
        r2 = r2_score(y_true, y_pred)
        adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)
        return adjusted_r2

class RankingMetrics:
    """æ’åºè¯„ä¼°æŒ‡æ ‡"""
    
    @staticmethod
    def ndcg_score(y_true, y_score, k=None):
        """NDCGåˆ†æ•°"""
        from sklearn.metrics import ndcg_score
        return ndcg_score(y_true, y_score, k=k)
    
    @staticmethod
    def dcg_score(y_true, y_score, k=None):
        """DCGåˆ†æ•°"""
        from sklearn.metrics import dcg_score
        return dcg_score(y_true, y_score, k=k)
    
    @staticmethod
    def mean_reciprocal_rank(y_true, y_score):
        """å¹³å‡å€’æ•°æ’å"""
        n_samples = len(y_true)
        mrr = 0
        
        for i in range(n_samples):
            # è·å–æ’åºç´¢å¼•
            sorted_indices = np.argsort(y_score[i])[::-1]
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„ä½ç½®
            for rank, idx in enumerate(sorted_indices, 1):
                if y_true[i][idx] == 1:
                    mrr += 1.0 / rank
                    break
        
        return mrr / n_samples
```

## 4. å¯è§†åŒ–è¯„ä¼° ğŸ“Š

```python
class EvaluationVisualizer:
    """è¯„ä¼°ç»“æœå¯è§†åŒ–"""
    
    def __init__(self, figsize=(12, 8)):
        self.figsize = figsize
    
    def plot_roc_curve(self, y_true, y_proba, title='ROC Curve'):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        fpr, tpr, thresholds = roc_curve(y_true, y_proba)
        auc = roc_auc_score(y_true, y_proba)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(title)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return {'fpr': fpr, 'tpr': tpr, 'auc': auc}
    
    def plot_precision_recall_curve(self, y_true, y_proba):
        """ç»˜åˆ¶ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿"""
        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return {'precision': precision, 'recall': recall}
    
    def plot_confusion_matrix(self, y_true, y_pred, labels=None):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=labels, yticklabels=labels)
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
        plt.show()
        
        return cm
    
    def plot_learning_curve(self, model, X, y, cv=5):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        from sklearn.model_selection import learning_curve
        
        train_sizes, train_scores, val_scores = learning_curve(
            model, X, y, cv=cv, n_jobs=-1,
            train_sizes=np.linspace(0.1, 1.0, 10)
        )
        
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, np.mean(train_scores, axis=1), 
                'o-', label='Training Score')
        plt.plot(train_sizes, np.mean(val_scores, axis=1), 
                'o-', label='Validation Score')
        plt.fill_between(train_sizes, 
                        np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),
                        np.mean(train_scores, axis=1) + np.std(train_scores, axis=1),
                        alpha=0.1)
        plt.fill_between(train_sizes,
                        np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),
                        np.mean(val_scores, axis=1) + np.std(val_scores, axis=1),
                        alpha=0.1)
        plt.xlabel('Training Set Size')
        plt.ylabel('Score')
        plt.title('Learning Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def plot_validation_curve(self, model, X, y, param_name, 
                            param_range, cv=5):
        """ç»˜åˆ¶éªŒè¯æ›²çº¿"""
        from sklearn.model_selection import validation_curve
        
        train_scores, val_scores = validation_curve(
            model, X, y, param_name=param_name,
            param_range=param_range, cv=cv
        )
        
        plt.figure(figsize=(10, 6))
        plt.plot(param_range, np.mean(train_scores, axis=1), 
                'o-', label='Training Score')
        plt.plot(param_range, np.mean(val_scores, axis=1), 
                'o-', label='Validation Score')
        plt.fill_between(param_range,
                        np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),
                        np.mean(train_scores, axis=1) + np.std(train_scores, axis=1),
                        alpha=0.1)
        plt.fill_between(param_range,
                        np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),
                        np.mean(val_scores, axis=1) + np.std(val_scores, axis=1),
                        alpha=0.1)
        plt.xlabel(param_name)
        plt.ylabel('Score')
        plt.title('Validation Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def plot_residuals(self, y_true, y_pred):
        """ç»˜åˆ¶æ®‹å·®å›¾"""
        residuals = y_true - y_pred
        
        fig, axes = plt.subplots(2, 2, figsize=self.figsize)
        
        # æ®‹å·® vs é¢„æµ‹å€¼
        axes[0, 0].scatter(y_pred, residuals, alpha=0.5)
        axes[0, 0].axhline(y=0, color='r', linestyle='--')
        axes[0, 0].set_xlabel('Predicted Values')
        axes[0, 0].set_ylabel('Residuals')
        axes[0, 0].set_title('Residuals vs Predicted')
        
        # QQå›¾
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('Q-Q Plot')
        
        # æ®‹å·®ç›´æ–¹å›¾
        axes[1, 0].hist(residuals, bins=30, edgecolor='black')
        axes[1, 0].set_xlabel('Residuals')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Residual Distribution')
        
        # å®é™…å€¼ vs é¢„æµ‹å€¼
        axes[1, 1].scatter(y_true, y_pred, alpha=0.5)
        axes[1, 1].plot([y_true.min(), y_true.max()], 
                       [y_true.min(), y_true.max()], 
                       'r--', lw=2)
        axes[1, 1].set_xlabel('Actual Values')
        axes[1, 1].set_ylabel('Predicted Values')
        axes[1, 1].set_title('Actual vs Predicted')
        
        plt.tight_layout()
        plt.show()
```

## 5. æ¨¡å‹æ¯”è¾ƒ ğŸ”

```python
class ModelComparison:
    """æ¨¡å‹æ¯”è¾ƒå·¥å…·"""
    
    def __init__(self):
        self.results = {}
    
    def compare_models(self, models, X, y, cv=5, scoring='accuracy'):
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹"""
        results = []
        
        for name, model in models.items():
            scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
            results.append({
                'Model': name,
                'Mean Score': np.mean(scores),
                'Std Score': np.std(scores),
                'Min Score': np.min(scores),
                'Max Score': np.max(scores)
            })
        
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('Mean Score', ascending=False)
        
        self.results = results_df
        return results_df
    
    def statistical_comparison(self, model1_scores, model2_scores):
        """ç»Ÿè®¡æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹"""
        from scipy import stats
        
        # é…å¯¹tæ£€éªŒ
        t_stat, p_value = stats.ttest_rel(model1_scores, model2_scores)
        
        # Wilcoxonç¬¦å·ç§©æ£€éªŒ
        w_stat, w_p_value = stats.wilcoxon(model1_scores, model2_scores)
        
        # è®¡ç®—æ•ˆåº”é‡
        effect_size = np.mean(model1_scores - model2_scores) / np.std(model1_scores - model2_scores)
        
        return {
            't_statistic': t_stat,
            't_p_value': p_value,
            'wilcoxon_statistic': w_stat,
            'wilcoxon_p_value': w_p_value,
            'effect_size': effect_size,
            'significant': p_value < 0.05
        }
    
    def plot_model_comparison(self):
        """å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ"""
        if self.results.empty:
            print("No results to plot")
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # æ¡å½¢å›¾
        axes[0].bar(self.results['Model'], self.results['Mean Score'])
        axes[0].errorbar(range(len(self.results)), 
                        self.results['Mean Score'],
                        yerr=self.results['Std Score'],
                        fmt='none', color='black', capsize=5)
        axes[0].set_xlabel('Model')
        axes[0].set_ylabel('Score')
        axes[0].set_title('Model Performance Comparison')
        axes[0].tick_params(axis='x', rotation=45)
        
        # ç®±çº¿å›¾ï¼ˆå¦‚æœæœ‰è¯¦ç»†åˆ†æ•°ï¼‰
        if hasattr(self, 'detailed_scores'):
            axes[1].boxplot(self.detailed_scores.values(), 
                          labels=self.detailed_scores.keys())
            axes[1].set_xlabel('Model')
            axes[1].set_ylabel('Score')
            axes[1].set_title('Score Distribution')
            axes[1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
    
    def mcnemar_test(self, y_true, model1_pred, model2_pred):
        """McNemaræ£€éªŒï¼ˆç”¨äºåˆ†ç±»æ¨¡å‹ï¼‰"""
        from statsmodels.stats.contingency_tables import mcnemar
        
        # æ„å»ºåˆ—è”è¡¨
        correct1_correct2 = np.sum((model1_pred == y_true) & (model2_pred == y_true))
        correct1_wrong2 = np.sum((model1_pred == y_true) & (model2_pred != y_true))
        wrong1_correct2 = np.sum((model1_pred != y_true) & (model2_pred == y_true))
        wrong1_wrong2 = np.sum((model1_pred != y_true) & (model2_pred != y_true))
        
        table = [[correct1_correct2, correct1_wrong2],
                [wrong1_correct2, wrong1_wrong2]]
        
        result = mcnemar(table, exact=True)
        
        return {
            'statistic': result.statistic,
            'p_value': result.pvalue,
            'significant': result.pvalue < 0.05
        }
```

## 6. åå·®æ–¹å·®åˆ†æ âš–ï¸

```python
class BiasVarianceAnalysis:
    """åå·®æ–¹å·®åˆ†æ"""
    
    def __init__(self, model, n_iterations=100):
        self.model = model
        self.n_iterations = n_iterations
    
    def decompose(self, X, y, test_size=0.3):
        """åå·®æ–¹å·®åˆ†è§£"""
        from sklearn.model_selection import train_test_split
        from sklearn.base import clone
        
        predictions = []
        
        for i in range(self.n_iterations):
            # éšæœºåˆ†å‰²æ•°æ®
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=i
            )
            
            # å…‹éš†å¹¶è®­ç»ƒæ¨¡å‹
            model_clone = clone(self.model)
            model_clone.fit(X_train, y_train)
            
            # é¢„æµ‹
            y_pred = model_clone.predict(X_test)
            predictions.append(y_pred)
        
        # è½¬æ¢ä¸ºæ•°ç»„
        predictions = np.array(predictions)
        
        # è®¡ç®—åå·®å’Œæ–¹å·®
        # åå·®ï¼šå¹³å‡é¢„æµ‹ä¸çœŸå®å€¼çš„å·®å¼‚
        avg_predicted = np.mean(predictions, axis=0)
        bias = np.mean((avg_predicted - y_test) ** 2)
        
        # æ–¹å·®ï¼šé¢„æµ‹çš„å˜å¼‚æ€§
        variance = np.mean(np.var(predictions, axis=0))
        
        # æ€»è¯¯å·®
        mse = np.mean([mean_squared_error(y_test, pred) 
                      for pred in predictions])
        
        return {
            'bias': bias,
            'variance': variance,
            'total_error': mse,
            'irreducible_error': mse - bias - variance
        }
    
    def plot_bias_variance_tradeoff(self, X, y, complexity_param, 
                                   param_range):
        """ç»˜åˆ¶åå·®æ–¹å·®æƒè¡¡"""
        bias_scores = []
        variance_scores = []
        
        for param in param_range:
            # è®¾ç½®æ¨¡å‹å¤æ‚åº¦å‚æ•°
            setattr(self.model, complexity_param, param)
            
            # è®¡ç®—åå·®å’Œæ–¹å·®
            results = self.decompose(X, y)
            bias_scores.append(results['bias'])
            variance_scores.append(results['variance'])
        
        plt.figure(figsize=(10, 6))
        plt.plot(param_range, bias_scores, 'o-', label='BiasÂ²')
        plt.plot(param_range, variance_scores, 'o-', label='Variance')
        plt.plot(param_range, np.array(bias_scores) + np.array(variance_scores),
                'o-', label='Total Error')
        plt.xlabel(complexity_param)
        plt.ylabel('Error')
        plt.title('Bias-Variance Tradeoff')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
```

## 7. ä¸šåŠ¡æŒ‡æ ‡è¯„ä¼° ğŸ’¼

```python
class BusinessMetrics:
    """ä¸šåŠ¡æŒ‡æ ‡è¯„ä¼°"""
    
    @staticmethod
    def calculate_lift(y_true, y_proba, percentile=10):
        """è®¡ç®—æå‡åº¦"""
        # æŒ‰æ¦‚ç‡æ’åº
        sorted_indices = np.argsort(y_proba)[::-1]
        n_top = int(len(y_true) * percentile / 100)
        
        # é¡¶éƒ¨percentileçš„æ­£ä¾‹ç‡
        top_positive_rate = np.mean(y_true[sorted_indices[:n_top]])
        
        # æ•´ä½“æ­£ä¾‹ç‡
        overall_positive_rate = np.mean(y_true)
        
        # æå‡åº¦
        lift = top_positive_rate / overall_positive_rate
        
        return lift
    
    @staticmethod
    def calculate_gain(y_true, y_proba):
        """è®¡ç®—å¢ç›Šå›¾æ•°æ®"""
        # æŒ‰æ¦‚ç‡æ’åº
        sorted_indices = np.argsort(y_proba)[::-1]
        y_true_sorted = y_true[sorted_indices]
        
        # ç´¯ç§¯æ­£ä¾‹
        cumulative_positives = np.cumsum(y_true_sorted)
        total_positives = np.sum(y_true)
        
        # å¢ç›Š
        gain = cumulative_positives / total_positives
        
        # ç™¾åˆ†æ¯”
        percentile = np.arange(1, len(y_true) + 1) / len(y_true)
        
        return percentile, gain
    
    @staticmethod
    def expected_calibration_error(y_true, y_proba, n_bins=10):
        """æœŸæœ›æ ¡å‡†è¯¯å·®"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (y_proba > bin_lower) & (y_proba <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                accuracy_in_bin = y_true[in_bin].mean()
                avg_confidence_in_bin = y_proba[in_bin].mean()
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
    
    @staticmethod
    def profit_curve(y_true, y_proba, cost_benefit_matrix):
        """åˆ©æ¶¦æ›²çº¿"""
        # cost_benefit_matrix: [[TP_benefit, FP_cost], [FN_cost, TN_benefit]]
        
        thresholds = np.sort(np.unique(y_proba))[::-1]
        profits = []
        
        for threshold in thresholds:
            y_pred = (y_proba >= threshold).astype(int)
            
            tp = np.sum((y_true == 1) & (y_pred == 1))
            fp = np.sum((y_true == 0) & (y_pred == 1))
            fn = np.sum((y_true == 1) & (y_pred == 0))
            tn = np.sum((y_true == 0) & (y_pred == 0))
            
            profit = (tp * cost_benefit_matrix[0][0] +
                     fp * cost_benefit_matrix[0][1] +
                     fn * cost_benefit_matrix[1][0] +
                     tn * cost_benefit_matrix[1][1])
            
            profits.append(profit)
        
        return thresholds, profits
```

## 8. æ—¶é—´åºåˆ—è¯„ä¼° ğŸ“…

```python
class TimeSeriesEvaluation:
    """æ—¶é—´åºåˆ—æ¨¡å‹è¯„ä¼°"""
    
    @staticmethod
    def walk_forward_validation(model, data, window_size, horizon):
        """å‘å‰æ»šåŠ¨éªŒè¯"""
        predictions = []
        actuals = []
        
        for i in range(window_size, len(data) - horizon):
            # è®­ç»ƒçª—å£
            train_data = data[i-window_size:i]
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(train_data)
            
            # é¢„æµ‹
            pred = model.predict(horizon)
            predictions.append(pred)
            
            # å®é™…å€¼
            actual = data[i:i+horizon]
            actuals.append(actual)
        
        return np.array(predictions), np.array(actuals)
    
    @staticmethod
    def time_series_cv_score(model, data, cv_splits, scoring_func):
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        tscv = TimeSeriesSplit(n_splits=cv_splits)
        scores = []
        
        for train_index, test_index in tscv.split(data):
            train, test = data[train_index], data[test_index]
            
            model.fit(train)
            predictions = model.predict(len(test))
            
            score = scoring_func(test, predictions)
            scores.append(score)
        
        return scores
    
    @staticmethod
    def directional_accuracy(y_true, y_pred):
        """æ–¹å‘å‡†ç¡®ç‡"""
        # è®¡ç®—å˜åŒ–æ–¹å‘
        true_direction = np.sign(np.diff(y_true))
        pred_direction = np.sign(np.diff(y_pred))
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = np.mean(true_direction == pred_direction)
        
        return accuracy
    
    @staticmethod
    def mase(y_true, y_pred, y_train):
        """å¹³å‡ç»å¯¹æ ‡å‡†è¯¯å·®"""
        n = len(y_true)
        d = np.abs(np.diff(y_train)).sum() / (len(y_train) - 1)
        
        errors = np.abs(y_true - y_pred)
        mase = errors.mean() / d
        
        return mase
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“

```python
def evaluation_best_practices():
    """è¯„ä¼°æœ€ä½³å®è·µ"""
    
    practices = {
        "æ•°æ®åˆ†å‰²": [
            "ä½¿ç”¨åˆ†å±‚é‡‡æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹",
            "æ—¶é—´åºåˆ—æ•°æ®ä¸èƒ½éšæœºåˆ†å‰²",
            "ä¿æŒéªŒè¯é›†å’Œæµ‹è¯•é›†ç‹¬ç«‹",
            "è€ƒè™‘æ•°æ®æ³„éœ²é£é™©"
        ],
        
        "æŒ‡æ ‡é€‰æ‹©": [
            "ä¸å¹³è¡¡æ•°æ®ä½¿ç”¨F1ã€AUCè€Œéå‡†ç¡®ç‡",
            "å›å½’é—®é¢˜åŒæ—¶è€ƒè™‘MAEå’ŒRMSE",
            "ä¸šåŠ¡åœºæ™¯å†³å®šæŒ‡æ ‡æƒé‡",
            "ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ç»¼åˆè¯„ä¼°"
        ],
        
        "äº¤å‰éªŒè¯": [
            "é€‰æ‹©åˆé€‚çš„CVç­–ç•¥",
            "åµŒå¥—CVç”¨äºè¶…å‚æ•°è°ƒä¼˜",
            "æ—¶é—´åºåˆ—ä½¿ç”¨å‰å‘éªŒè¯",
            "åˆ†ç»„æ•°æ®ä½¿ç”¨GroupKFold"
        ],
        
        "ç»Ÿè®¡æ£€éªŒ": [
            "ä½¿ç”¨é…å¯¹tæ£€éªŒæ¯”è¾ƒæ¨¡å‹",
            "å¤šé‡æ¯”è¾ƒéœ€è¦æ ¡æ­£",
            "æ³¨æ„æ ·æœ¬é‡å¯¹æ˜¾è‘—æ€§çš„å½±å“",
            "æŠ¥å‘Šæ•ˆåº”é‡è€Œéä»…på€¼"
        ],
        
        "å¯è§†åŒ–": [
            "å§‹ç»ˆç»˜åˆ¶å­¦ä¹ æ›²çº¿",
            "æ£€æŸ¥æ®‹å·®åˆ†å¸ƒ",
            "ä½¿ç”¨æ··æ·†çŸ©é˜µç†è§£é”™è¯¯",
            "ç»˜åˆ¶æ ¡å‡†æ›²çº¿"
        ]
    }
    
    return practices

# å¸¸è§é™·é˜±
common_pitfalls = """
1. æ•°æ®æ³„éœ²ï¼šåœ¨é¢„å¤„ç†æ—¶ä½¿ç”¨äº†æµ‹è¯•é›†ä¿¡æ¯
2. è¿‡æ‹ŸåˆéªŒè¯é›†ï¼šå¤šæ¬¡è°ƒå‚å¯¼è‡´å¯¹éªŒè¯é›†è¿‡æ‹Ÿåˆ
3. å¿½è§†ç±»åˆ«ä¸å¹³è¡¡ï¼šä½¿ç”¨ä¸å½“çš„è¯„ä¼°æŒ‡æ ‡
4. å•ä¸€æŒ‡æ ‡ï¼šåªå…³æ³¨ä¸€ä¸ªæŒ‡æ ‡å¿½ç•¥å…¶ä»–
5. å¿½è§†ç»Ÿè®¡æ˜¾è‘—æ€§ï¼šå°æ ·æœ¬çš„å¶ç„¶æ€§
6. é”™è¯¯çš„CVç­–ç•¥ï¼šæ—¶é—´åºåˆ—ç”¨äº†éšæœºCV
7. å¿½è§†ä¸šåŠ¡æŒ‡æ ‡ï¼šæ¨¡å‹æŒ‡æ ‡å¥½ä½†ä¸šåŠ¡ä»·å€¼ä½
"""

print("æ¨¡å‹è¯„ä¼°æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [ç‰¹å¾å·¥ç¨‹](feature_engineering.md) - æå‡æ¨¡å‹è¾“å…¥è´¨é‡
- [è¶…å‚æ•°è°ƒä¼˜](hyperparameter_tuning.md) - ä¼˜åŒ–æ¨¡å‹å‚æ•°
- [é›†æˆå­¦ä¹ ](ensemble.md) - ç»„åˆå¤šä¸ªæ¨¡å‹