# å›å½’ç®—æ³•è¯¦è§£ ğŸ“ˆ

å›å½’åˆ†æå°±åƒæ˜¯åœ¨å¯»æ‰¾æ•°æ®ç‚¹ä¹‹é—´çš„"æœ€ä½³æ‹Ÿåˆçº¿"ï¼Œå¸®åŠ©æˆ‘ä»¬é¢„æµ‹è¿ç»­çš„æ•°å€¼ã€‚

## 1. çº¿æ€§å›å½’ (Linear Regression)

### æ ¸å¿ƒæ€æƒ³
æƒ³è±¡ä½ åœ¨æ•£ç‚¹å›¾ä¸Šç”»ä¸€æ¡ç›´çº¿ï¼Œè®©æ‰€æœ‰ç‚¹åˆ°è¿™æ¡çº¿çš„è·ç¦»ä¹‹å’Œæœ€å°ã€‚è¿™å°±æ˜¯çº¿æ€§å›å½’ï¼

### æ•°å­¦åŸç†
```
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ

å…¶ä¸­ï¼š
- y: ç›®æ ‡å˜é‡
- x: ç‰¹å¾å˜é‡
- Î²: å›å½’ç³»æ•°
- Îµ: è¯¯å·®é¡¹
```

### å®Œæ•´ä»£ç å®ç°

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
n_samples = 1000

# åˆ›å»ºç‰¹å¾
X = np.random.randn(n_samples, 3)
# åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæœ‰çº¿æ€§å…³ç³»ï¼‰
true_coefficients = [3.5, -2.1, 1.8]
y = X @ true_coefficients + np.random.randn(n_samples) * 0.5

# è½¬æ¢ä¸ºDataFrameä¾¿äºæŸ¥çœ‹
df = pd.DataFrame(X, columns=['ç‰¹å¾1', 'ç‰¹å¾2', 'ç‰¹å¾3'])
df['ç›®æ ‡å€¼'] = y

print("æ•°æ®é›†å‰5è¡Œï¼š")
print(df.head())
print(f"\næ•°æ®é›†å½¢çŠ¶ï¼š{df.shape}")

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# è¯„ä¼°æ¨¡å‹
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print("\næ¨¡å‹è¯„ä¼°ç»“æœï¼š")
print(f"è®­ç»ƒé›† MSE: {train_mse:.4f}, RÂ²: {train_r2:.4f}")
print(f"æµ‹è¯•é›† MSE: {test_mse:.4f}, RÂ²: {test_r2:.4f}")

print("\nå­¦ä¹ åˆ°çš„ç³»æ•°ï¼š")
print(f"çœŸå®ç³»æ•°: {true_coefficients}")
print(f"å­¦ä¹ ç³»æ•°: {model.coef_.tolist()}")
print(f"æˆªè·é¡¹: {model.intercept_:.4f}")

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. é¢„æµ‹å€¼ vs çœŸå®å€¼
axes[0, 0].scatter(y_test, y_test_pred, alpha=0.5)
axes[0, 0].plot([y_test.min(), y_test.max()], 
                [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0, 0].set_xlabel('çœŸå®å€¼')
axes[0, 0].set_ylabel('é¢„æµ‹å€¼')
axes[0, 0].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')

# 2. æ®‹å·®å›¾
residuals = y_test - y_test_pred
axes[0, 1].scatter(y_test_pred, residuals, alpha=0.5)
axes[0, 1].axhline(y=0, color='r', linestyle='--')
axes[0, 1].set_xlabel('é¢„æµ‹å€¼')
axes[0, 1].set_ylabel('æ®‹å·®')
axes[0, 1].set_title('æ®‹å·®å›¾')

# 3. æ®‹å·®åˆ†å¸ƒ
axes[1, 0].hist(residuals, bins=30, edgecolor='black')
axes[1, 0].set_xlabel('æ®‹å·®')
axes[1, 0].set_ylabel('é¢‘æ•°')
axes[1, 0].set_title('æ®‹å·®åˆ†å¸ƒ')

# 4. ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'ç‰¹å¾': ['ç‰¹å¾1', 'ç‰¹å¾2', 'ç‰¹å¾3'],
    'ç³»æ•°': model.coef_
})
axes[1, 1].bar(feature_importance['ç‰¹å¾'], 
               feature_importance['ç³»æ•°'])
axes[1, 1].set_xlabel('ç‰¹å¾')
axes[1, 1].set_ylabel('ç³»æ•°å€¼')
axes[1, 1].set_title('ç‰¹å¾é‡è¦æ€§')

plt.tight_layout()
plt.show()
```

### å®æˆ˜æ¡ˆä¾‹ï¼šæˆ¿ä»·é¢„æµ‹

```python
# ä½¿ç”¨çœŸå®åœºæ™¯çš„ç‰¹å¾å
def create_house_price_data(n_samples=1000):
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æˆ¿ä»·æ•°æ®"""
    np.random.seed(42)
    
    # ç”Ÿæˆç‰¹å¾
    area = np.random.uniform(50, 300, n_samples)  # é¢ç§¯(å¹³æ–¹ç±³)
    rooms = np.random.randint(1, 6, n_samples)    # æˆ¿é—´æ•°
    age = np.random.uniform(0, 50, n_samples)     # æˆ¿é¾„
    distance_to_center = np.random.uniform(1, 30, n_samples)  # åˆ°å¸‚ä¸­å¿ƒè·ç¦»(km)
    
    # è®¡ç®—æˆ¿ä»·ï¼ˆå¸¦æœ‰çœŸå®çš„é€»è¾‘å…³ç³»ï¼‰
    price = (
        area * 15000 +                    # æ¯å¹³ç±³15000å…ƒ
        rooms * 50000 +                    # æ¯ä¸ªæˆ¿é—´åŠ 5ä¸‡
        age * (-5000) +                    # æ¯å¹´æŠ˜æ—§5000
        distance_to_center * (-10000) +   # æ¯å…¬é‡Œç¦»å¸‚ä¸­å¿ƒå‡1ä¸‡
        np.random.randn(n_samples) * 50000  # éšæœºå™ªå£°
    )
    
    # ç¡®ä¿ä»·æ ¼ä¸ºæ­£
    price = np.maximum(price, 100000)
    
    return pd.DataFrame({
        'é¢ç§¯': area,
        'æˆ¿é—´æ•°': rooms,
        'æˆ¿é¾„': age,
        'åˆ°å¸‚ä¸­å¿ƒè·ç¦»': distance_to_center,
        'æˆ¿ä»·': price
    })

# åˆ›å»ºæ•°æ®
house_data = create_house_price_data()
print("æˆ¿ä»·æ•°æ®ç»Ÿè®¡ï¼š")
print(house_data.describe())

# å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
X = house_data[['é¢ç§¯', 'æˆ¿é—´æ•°', 'æˆ¿é¾„', 'åˆ°å¸‚ä¸­å¿ƒè·ç¦»']]
y = house_data['æˆ¿ä»·']

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# åˆ›å»ºç»“æœDataFrame
results = pd.DataFrame({
    'çœŸå®æˆ¿ä»·': y_test.values,
    'é¢„æµ‹æˆ¿ä»·': y_pred,
    'è¯¯å·®': y_test.values - y_pred,
    'è¯¯å·®ç™¾åˆ†æ¯”': np.abs((y_test.values - y_pred) / y_test.values * 100)
})

print("\né¢„æµ‹ç»“æœç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰ï¼š")
print(results.head(10))

print(f"\nå¹³å‡ç»å¯¹è¯¯å·®: {np.mean(np.abs(results['è¯¯å·®'])):.2f}å…ƒ")
print(f"å¹³å‡è¯¯å·®ç™¾åˆ†æ¯”: {results['è¯¯å·®ç™¾åˆ†æ¯”'].mean():.2f}%")

# ç‰¹å¾å½±å“åˆ†æ
feature_impact = pd.DataFrame({
    'ç‰¹å¾': X.columns,
    'ç³»æ•°': model.coef_,
    'å½±å“è¯´æ˜': [
        f'æ¯å¹³ç±³å½±å“{model.coef_[0]:.2f}å…ƒ',
        f'æ¯ä¸ªæˆ¿é—´å½±å“{model.coef_[1]:.2f}å…ƒ',
        f'æ¯å¹´æˆ¿é¾„å½±å“{model.coef_[2]:.2f}å…ƒ',
        f'æ¯å…¬é‡Œè·ç¦»å½±å“{model.coef_[3]:.2f}å…ƒ'
    ]
})
print("\nç‰¹å¾å½±å“åˆ†æï¼š")
print(feature_impact)
```

## 2. Lassoå›å½’ (L1æ­£åˆ™åŒ–)

### æ ¸å¿ƒæ€æƒ³
Lassoåƒæ˜¯ä¸€ä¸ª"èŠ‚ä¿­"çš„çº¿æ€§å›å½’ï¼Œå®ƒä¼šæŠŠä¸é‡è¦çš„ç‰¹å¾ç³»æ•°ç›´æ¥å‹ç¼©åˆ°0ï¼Œå®ç°ç‰¹å¾é€‰æ‹©ã€‚

### å®Œæ•´ä»£ç å®ç°

```python
from sklearn.linear_model import Lasso, LassoCV
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# åˆ›å»ºä¸€ä¸ªæœ‰å¾ˆå¤šç‰¹å¾çš„æ•°æ®é›†ï¼ˆåŒ…å«æ— ç”¨ç‰¹å¾ï¼‰
np.random.seed(42)
n_samples = 500
n_features = 20
n_informative = 5  # åªæœ‰5ä¸ªç‰¹å¾æ˜¯æœ‰ç”¨çš„

# ç”Ÿæˆæ•°æ®
X = np.random.randn(n_samples, n_features)
# åªä½¿ç”¨å‰5ä¸ªç‰¹å¾ç”Ÿæˆy
true_coef = np.zeros(n_features)
true_coef[:n_informative] = [3, -2, 1.5, 4, -3.5]
y = X @ true_coef + np.random.randn(n_samples) * 0.1

# ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆLassoå¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³alpha
lasso_cv = LassoCV(cv=5, random_state=42, max_iter=10000)
lasso_cv.fit(X_train, y_train)

print(f"æœ€ä½³alphaå€¼: {lasso_cv.alpha_:.4f}")

# è®­ç»ƒæœ€ç»ˆæ¨¡å‹
lasso = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)
lasso.fit(X_train, y_train)

# è¯„ä¼°
y_pred = lasso.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\næ¨¡å‹æ€§èƒ½:")
print(f"MSE: {mse:.4f}")
print(f"RÂ²: {r2:.4f}")

# ç‰¹å¾é€‰æ‹©æ•ˆæœ
selected_features = np.where(np.abs(lasso.coef_) > 0.01)[0]
print(f"\nç‰¹å¾é€‰æ‹©ç»“æœ:")
print(f"åŸå§‹ç‰¹å¾æ•°: {n_features}")
print(f"é€‰ä¸­ç‰¹å¾æ•°: {len(selected_features)}")
print(f"é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•: {selected_features.tolist()}")

# å¯è§†åŒ–ç³»æ•°
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# 1. çœŸå®ç³»æ•°
axes[0].bar(range(n_features), true_coef)
axes[0].set_title('çœŸå®ç³»æ•°')
axes[0].set_xlabel('ç‰¹å¾ç´¢å¼•')
axes[0].set_ylabel('ç³»æ•°å€¼')

# 2. Lassoç³»æ•°
axes[1].bar(range(n_features), lasso.coef_)
axes[1].set_title('Lassoå­¦ä¹ çš„ç³»æ•°')
axes[1].set_xlabel('ç‰¹å¾ç´¢å¼•')
axes[1].set_ylabel('ç³»æ•°å€¼')

# 3. å¯¹æ¯”æ™®é€šçº¿æ€§å›å½’
lr = LinearRegression()
lr.fit(X_train, y_train)
axes[2].bar(range(n_features), lr.coef_)
axes[2].set_title('æ™®é€šçº¿æ€§å›å½’ç³»æ•°')
axes[2].set_xlabel('ç‰¹å¾ç´¢å¼•')
axes[2].set_ylabel('ç³»æ•°å€¼')

plt.tight_layout()
plt.show()

# Lassoè·¯å¾„ï¼šå±•ç¤ºä¸åŒalphaä¸‹çš„ç³»æ•°å˜åŒ–
alphas = np.logspace(-4, 1, 50)
coefs = []

for alpha in alphas:
    lasso_temp = Lasso(alpha=alpha, max_iter=10000)
    lasso_temp.fit(X_train, y_train)
    coefs.append(lasso_temp.coef_)

# ç»˜åˆ¶Lassoè·¯å¾„
plt.figure(figsize=(10, 6))
for i in range(n_features):
    plt.plot(alphas, [coef[i] for coef in coefs], 
             label=f'ç‰¹å¾{i}' if i < 5 else None)

plt.xscale('log')
plt.xlabel('Alpha (æ­£åˆ™åŒ–å¼ºåº¦)')
plt.ylabel('ç³»æ•°å€¼')
plt.title('Lassoè·¯å¾„ï¼šç³»æ•°éšæ­£åˆ™åŒ–å¼ºåº¦çš„å˜åŒ–')
plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.show()
```

## 3. Ridgeå›å½’ (L2æ­£åˆ™åŒ–)

### æ ¸å¿ƒæ€æƒ³
Ridgeåƒæ˜¯ä¸€ä¸ª"è°¨æ…"çš„çº¿æ€§å›å½’ï¼Œå®ƒä¼šç¼©å°æ‰€æœ‰ç³»æ•°ä½†ä¸ä¼šå°†å®ƒä»¬å‹ç¼©åˆ°0ã€‚

```python
from sklearn.linear_model import Ridge, RidgeCV

# ä½¿ç”¨ç›¸åŒçš„æ•°æ®
# Ridgeå›å½’
ridge_cv = RidgeCV(alphas=np.logspace(-4, 4, 100), cv=5)
ridge_cv.fit(X_train, y_train)

print(f"Ridgeæœ€ä½³alpha: {ridge_cv.alpha_:.4f}")

ridge = Ridge(alpha=ridge_cv.alpha_)
ridge.fit(X_train, y_train)

# å¯¹æ¯”ä¸‰ç§æ–¹æ³•
models = {
    'Linear': LinearRegression(),
    'Lasso': Lasso(alpha=lasso_cv.alpha_, max_iter=10000),
    'Ridge': Ridge(alpha=ridge_cv.alpha_)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = {
        'MSE': mean_squared_error(y_test, y_pred),
        'RÂ²': r2_score(y_test, y_pred),
        'éé›¶ç³»æ•°': np.sum(np.abs(model.coef_) > 0.01)
    }

# åˆ›å»ºå¯¹æ¯”è¡¨
comparison_df = pd.DataFrame(results).T
print("\nä¸‰ç§å›å½’æ–¹æ³•å¯¹æ¯”:")
print(comparison_df)

# å¯è§†åŒ–å¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# ç³»æ•°å¯¹æ¯”
models_coef = {
    'Linear': LinearRegression().fit(X_train, y_train).coef_,
    'Lasso': lasso.coef_,
    'Ridge': ridge.coef_
}

x_pos = np.arange(n_features)
width = 0.25

for idx, (name, coef) in enumerate(models_coef.items()):
    axes[0, 0].bar(x_pos + idx * width, coef, width, label=name)

axes[0, 0].set_xlabel('ç‰¹å¾ç´¢å¼•')
axes[0, 0].set_ylabel('ç³»æ•°å€¼')
axes[0, 0].set_title('ä¸åŒæ¨¡å‹çš„ç³»æ•°å¯¹æ¯”')
axes[0, 0].legend()

# MSEå¯¹æ¯”
axes[0, 1].bar(results.keys(), [r['MSE'] for r in results.values()])
axes[0, 1].set_ylabel('MSE')
axes[0, 1].set_title('å‡æ–¹è¯¯å·®å¯¹æ¯”')

# RÂ²å¯¹æ¯”
axes[1, 0].bar(results.keys(), [r['RÂ²'] for r in results.values()])
axes[1, 0].set_ylabel('RÂ²')
axes[1, 0].set_title('RÂ²å¾—åˆ†å¯¹æ¯”')

# éé›¶ç³»æ•°æ•°é‡
axes[1, 1].bar(results.keys(), [r['éé›¶ç³»æ•°'] for r in results.values()])
axes[1, 1].set_ylabel('éé›¶ç³»æ•°æ•°é‡')
axes[1, 1].set_title('ç‰¹å¾é€‰æ‹©æ•ˆæœ')

plt.tight_layout()
plt.show()
```

## 4. æ”¯æŒå‘é‡å›å½’ (SVR)

### æ ¸å¿ƒæ€æƒ³
SVRåœ¨ä¸€ä¸ª"ç®¡é“"å†…æ‹Ÿåˆæ•°æ®ï¼Œåªå…³å¿ƒç®¡é“å¤–çš„ç‚¹ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ã€‚

```python
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

# åˆ›å»ºéçº¿æ€§æ•°æ®
np.random.seed(42)
X_nonlinear = np.sort(5 * np.random.rand(100, 1), axis=0)
y_nonlinear = np.sin(X_nonlinear).ravel() + np.random.randn(100) * 0.1

# å°è¯•ä¸åŒçš„æ ¸å‡½æ•°
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
svr_models = {}

plt.figure(figsize=(15, 10))

for i, kernel in enumerate(kernels, 1):
    # è®­ç»ƒSVR
    if kernel == 'rbf':
        svr = SVR(kernel=kernel, C=100, gamma=0.1, epsilon=0.01)
    else:
        svr = SVR(kernel=kernel, C=100, epsilon=0.01)
    
    svr.fit(X_nonlinear, y_nonlinear)
    svr_models[kernel] = svr
    
    # é¢„æµ‹
    X_plot = np.linspace(0, 5, 200).reshape(-1, 1)
    y_pred = svr.predict(X_plot)
    
    # å¯è§†åŒ–
    plt.subplot(2, 2, i)
    plt.scatter(X_nonlinear, y_nonlinear, alpha=0.5, label='æ•°æ®ç‚¹')
    plt.plot(X_plot, y_pred, 'r-', label=f'SVR ({kernel})')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'SVR with {kernel} kernel')
    plt.legend()

plt.tight_layout()
plt.show()

# ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–RBFæ ¸SVR
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
    'epsilon': [0.01, 0.1, 0.2]
}

svr_rbf = SVR(kernel='rbf')
grid_search = GridSearchCV(svr_rbf, param_grid, cv=5, 
                          scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_nonlinear, y_nonlinear)

print("SVRæœ€ä½³å‚æ•°:")
print(grid_search.best_params_)
print(f"æœ€ä½³å¾—åˆ†: {-grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³å‚æ•°çš„æ¨¡å‹
best_svr = grid_search.best_estimator_
X_plot = np.linspace(0, 5, 200).reshape(-1, 1)
y_pred_best = best_svr.predict(X_plot)

plt.figure(figsize=(10, 6))
plt.scatter(X_nonlinear, y_nonlinear, alpha=0.5, label='è®­ç»ƒæ•°æ®')
plt.plot(X_plot, y_pred_best, 'r-', label='ä¼˜åŒ–åçš„SVR', linewidth=2)
plt.plot(X_plot, np.sin(X_plot), 'g--', label='çœŸå®å‡½æ•°', linewidth=2)
plt.xlabel('X')
plt.ylabel('y')
plt.title('ä¼˜åŒ–åçš„SVRæ‹Ÿåˆæ•ˆæœ')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## 5. å®æˆ˜é¡¹ç›®ï¼šå¤šç®—æ³•å¯¹æ¯”

```python
def compare_regression_models(X, y, model_dict, test_size=0.2):
    """
    å¯¹æ¯”å¤šä¸ªå›å½’æ¨¡å‹çš„æ€§èƒ½
    """
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=test_size, random_state=42
    )
    
    results = []
    predictions = {}
    
    for name, model in model_dict.items():
        # è®­ç»ƒ
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        predictions[name] = y_test_pred
        
        # è¯„ä¼°
        train_mse = mean_squared_error(y_train, y_train_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        
        results.append({
            'æ¨¡å‹': name,
            'è®­ç»ƒMSE': train_mse,
            'æµ‹è¯•MSE': test_mse,
            'è®­ç»ƒRÂ²': train_r2,
            'æµ‹è¯•RÂ²': test_r2,
            'è¿‡æ‹Ÿåˆåº¦': train_mse - test_mse
        })
    
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('æµ‹è¯•RÂ²', ascending=False)
    
    return results_df, predictions, y_test

# åˆ›å»ºç¤ºä¾‹æ•°æ®
from sklearn.datasets import make_regression
X, y = make_regression(n_samples=1000, n_features=20, 
                       n_informative=10, noise=10, random_state=42)

# å®šä¹‰æ¨¡å‹
models = {
    'Linear': LinearRegression(),
    'Lasso': Lasso(alpha=0.1, max_iter=10000),
    'Ridge': Ridge(alpha=1.0),
    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000),
    'SVR_linear': SVR(kernel='linear', C=1.0),
    'SVR_rbf': SVR(kernel='rbf', C=100, gamma=0.01)
}

# æ¯”è¾ƒæ¨¡å‹
results_df, predictions, y_test = compare_regression_models(X, y, models)

print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
print(results_df.to_string())

# å¯è§†åŒ–ç»“æœ
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for idx, (name, y_pred) in enumerate(predictions.items()):
    row, col = divmod(idx, 3)
    ax = axes[row, col]
    
    # ç»˜åˆ¶é¢„æµ‹vsçœŸå®
    ax.scatter(y_test, y_pred, alpha=0.5, s=10)
    ax.plot([y_test.min(), y_test.max()], 
            [y_test.min(), y_test.max()], 'r--', lw=2)
    ax.set_xlabel('çœŸå®å€¼')
    ax.set_ylabel('é¢„æµ‹å€¼')
    ax.set_title(f'{name}\nRÂ²={r2_score(y_test, y_pred):.3f}')
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# RÂ²å¯¹æ¯”
axes[0].barh(results_df['æ¨¡å‹'], results_df['æµ‹è¯•RÂ²'])
axes[0].set_xlabel('RÂ² Score')
axes[0].set_title('æ¨¡å‹RÂ²å¾—åˆ†å¯¹æ¯”')

# MSEå¯¹æ¯”
axes[1].barh(results_df['æ¨¡å‹'], results_df['æµ‹è¯•MSE'])
axes[1].set_xlabel('MSE')
axes[1].set_title('æ¨¡å‹MSEå¯¹æ¯”')

plt.tight_layout()
plt.show()
```

## æœ€ä½³å®è·µå»ºè®®

### 1. ç®—æ³•é€‰æ‹©æŒ‡å—
- **çº¿æ€§å›å½’**ï¼šæ•°æ®çº¿æ€§å…³ç³»æ˜æ˜¾ï¼Œç‰¹å¾è¾ƒå°‘
- **Lasso**ï¼šç‰¹å¾å¾ˆå¤šï¼Œéœ€è¦ç‰¹å¾é€‰æ‹©
- **Ridge**ï¼šç‰¹å¾ä¹‹é—´æœ‰å¤šé‡å…±çº¿æ€§
- **SVR**ï¼šæ•°æ®æœ‰å¼‚å¸¸å€¼ï¼Œéçº¿æ€§å…³ç³»

### 2. è°ƒå‚æŠ€å·§
```python
# é€šç”¨çš„è¶…å‚æ•°è°ƒä¼˜æ¨¡æ¿
from sklearn.model_selection import RandomizedSearchCV

def tune_model(model, param_distributions, X_train, y_train):
    """
    ä½¿ç”¨éšæœºæœç´¢è°ƒä¼˜æ¨¡å‹
    """
    random_search = RandomizedSearchCV(
        model, 
        param_distributions,
        n_iter=100,
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42
    )
    
    random_search.fit(X_train, y_train)
    
    print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
    print(f"æœ€ä½³å¾—åˆ†: {-random_search.best_score_:.4f}")
    
    return random_search.best_estimator_
```

### 3. ç‰¹å¾å·¥ç¨‹æç¤º
- æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯ä½¿ç”¨æ­£åˆ™åŒ–æ—¶ï¼‰
- å¤„ç†ç¼ºå¤±å€¼
- åˆ›å»ºäº¤äº’ç‰¹å¾
- ä½¿ç”¨å¤šé¡¹å¼ç‰¹å¾æ‰©å±•

### 4. é¿å…è¿‡æ‹Ÿåˆ
- ä½¿ç”¨äº¤å‰éªŒè¯
- å¢åŠ æ­£åˆ™åŒ–
- å‡å°‘ç‰¹å¾æ•°é‡
- å¢åŠ è®­ç»ƒæ•°æ®

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [åˆ†ç±»ç®—æ³•](classification.md) - å­¦ä¹ å¦‚ä½•é¢„æµ‹ç±»åˆ«
- [é›†æˆå­¦ä¹ ](ensemble.md) - äº†è§£æ›´å¼ºå¤§çš„ç®—æ³•
- [ç‰¹å¾å·¥ç¨‹](feature_engineering.md) - æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®