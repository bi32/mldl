# ä¿¡æ¯ç†è®ºï¼šæœºå™¨å­¦ä¹ çš„ä¿¡æ¯è®ºåŸºç¡€ ğŸ“¡

æ·±å…¥ç†è§£ä¿¡æ¯ç†è®ºåœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œä»ç†µåˆ°äº’ä¿¡æ¯ã€‚

## 1. ä¿¡æ¯è®ºåŸºç¡€æ¦‚å¿µ ğŸ’¡

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.datasets import make_classification
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import mutual_info_score
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

class InformationTheoryBasics:
    """ä¿¡æ¯è®ºåŸºç¡€"""
    
    def __init__(self):
        self.concepts = {}
    
    def entropy_concept(self):
        """ç†µçš„æ¦‚å¿µ"""
        print("=== ä¿¡æ¯ç†µ (Information Entropy) ===")
        
        print("Shannonç†µå®šä¹‰:")
        print("H(X) = -Î£áµ¢ P(xáµ¢) logâ‚‚ P(xáµ¢)")
        print()
        print("ç‰©ç†æ„ä¹‰:")
        print("- æµ‹é‡éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§")
        print("- ç¼–ç è¯¥å˜é‡æ‰€éœ€çš„æœ€å°‘æ¯”ç‰¹æ•°")
        print("- ç³»ç»Ÿçš„ä¿¡æ¯å«é‡")
        print()
        
        entropy_properties = {
            "éè´Ÿæ€§": "H(X) â‰¥ 0ï¼Œå½“ä¸”ä»…å½“Xç¡®å®šæ—¶ç­‰å·æˆç«‹",
            "æœ€å¤§å€¼": "H(X) â‰¤ logâ‚‚|ğ’³|ï¼Œå‡åŒ€åˆ†å¸ƒæ—¶è¾¾åˆ°æœ€å¤§",
            "å¯åŠ æ€§": "ç‹¬ç«‹å˜é‡ï¼šH(X,Y) = H(X) + H(Y)",
            "å‡¹å‡½æ•°": "ç†µæ˜¯æ¦‚ç‡åˆ†å¸ƒçš„å‡¹å‡½æ•°"
        }
        
        for prop, desc in entropy_properties.items():
            print(f"{prop}: {desc}")
        
        # ç†µçš„è®¡ç®—ç¤ºä¾‹
        self.demonstrate_entropy()
        
        return entropy_properties
    
    def demonstrate_entropy(self):
        """æ¼”ç¤ºç†µçš„è®¡ç®—"""
        print("\n=== ç†µè®¡ç®—ç¤ºä¾‹ ===")
        
        # ä¸åŒæ¦‚ç‡åˆ†å¸ƒçš„ç†µ
        distributions = {
            "ç¡®å®šæ€§": [1.0, 0.0, 0.0, 0.0],
            "åæ–œåˆ†å¸ƒ": [0.7, 0.2, 0.08, 0.02],
            "å‡åŒ€åˆ†å¸ƒ": [0.25, 0.25, 0.25, 0.25]
        }
        
        def calculate_entropy(probs):
            """è®¡ç®—ç†µ"""
            probs = np.array(probs)
            probs = probs[probs > 0]  # é¿å…log(0)
            return -np.sum(probs * np.log2(probs))
        
        entropies = {}
        for name, probs in distributions.items():
            entropy = calculate_entropy(probs)
            entropies[name] = entropy
            print(f"{name}: H = {entropy:.3f} bits")
        
        # å¯è§†åŒ–ä¸åŒåˆ†å¸ƒçš„ç†µ
        self.visualize_entropy_distributions(distributions, entropies)
        
        return entropies
    
    def visualize_entropy_distributions(self, distributions, entropies):
        """å¯è§†åŒ–ä¸åŒåˆ†å¸ƒçš„ç†µ"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # ç»˜åˆ¶æ¦‚ç‡åˆ†å¸ƒ
        for i, (name, probs) in enumerate(distributions.items()):
            ax = axes[0, i] if i < 2 else axes[1, 0]
            
            categories = [f'X{j+1}' for j in range(len(probs))]
            bars = ax.bar(categories, probs, alpha=0.7, 
                         color=plt.cm.viridis(i/len(distributions)))
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, prob in zip(bars, probs):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{prob:.2f}', ha='center', va='bottom')
            
            ax.set_title(f'{name}\nH = {entropies[name]:.3f} bits')
            ax.set_ylabel('æ¦‚ç‡')
            ax.set_ylim(0, 1.1)
            ax.grid(True, alpha=0.3)
        
        # ç†µå€¼æ¯”è¾ƒ
        ax = axes[1, 1]
        names = list(entropies.keys())
        values = list(entropies.values())
        
        bars = ax.bar(names, values, alpha=0.7, color='orange')
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   f'{value:.3f}', ha='center', va='bottom')
        
        ax.set_title('ç†µå€¼æ¯”è¾ƒ')
        ax.set_ylabel('ç†µ (bits)')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def conditional_entropy(self):
        """æ¡ä»¶ç†µ"""
        print("=== æ¡ä»¶ç†µ (Conditional Entropy) ===")
        
        print("å®šä¹‰:")
        print("H(Y|X) = Î£â‚“ P(x) H(Y|X=x)")
        print("     = -Î£â‚“áµ§ P(x,y) logâ‚‚ P(y|x)")
        print()
        
        print("æ€§è´¨:")
        print("- H(Y|X) â‰¤ H(Y)ï¼Œç­‰å·æˆç«‹å½“ä¸”ä»…å½“X,Yç‹¬ç«‹")
        print("- H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)")
        print("- é“¾å¼æ³•åˆ™ï¼šH(Xâ‚,...,Xâ‚™) = Î£áµ¢ H(Xáµ¢|Xâ‚,...,Xáµ¢â‚‹â‚)")
        print()
        
        # æ¡ä»¶ç†µå®ä¾‹
        self.demonstrate_conditional_entropy()
        
        return self.visualize_entropy_relationships()
    
    def demonstrate_conditional_entropy(self):
        """æ¼”ç¤ºæ¡ä»¶ç†µ"""
        print("=== æ¡ä»¶ç†µè®¡ç®—ç¤ºä¾‹ ===")
        
        # åˆ›å»ºè”åˆåˆ†å¸ƒè¡¨
        # X: å¤©æ°” (æ™´å¤©=0, é›¨å¤©=1)
        # Y: å¿ƒæƒ… (å¥½=0, å=1)
        joint_prob = np.array([
            [0.4, 0.1],  # P(X=0,Y=0), P(X=0,Y=1) - æ™´å¤©
            [0.2, 0.3]   # P(X=1,Y=0), P(X=1,Y=1) - é›¨å¤©
        ])
        
        # è¾¹é™…æ¦‚ç‡
        p_x = joint_prob.sum(axis=1)  # P(X)
        p_y = joint_prob.sum(axis=0)  # P(Y)
        
        # æ¡ä»¶æ¦‚ç‡ P(Y|X)
        p_y_given_x = joint_prob / p_x[:, np.newaxis]
        
        # è®¡ç®—å„ç§ç†µ
        def entropy(probs):
            probs = probs[probs > 0]
            return -np.sum(probs * np.log2(probs))
        
        H_X = entropy(p_x)
        H_Y = entropy(p_y)
        H_XY = entropy(joint_prob.flatten())
        
        # æ¡ä»¶ç†µ H(Y|X)
        H_Y_given_X = np.sum(p_x * [entropy(p_y_given_x[i]) for i in range(len(p_x))])
        
        print(f"H(X) = {H_X:.3f} bits")
        print(f"H(Y) = {H_Y:.3f} bits")
        print(f"H(X,Y) = {H_XY:.3f} bits")
        print(f"H(Y|X) = {H_Y_given_X:.3f} bits")
        print(f"éªŒè¯é“¾å¼æ³•åˆ™: H(X,Y) = H(X) + H(Y|X) = {H_X + H_Y_given_X:.3f}")
        
        return joint_prob, p_x, p_y
    
    def visualize_entropy_relationships(self):
        """å¯è§†åŒ–ç†µçš„å…³ç³»"""
        print("\n=== ç†µå…³ç³»å›¾ ===")
        
        # åˆ›å»ºç»´æ©å›¾é£æ ¼çš„ç†µå…³ç³»å›¾
        fig, ax = plt.subplots(1, 1, figsize=(10, 8))
        
        # ç»˜åˆ¶æ¦‚å¿µå›¾
        from matplotlib.patches import Circle, Rectangle
        
        # H(X,Y) æ€»æ¡†
        rect = Rectangle((0, 0), 4, 3, linewidth=2, edgecolor='black', 
                        facecolor='lightblue', alpha=0.3)
        ax.add_patch(rect)
        
        # H(X) åœ†
        circle_x = Circle((1.2, 1.5), 0.8, linewidth=2, edgecolor='red', 
                         facecolor='pink', alpha=0.5)
        ax.add_patch(circle_x)
        
        # H(Y) åœ†
        circle_y = Circle((2.8, 1.5), 0.8, linewidth=2, edgecolor='blue', 
                         facecolor='lightcyan', alpha=0.5)
        ax.add_patch(circle_y)
        
        # æ ‡æ³¨
        ax.text(0.8, 1.5, 'H(X|Y)', fontsize=12, ha='center', va='center')
        ax.text(2, 1.5, 'I(X;Y)', fontsize=12, ha='center', va='center', 
               bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
        ax.text(3.2, 1.5, 'H(Y|X)', fontsize=12, ha='center', va='center')
        ax.text(1.2, 2.5, 'H(X)', fontsize=12, ha='center', va='center', weight='bold')
        ax.text(2.8, 2.5, 'H(Y)', fontsize=12, ha='center', va='center', weight='bold')
        ax.text(2, 0.3, 'H(X,Y)', fontsize=14, ha='center', va='center', weight='bold')
        
        ax.set_xlim(-0.5, 4.5)
        ax.set_ylim(-0.5, 3.5)
        ax.set_aspect('equal')
        ax.axis('off')
        ax.set_title('ä¿¡æ¯è®ºé‡ä¹‹é—´çš„å…³ç³»', fontsize=16)
        
        plt.tight_layout()
        plt.show()

class MutualInformation:
    """äº’ä¿¡æ¯"""
    
    def __init__(self):
        pass
    
    def mutual_information_concept(self):
        """äº’ä¿¡æ¯æ¦‚å¿µ"""
        print("=== äº’ä¿¡æ¯ (Mutual Information) ===")
        
        print("å®šä¹‰:")
        print("I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)")
        print("      = H(X) + H(Y) - H(X,Y)")
        print("      = Î£â‚“áµ§ P(x,y) logâ‚‚[P(x,y)/(P(x)P(y))]")
        print()
        
        print("ç‰©ç†æ„ä¹‰:")
        print("- æµ‹é‡ä¸¤ä¸ªéšæœºå˜é‡ä¹‹é—´çš„ç›¸äº’ä¾èµ–ç¨‹åº¦")
        print("- Xå…³äºYæä¾›çš„ä¿¡æ¯é‡")
        print("- å‡å°‘çš„ä¸ç¡®å®šæ€§")
        print()
        
        print("æ€§è´¨:")
        print("- I(X;Y) â‰¥ 0ï¼Œç­‰å·æˆç«‹å½“ä¸”ä»…å½“X,Yç‹¬ç«‹")
        print("- I(X;Y) = I(Y;X) (å¯¹ç§°æ€§)")
        print("- I(X;X) = H(X)")
        print("- I(X;Y) â‰¤ min(H(X), H(Y))")
        
        # äº’ä¿¡æ¯è®¡ç®—ç¤ºä¾‹
        self.demonstrate_mutual_information()
        
        return self.feature_selection_example()
    
    def demonstrate_mutual_information(self):
        """æ¼”ç¤ºäº’ä¿¡æ¯è®¡ç®—"""
        print("\n=== äº’ä¿¡æ¯è®¡ç®—ç¤ºä¾‹ ===")
        
        # ä¸åŒç›¸å…³ç¨‹åº¦çš„å˜é‡å¯¹
        np.random.seed(42)
        n_samples = 1000
        
        # æƒ…å†µ1: å®Œå…¨ç‹¬ç«‹
        x1 = np.random.binomial(1, 0.5, n_samples)
        y1 = np.random.binomial(1, 0.5, n_samples)
        
        # æƒ…å†µ2: éƒ¨åˆ†ç›¸å…³
        x2 = np.random.binomial(1, 0.5, n_samples)
        y2 = np.where(np.random.random(n_samples) < 0.7, x2, 1-x2)
        
        # æƒ…å†µ3: å®Œå…¨ç›¸å…³
        x3 = np.random.binomial(1, 0.5, n_samples)
        y3 = x3.copy()
        
        # è®¡ç®—äº’ä¿¡æ¯
        cases = [
            ("ç‹¬ç«‹å˜é‡", x1, y1),
            ("éƒ¨åˆ†ç›¸å…³", x2, y2),
            ("å®Œå…¨ç›¸å…³", x3, y3)
        ]
        
        mutual_infos = []
        for name, x, y in cases:
            mi = mutual_info_score(x, y)
            mutual_infos.append(mi)
            print(f"{name}: I(X;Y) = {mi:.3f} bits")
        
        # å¯è§†åŒ–äº’ä¿¡æ¯
        self.visualize_mutual_information(cases, mutual_infos)
        
        return mutual_infos
    
    def visualize_mutual_information(self, cases, mutual_infos):
        """å¯è§†åŒ–äº’ä¿¡æ¯"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        for i, ((name, x, y), mi) in enumerate(zip(cases, mutual_infos)):
            # æ•£ç‚¹å›¾
            ax1 = axes[0, i]
            ax1.scatter(x + np.random.normal(0, 0.05, len(x)), 
                       y + np.random.normal(0, 0.05, len(y)), 
                       alpha=0.6, s=10)
            ax1.set_xlabel('X')
            ax1.set_ylabel('Y')
            ax1.set_title(f'{name}\nI(X;Y) = {mi:.3f}')
            ax1.grid(True, alpha=0.3)
            
            # è”åˆåˆ†å¸ƒçƒ­å›¾
            ax2 = axes[1, i]
            
            # è®¡ç®—è”åˆæ¦‚ç‡
            joint_counts = np.zeros((2, 2))
            for xi, yi in zip(x, y):
                joint_counts[int(xi), int(yi)] += 1
            joint_prob = joint_counts / len(x)
            
            im = ax2.imshow(joint_prob, cmap='Blues', interpolation='nearest')
            ax2.set_xticks([0, 1])
            ax2.set_yticks([0, 1])
            ax2.set_xlabel('Y')
            ax2.set_ylabel('X')
            ax2.set_title(f'è”åˆåˆ†å¸ƒ P(X,Y)')
            
            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for xi in range(2):
                for yi in range(2):
                    ax2.text(yi, xi, f'{joint_prob[xi, yi]:.2f}', 
                            ha='center', va='center', color='red')
        
        plt.tight_layout()
        plt.show()
    
    def feature_selection_example(self):
        """ç‰¹å¾é€‰æ‹©ç¤ºä¾‹"""
        print("\n=== åŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é€‰æ‹© ===")
        
        # ç”Ÿæˆæ•°æ®
        X, y = make_classification(n_samples=1000, n_features=10, 
                                 n_informative=5, n_redundant=2, 
                                 random_state=42)
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡çš„äº’ä¿¡æ¯
        mi_scores = mutual_info_classif(X, y, random_state=42)
        
        # åˆ›å»ºç‰¹å¾åç§°
        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
        
        # æŒ‰äº’ä¿¡æ¯æ’åº
        mi_df = pd.DataFrame({
            'Feature': feature_names,
            'Mutual_Information': mi_scores
        }).sort_values('Mutual_Information', ascending=False)
        
        print("ç‰¹å¾çš„äº’ä¿¡æ¯å¾—åˆ†:")
        print(mi_df)
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(10, 6))
        bars = plt.bar(mi_df['Feature'], mi_df['Mutual_Information'], alpha=0.7)
        plt.xlabel('ç‰¹å¾')
        plt.ylabel('äº’ä¿¡æ¯å¾—åˆ†')
        plt.title('åŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é‡è¦æ€§')
        plt.xticks(rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, mi_df['Mutual_Information']):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{score:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        return mi_df

class KLDivergence:
    """KLæ•£åº¦"""
    
    def __init__(self):
        pass
    
    def kl_divergence_concept(self):
        """KLæ•£åº¦æ¦‚å¿µ"""
        print("=== KLæ•£åº¦ (Kullback-Leibler Divergence) ===")
        
        print("å®šä¹‰:")
        print("D_KL(P||Q) = Î£áµ¢ P(i) log[P(i)/Q(i)]")
        print("         = E_P[log P(X) - log Q(X)]")
        print()
        
        print("æ€§è´¨:")
        print("- D_KL(P||Q) â‰¥ 0ï¼Œç­‰å·æˆç«‹å½“ä¸”ä»…å½“P=Q")
        print("- éå¯¹ç§°ï¼šD_KL(P||Q) â‰  D_KL(Q||P)")
        print("- ä¸æ»¡è¶³ä¸‰è§’ä¸ç­‰å¼ï¼Œä¸æ˜¯çœŸæ­£çš„è·ç¦»")
        print()
        
        print("æœºå™¨å­¦ä¹ åº”ç”¨:")
        print("- æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰")
        print("- å˜åˆ†æ¨æ–­")
        print("- ç”Ÿæˆæ¨¡å‹è®­ç»ƒ")
        print("- æ¨¡å‹å‹ç¼©")
        
        # KLæ•£åº¦è®¡ç®—ç¤ºä¾‹
        self.demonstrate_kl_divergence()
        
        return self.cross_entropy_connection()
    
    def demonstrate_kl_divergence(self):
        """æ¼”ç¤ºKLæ•£åº¦è®¡ç®—"""
        print("\n=== KLæ•£åº¦è®¡ç®—ç¤ºä¾‹ ===")
        
        # ä¸åŒåˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦
        x = np.linspace(0.01, 0.99, 100)
        
        # åˆ†å¸ƒPï¼ˆçœŸå®åˆ†å¸ƒï¼‰
        p = stats.beta.pdf(x, 2, 5)
        p = p / np.sum(p)  # å½’ä¸€åŒ–
        
        # ä¸åŒçš„è¿‘ä¼¼åˆ†å¸ƒQ
        q1 = stats.beta.pdf(x, 2, 5)  # ç›¸åŒåˆ†å¸ƒ
        q1 = q1 / np.sum(q1)
        
        q2 = stats.beta.pdf(x, 3, 4)  # ç›¸ä¼¼åˆ†å¸ƒ
        q2 = q2 / np.sum(q2)
        
        q3 = np.ones_like(x)  # å‡åŒ€åˆ†å¸ƒ
        q3 = q3 / np.sum(q3)
        
        # è®¡ç®—KLæ•£åº¦
        def kl_divergence(p, q):
            # é¿å…log(0)
            epsilon = 1e-10
            q = np.maximum(q, epsilon)
            return np.sum(p * np.log(p / q))
        
        kl1 = kl_divergence(p, q1)
        kl2 = kl_divergence(p, q2)
        kl3 = kl_divergence(p, q3)
        
        print(f"D_KL(P||P) = {kl1:.4f}")
        print(f"D_KL(P||Q_similar) = {kl2:.4f}")
        print(f"D_KL(P||Q_uniform) = {kl3:.4f}")
        
        # å¯è§†åŒ–KLæ•£åº¦
        self.visualize_kl_divergence(x, p, [q1, q2, q3], [kl1, kl2, kl3])
        
        return [kl1, kl2, kl3]
    
    def visualize_kl_divergence(self, x, p, q_list, kl_list):
        """å¯è§†åŒ–KLæ•£åº¦"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # åŸå§‹åˆ†å¸ƒ
        axes[0, 0].plot(x, p, 'b-', linewidth=2, label='P (çœŸå®åˆ†å¸ƒ)')
        axes[0, 0].fill_between(x, p, alpha=0.3, color='blue')
        axes[0, 0].set_title('çœŸå®åˆ†å¸ƒ P')
        axes[0, 0].set_xlabel('x')
        axes[0, 0].set_ylabel('æ¦‚ç‡å¯†åº¦')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # ä¸åŒè¿‘ä¼¼åˆ†å¸ƒ
        labels = ['ç›¸åŒåˆ†å¸ƒ', 'ç›¸ä¼¼åˆ†å¸ƒ', 'å‡åŒ€åˆ†å¸ƒ']
        colors = ['green', 'orange', 'red']
        
        for i, (q, kl, label, color) in enumerate(zip(q_list, kl_list, labels, colors)):
            ax = axes[0, 1] if i == 0 else axes[1, i-1]
            
            ax.plot(x, p, 'b-', linewidth=2, label='P', alpha=0.7)
            ax.plot(x, q, color=color, linewidth=2, label=f'Q ({label})')
            ax.fill_between(x, p, alpha=0.3, color='blue')
            ax.fill_between(x, q, alpha=0.3, color=color)
            
            ax.set_title(f'{label}\nD_KL(P||Q) = {kl:.4f}')
            ax.set_xlabel('x')
            ax.set_ylabel('æ¦‚ç‡å¯†åº¦')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def cross_entropy_connection(self):
        """äº¤å‰ç†µè¿æ¥"""
        print("\n=== KLæ•£åº¦ä¸äº¤å‰ç†µçš„å…³ç³» ===")
        
        print("äº¤å‰ç†µå®šä¹‰:")
        print("H(P,Q) = -Î£áµ¢ P(i) log Q(i)")
        print()
        print("å…³ç³»:")
        print("D_KL(P||Q) = H(P,Q) - H(P)")
        print("å› æ­¤ï¼šH(P,Q) = H(P) + D_KL(P||Q)")
        print()
        print("åœ¨æœºå™¨å­¦ä¹ ä¸­:")
        print("- P: çœŸå®æ ‡ç­¾åˆ†å¸ƒ")
        print("- Q: æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ")
        print("- æœ€å°åŒ–äº¤å‰ç†µ â‰¡ æœ€å°åŒ–KLæ•£åº¦ï¼ˆå› ä¸ºH(P)æ˜¯å¸¸æ•°ï¼‰")
        
        # åˆ†ç±»é—®é¢˜ä¸­çš„äº¤å‰ç†µç¤ºä¾‹
        self.classification_cross_entropy_example()
        
        return self.js_divergence()
    
    def classification_cross_entropy_example(self):
        """åˆ†ç±»é—®é¢˜ä¸­çš„äº¤å‰ç†µç¤ºä¾‹"""
        print("\n=== åˆ†ç±»äº¤å‰ç†µç¤ºä¾‹ ===")
        
        # çœŸå®æ ‡ç­¾ï¼ˆone-hotç¼–ç ï¼‰
        y_true = np.array([
            [1, 0, 0],  # ç±»åˆ«0
            [0, 1, 0],  # ç±»åˆ«1
            [0, 0, 1],  # ç±»åˆ«2
        ])
        
        # ä¸åŒè´¨é‡çš„é¢„æµ‹
        predictions = {
            "å®Œç¾é¢„æµ‹": np.array([
                [0.99, 0.005, 0.005],
                [0.005, 0.99, 0.005],
                [0.005, 0.005, 0.99]
            ]),
            "å¥½é¢„æµ‹": np.array([
                [0.8, 0.15, 0.05],
                [0.1, 0.8, 0.1],
                [0.05, 0.15, 0.8]
            ]),
            "å·®é¢„æµ‹": np.array([
                [0.4, 0.35, 0.25],
                [0.3, 0.4, 0.3],
                [0.25, 0.35, 0.4]
            ])
        }
        
        # è®¡ç®—äº¤å‰ç†µ
        def cross_entropy(y_true, y_pred):
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
        
        results = {}
        for name, y_pred in predictions.items():
            ce = cross_entropy(y_true, y_pred)
            results[name] = ce
            print(f"{name}: äº¤å‰ç†µ = {ce:.4f}")
        
        # å¯è§†åŒ–é¢„æµ‹è´¨é‡
        self.visualize_prediction_quality(y_true, predictions, results)
        
        return results
    
    def visualize_prediction_quality(self, y_true, predictions, results):
        """å¯è§†åŒ–é¢„æµ‹è´¨é‡"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # çœŸå®æ ‡ç­¾
        axes[0, 0].imshow(y_true, cmap='Blues', interpolation='nearest')
        axes[0, 0].set_title('çœŸå®æ ‡ç­¾')
        axes[0, 0].set_xlabel('ç±»åˆ«')
        axes[0, 0].set_ylabel('æ ·æœ¬')
        
        # é¢„æµ‹ç»“æœ
        for i, (name, y_pred) in enumerate(predictions.items()):
            ax = axes[0, 1] if i == 0 else axes[1, i-1]
            
            im = ax.imshow(y_pred, cmap='Reds', interpolation='nearest')
            ax.set_title(f'{name}\näº¤å‰ç†µ: {results[name]:.4f}')
            ax.set_xlabel('ç±»åˆ«')
            ax.set_ylabel('æ ·æœ¬')
            
            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for row in range(y_pred.shape[0]):
                for col in range(y_pred.shape[1]):
                    ax.text(col, row, f'{y_pred[row, col]:.2f}', 
                           ha='center', va='center', color='white')
        
        plt.tight_layout()
        plt.show()
    
    def js_divergence(self):
        """JSæ•£åº¦"""
        print("\n=== JSæ•£åº¦ (Jensen-Shannon Divergence) ===")
        
        print("å®šä¹‰:")
        print("JS(P,Q) = (1/2)D_KL(P||M) + (1/2)D_KL(Q||M)")
        print("å…¶ä¸­ M = (1/2)(P + Q)")
        print()
        print("æ€§è´¨:")
        print("- å¯¹ç§°ï¼šJS(P,Q) = JS(Q,P)")
        print("- æœ‰ç•Œï¼š0 â‰¤ JS(P,Q) â‰¤ 1")
        print("- å¹³æ–¹æ ¹âˆšJS(P,Q)æ˜¯çœŸæ­£çš„è·ç¦»åº¦é‡")
        print()
        print("åº”ç”¨:")
        print("- GANä¸­çš„æŸå¤±å‡½æ•°")
        print("- æ¨¡å‹æ¯”è¾ƒ")
        print("- èšç±»åˆ†æ")
        
        # JSæ•£åº¦è®¡ç®—ç¤ºä¾‹
        return self.demonstrate_js_divergence()
    
    def demonstrate_js_divergence(self):
        """æ¼”ç¤ºJSæ•£åº¦"""
        print("\n=== JSæ•£åº¦è®¡ç®—ç¤ºä¾‹ ===")
        
        # ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ
        p = np.array([0.5, 0.3, 0.2])
        q1 = np.array([0.5, 0.3, 0.2])  # ç›¸åŒåˆ†å¸ƒ
        q2 = np.array([0.3, 0.4, 0.3])  # ä¸åŒåˆ†å¸ƒ
        
        def js_divergence(p, q):
            """è®¡ç®—JSæ•£åº¦"""
            def kl_div(p, q):
                epsilon = 1e-10
                return np.sum(p * np.log((p + epsilon) / (q + epsilon)))
            
            m = 0.5 * (p + q)
            return 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)
        
        js1 = js_divergence(p, q1)
        js2 = js_divergence(p, q2)
        
        print(f"JS(P,P) = {js1:.4f}")
        print(f"JS(P,Q) = {js2:.4f}")
        
        # å¯è§†åŒ–JSæ•£åº¦
        distributions = [p, q1, q2]
        names = ['P', 'Q (ç›¸åŒ)', 'Q (ä¸åŒ)']
        
        plt.figure(figsize=(12, 4))
        
        for i, (dist, name) in enumerate(zip(distributions, names)):
            plt.subplot(1, 3, i+1)
            plt.bar(['Xâ‚', 'Xâ‚‚', 'Xâ‚ƒ'], dist, alpha=0.7)
            plt.title(name)
            plt.ylabel('æ¦‚ç‡')
            plt.ylim(0, 0.6)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, val in enumerate(dist):
                plt.text(j, val + 0.02, f'{val:.1f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        return js1, js2

def comprehensive_information_theory_summary():
    """ä¿¡æ¯ç†è®ºç»¼åˆæ€»ç»“"""
    print("=== ä¿¡æ¯ç†è®ºç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "åŸºæœ¬æ¦‚å¿µ": {
            "ä¿¡æ¯é‡": "I(x) = -logâ‚‚ P(x)",
            "ç†µ": "H(X) = E[-logâ‚‚ P(X)]",
            "æ¡ä»¶ç†µ": "H(Y|X) = E[H(Y|X=x)]",
            "è”åˆç†µ": "H(X,Y) = E[-logâ‚‚ P(X,Y)]"
        },
        
        "ä¿¡æ¯åº¦é‡": {
            "äº’ä¿¡æ¯": "I(X;Y) = H(X) - H(X|Y)",
            "KLæ•£åº¦": "D_KL(P||Q) = E_P[log P/Q]",
            "JSæ•£åº¦": "JS(P,Q) = (D_KL(P||M) + D_KL(Q||M))/2",
            "äº¤å‰ç†µ": "H(P,Q) = -E_P[log Q]"
        },
        
        "é‡è¦æ€§è´¨": {
            "é“¾å¼æ³•åˆ™": "H(X,Y) = H(X) + H(Y|X)",
            "æ•°æ®å¤„ç†ä¸ç­‰å¼": "I(X;Z) â‰¤ I(X;Y) if X-Y-Z",
            "Fanoä¸ç­‰å¼": "å…³è”é”™è¯¯æ¦‚ç‡ä¸æ¡ä»¶ç†µ",
            "ä¿¡æ¯ç“¶é¢ˆ": "æœ€å¤§åŒ–ç›¸å…³ä¿¡æ¯ï¼Œæœ€å°åŒ–æ— å…³ä¿¡æ¯"
        },
        
        "MLåº”ç”¨": {
            "æŸå¤±å‡½æ•°": "äº¤å‰ç†µæŸå¤±ã€KLæ•£åº¦æŸå¤±",
            "ç‰¹å¾é€‰æ‹©": "åŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é‡è¦æ€§",
            "æ¨¡å‹å‹ç¼©": "çŸ¥è¯†è’¸é¦ä¸­çš„KLæ•£åº¦",
            "ç”Ÿæˆæ¨¡å‹": "VAEä¸­çš„ELBOã€GANä¸­çš„JSæ•£åº¦",
            "å¼ºåŒ–å­¦ä¹ ": "ç­–ç•¥æ¢¯åº¦ä¸­çš„ç†µæ­£åˆ™åŒ–"
        },
        
        "å®è·µæŠ€å·§": {
            "æ•°å€¼ç¨³å®šæ€§": "log-sum-expæŠ€å·§",
            "ç¦»æ•£åŒ–": "è¿ç»­å˜é‡çš„ä¿¡æ¯ä¼°è®¡",
            "é‡‡æ ·ä¼°è®¡": "å¤§æ•°æ®é›†çš„ä¿¡æ¯åº¦é‡",
            "æ­£åˆ™åŒ–": "ç†µä½œä¸ºæ­£åˆ™åŒ–é¡¹"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("ä¿¡æ¯ç†è®ºåŸºç¡€æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Cover & Thomas (2006): "Elements of Information Theory"
- MacKay (2003): "Information Theory, Inference and Learning Algorithms"
- Shannon (1948): "A Mathematical Theory of Communication"
- Kullback & Leibler (1951): "On Information and Sufficiency"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [æ¦‚ç‡ç»Ÿè®¡ç†è®º](probability_statistics_theory.md) - æ¦‚ç‡è®ºåŸºç¡€
- [ä¼˜åŒ–ç®—æ³•ç†è®º](optimization_theory.md) - ä¼˜åŒ–æ–¹æ³•
- [è´å¶æ–¯æœºå™¨å­¦ä¹ ](bayesian_ml_theory.md) - è´å¶æ–¯æ–¹æ³•