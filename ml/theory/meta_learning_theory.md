# å…ƒå­¦ä¹ ç†è®ºï¼šå­¦ä¼šå­¦ä¹ çš„è‰ºæœ¯ ğŸ§ 

æ·±å…¥ç†è§£å…ƒå­¦ä¹ çš„æ ¸å¿ƒç†è®ºï¼Œä»å°‘æ ·æœ¬å­¦ä¹ åˆ°å¿«é€Ÿé€‚åº”ç®—æ³•ã€‚

## 1. å…ƒå­¦ä¹ åŸºç¡€æ¦‚å¿µ ğŸ¯

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import make_classification, make_regression
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import accuracy_score, mean_squared_error
import pandas as pd
from typing import List, Tuple, Dict, Callable
import warnings
warnings.filterwarnings('ignore')

class MetaLearningBasics:
    """å…ƒå­¦ä¹ åŸºç¡€æ¦‚å¿µ"""
    
    def __init__(self):
        self.concepts = {}
    
    def meta_learning_definition(self):
        """å…ƒå­¦ä¹ å®šä¹‰"""
        print("=== å…ƒå­¦ä¹ å®šä¹‰ ===")
        
        print("æ ¸å¿ƒæ€æƒ³:")
        print("- å­¦ä¼šå­¦ä¹ ï¼šä»å¤šä¸ªç›¸å…³ä»»åŠ¡ä¸­å­¦ä¹ å¦‚ä½•å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡")
        print("- Few-shot Learningï¼šç”¨å¾ˆå°‘çš„æ ·æœ¬å¿«é€Ÿå­¦ä¹ æ–°ä»»åŠ¡")
        print("- è·¨ä»»åŠ¡çŸ¥è¯†è¿ç§»ï¼šåˆ©ç”¨ä¹‹å‰ä»»åŠ¡çš„ç»éªŒ")
        print()
        
        print("æ­£å¼å®šä¹‰:")
        print("ç»™å®šä»»åŠ¡åˆ†å¸ƒ p(T)ï¼Œå…ƒå­¦ä¹ ç›®æ ‡æ˜¯å­¦ä¹ ç®—æ³• Aï¼Œä½¿å¾—:")
        print("E_{T~p(T)} [L_T(A(D_T^train))] æœ€å°")
        print("å…¶ä¸­:")
        print("- T: ä»»åŠ¡")
        print("- D_T^train: ä»»åŠ¡Tçš„è®­ç»ƒæ•°æ®ï¼ˆæ”¯æŒé›†ï¼‰")
        print("- L_T: ä»»åŠ¡Tçš„æŸå¤±å‡½æ•°")
        print("- A: å­¦ä¹ ç®—æ³•")
        print()
        
        print("å…³é”®ç»„ä»¶:")
        components = {
            "ä»»åŠ¡åˆ†å¸ƒ": {
                "å®šä¹‰": "æ‰€æœ‰å¯èƒ½ä»»åŠ¡çš„åˆ†å¸ƒ",
                "ä¾‹å­": "ä¸åŒç±»åˆ«çš„å›¾åƒåˆ†ç±»ä»»åŠ¡",
                "é‡è¦æ€§": "å†³å®šå…ƒå­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›",
                "å‡è®¾": "æ–°ä»»åŠ¡æ¥è‡ªåŒä¸€åˆ†å¸ƒ"
            },
            "æ”¯æŒé›†": {
                "å®šä¹‰": "æ¯ä¸ªä»»åŠ¡çš„å°‘é‡è®­ç»ƒæ ·æœ¬",
                "å¤§å°": "é€šå¸¸1-5ä¸ªæ ·æœ¬perç±»",
                "ä½œç”¨": "å¿«é€Ÿé€‚åº”çš„åŸºç¡€",
                "æŒ‘æˆ˜": "æ ·æœ¬æå°‘ï¼Œæ˜“è¿‡æ‹Ÿåˆ"
            },
            "æŸ¥è¯¢é›†": {
                "å®šä¹‰": "ç”¨äºè¯„ä¼°é€‚åº”æ•ˆæœçš„æµ‹è¯•æ ·æœ¬",
                "ç”¨é€”": "è¡¡é‡å¿«é€Ÿé€‚åº”çš„æ€§èƒ½",
                "åˆ†ç¦»": "ä¸æ”¯æŒé›†ä¸¥æ ¼åˆ†ç¦»",
                "é‡è¦æ€§": "è¯„ä¼°å…ƒå­¦ä¹ ç®—æ³•ä¼˜åŠ£"
            },
            "å…ƒçŸ¥è¯†": {
                "å®šä¹‰": "ä»å¤šä»»åŠ¡ä¸­æå–çš„å…±åŒæ¨¡å¼",
                "å½¢å¼": "ç½‘ç»œåˆå§‹åŒ–ã€ä¼˜åŒ–å™¨ã€ç‰¹å¾è¡¨ç¤º",
                "è·å¾—": "åœ¨å…ƒè®­ç»ƒé˜¶æ®µå­¦ä¹ ",
                "åº”ç”¨": "æŒ‡å¯¼æ–°ä»»åŠ¡çš„å¿«é€Ÿå­¦ä¹ "
            }
        }
        
        for comp, details in components.items():
            print(f"{comp}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # å¯è§†åŒ–å…ƒå­¦ä¹ æ¦‚å¿µ
        self.visualize_meta_learning_concept()
        
        return components
    
    def visualize_meta_learning_concept(self):
        """å¯è§†åŒ–å…ƒå­¦ä¹ æ¦‚å¿µ"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. ä¼ ç»Ÿå­¦ä¹ vså…ƒå­¦ä¹ 
        ax1 = axes[0, 0]
        
        # ä¼ ç»Ÿå­¦ä¹ æ›²çº¿
        epochs = np.arange(0, 100)
        traditional_curve = 1 - np.exp(-epochs / 30) + np.random.normal(0, 0.02, len(epochs))
        traditional_curve = np.clip(traditional_curve, 0, 1)
        
        # å…ƒå­¦ä¹ æ›²çº¿ï¼ˆå¿«é€Ÿé€‚åº”ï¼‰
        meta_curve = 1 - np.exp(-epochs / 5) + np.random.normal(0, 0.02, len(epochs))
        meta_curve = np.clip(meta_curve, 0, 1)
        
        ax1.plot(epochs, traditional_curve, label='ä¼ ç»Ÿå­¦ä¹ ', linewidth=2, color='blue')
        ax1.plot(epochs[:20], meta_curve[:20], label='å…ƒå­¦ä¹ ï¼ˆå¿«é€Ÿé€‚åº”ï¼‰', linewidth=2, color='red')
        ax1.axvline(x=5, color='gray', linestyle='--', alpha=0.7, label='å°‘æ ·æœ¬åŒºåŸŸ')
        
        ax1.set_xlabel('è®­ç»ƒæ ·æœ¬æ•° / è¿­ä»£æ¬¡æ•°')
        ax1.set_ylabel('æ€§èƒ½')
        ax1.set_title('ä¼ ç»Ÿå­¦ä¹  vs å…ƒå­¦ä¹ ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å…ƒå­¦ä¹ çš„ä»»åŠ¡åˆ†å¸ƒ
        ax2 = axes[0, 1]
        
        # æ¨¡æ‹Ÿä¸åŒä»»åŠ¡åœ¨ç‰¹å¾ç©ºé—´çš„åˆ†å¸ƒ
        np.random.seed(42)
        n_tasks = 5
        colors = plt.cm.Set1(np.linspace(0, 1, n_tasks))
        
        for i in range(n_tasks):
            # æ¯ä¸ªä»»åŠ¡çš„æ•°æ®ç‚¹
            center = np.random.uniform(-2, 2, 2)
            X_task = np.random.multivariate_normal(center, [[0.3, 0], [0, 0.3]], 50)
            ax2.scatter(X_task[:, 0], X_task[:, 1], c=[colors[i]], alpha=0.6, 
                       label=f'ä»»åŠ¡ {i+1}', s=30)
        
        ax2.set_xlabel('ç‰¹å¾ç»´åº¦ 1')
        ax2.set_ylabel('ç‰¹å¾ç»´åº¦ 2')
        ax2.set_title('å…ƒå­¦ä¹ ä¸­çš„ä»»åŠ¡åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æ”¯æŒé›†ä¸æŸ¥è¯¢é›†
        ax3 = axes[1, 0]
        
        # æ¨¡æ‹Ÿä¸€ä¸ªfew-shotä»»åŠ¡
        np.random.seed(123)
        n_classes = 3
        n_support = 2
        n_query = 5
        
        class_colors = ['red', 'blue', 'green']
        
        for i, color in enumerate(class_colors):
            # æ”¯æŒé›†
            support_x = np.random.normal(i*2, 0.3, n_support)
            support_y = np.random.normal(i*2, 0.3, n_support)
            ax3.scatter(support_x, support_y, c=color, marker='s', s=100, 
                       label=f'ç±»åˆ«{i+1}æ”¯æŒé›†', edgecolors='black', linewidth=2)
            
            # æŸ¥è¯¢é›†
            query_x = np.random.normal(i*2, 0.3, n_query)
            query_y = np.random.normal(i*2, 0.3, n_query)
            ax3.scatter(query_x, query_y, c=color, marker='o', s=60, 
                       alpha=0.6, label=f'ç±»åˆ«{i+1}æŸ¥è¯¢é›†')
        
        ax3.set_xlabel('ç‰¹å¾ç»´åº¦ 1')
        ax3.set_ylabel('ç‰¹å¾ç»´åº¦ 2')
        ax3.set_title('æ”¯æŒé›† vs æŸ¥è¯¢é›†ï¼ˆ3-way 2-shotï¼‰')
        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax3.grid(True, alpha=0.3)
        
        # 4. å…ƒå­¦ä¹ è®­ç»ƒæµç¨‹
        ax4 = axes[1, 1]
        ax4.text(0.1, 0.9, "å…ƒå­¦ä¹ è®­ç»ƒæµç¨‹", fontsize=14, weight='bold', transform=ax4.transAxes)
        
        steps = [
            "1. ä»ä»»åŠ¡åˆ†å¸ƒé‡‡æ ·ä»»åŠ¡",
            "2. å°†ä»»åŠ¡åˆ†ä¸ºæ”¯æŒé›†å’ŒæŸ¥è¯¢é›†", 
            "3. åœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚åº”",
            "4. åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°æ€§èƒ½",
            "5. æ›´æ–°å…ƒçŸ¥è¯†ï¼ˆå…ƒå‚æ•°ï¼‰",
            "6. é‡å¤1-5ç›´åˆ°æ”¶æ•›"
        ]
        
        for i, step in enumerate(steps):
            ax4.text(0.1, 0.8 - i*0.1, step, fontsize=11, transform=ax4.transAxes)
        
        ax4.axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def meta_learning_taxonomy(self):
        """å…ƒå­¦ä¹ åˆ†ç±»æ³•"""
        print("=== å…ƒå­¦ä¹ åˆ†ç±»æ³• ===")
        
        print("æŒ‰ç…§å­¦ä¹ å†…å®¹åˆ†ç±»:")
        taxonomy = {
            "å…ƒå­¦ä¹ ä»€ä¹ˆ": {
                "åˆå§‹åŒ–": {
                    "å­¦ä¹ å†…å®¹": "è‰¯å¥½çš„å‚æ•°åˆå§‹åŒ–",
                    "ä»£è¡¨æ–¹æ³•": "MAML, Reptile",
                    "ä¼˜åŠ¿": "ç®€å•æœ‰æ•ˆï¼Œé€šç”¨æ€§å¼º",
                    "ç¼ºç‚¹": "å¯èƒ½å­˜åœ¨å±€éƒ¨æœ€ä¼˜"
                },
                "ä¼˜åŒ–å™¨": {
                    "å­¦ä¹ å†…å®¹": "å­¦ä¹ ç®—æ³•æœ¬èº«",
                    "ä»£è¡¨æ–¹æ³•": "Learning to Learn by GD",
                    "ä¼˜åŠ¿": "è‡ªé€‚åº”å­¦ä¹ ç‡å’Œæ–¹å‘",
                    "ç¼ºç‚¹": "è®¡ç®—å¤æ‚åº¦é«˜"
                },
                "ç½‘ç»œç»“æ„": {
                    "å­¦ä¹ å†…å®¹": "é€‚åº”æ€§ç½‘ç»œæ¶æ„",
                    "ä»£è¡¨æ–¹æ³•": "Meta-Networks, Dynamic Networks",
                    "ä¼˜åŠ¿": "ç»“æ„é€‚åº”ä»»åŠ¡",
                    "ç¼ºç‚¹": "æœç´¢ç©ºé—´å·¨å¤§"
                },
                "æŸå¤±å‡½æ•°": {
                    "å­¦ä¹ å†…å®¹": "ä»»åŠ¡ç‰¹å®šæŸå¤±",
                    "ä»£è¡¨æ–¹æ³•": "Meta-Loss Networks",
                    "ä¼˜åŠ¿": "ä»»åŠ¡å®šåˆ¶åŒ–",
                    "ç¼ºç‚¹": "ç†è®ºåˆ†æå›°éš¾"
                }
            },
            
            "å…ƒå­¦ä¹ æ–¹æ³•": {
                "åŸºäºåº¦é‡": {
                    "æ ¸å¿ƒæ€æƒ³": "å­¦ä¹ ç›¸ä¼¼æ€§åº¦é‡",
                    "ä»£è¡¨æ–¹æ³•": "Siamese Networks, Prototypical Networks",
                    "é€‚ç”¨åœºæ™¯": "åˆ†ç±»ä»»åŠ¡",
                    "ä¼˜åŠ¿": "ç›´è§‚æ˜“ç†è§£"
                },
                "åŸºäºæ¨¡å‹": {
                    "æ ¸å¿ƒæ€æƒ³": "å­¦ä¹ å¿«é€Ÿé€‚åº”çš„æ¨¡å‹",
                    "ä»£è¡¨æ–¹æ³•": "MAML, FOMAML",
                    "é€‚ç”¨åœºæ™¯": "é€šç”¨ä»»åŠ¡",
                    "ä¼˜åŠ¿": "ç†è®ºåŸºç¡€å¼º"
                },
                "åŸºäºä¼˜åŒ–": {
                    "æ ¸å¿ƒæ€æƒ³": "å­¦ä¹ ä¼˜åŒ–è¿‡ç¨‹",
                    "ä»£è¡¨æ–¹æ³•": "LSTM Meta-Learner",
                    "é€‚ç”¨åœºæ™¯": "ä¼˜åŒ–å›°éš¾ä»»åŠ¡",
                    "ä¼˜åŠ¿": "é€‚åº”æ€§å¼º"
                }
            }
        }
        
        for category, subcategories in taxonomy.items():
            print(f"\n{category}:")
            for subcat, details in subcategories.items():
                print(f"  {subcat}:")
                for key, value in details.items():
                    print(f"    {key}: {value}")
                print()
        
        # å¯è§†åŒ–åˆ†ç±»æ³•
        self.visualize_meta_learning_taxonomy()
        
        return taxonomy
    
    def visualize_meta_learning_taxonomy(self):
        """å¯è§†åŒ–å…ƒå­¦ä¹ åˆ†ç±»æ³•"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. æ–¹æ³•æ¯”è¾ƒé›·è¾¾å›¾
        methods = ['MAML', 'Prototypical\nNetworks', 'Meta-LSTM', 'Matching\nNetworks']
        criteria = ['ç†è®ºåŸºç¡€', 'å®ç°éš¾åº¦', 'é€šç”¨æ€§', 'æ€§èƒ½', 'è®¡ç®—æ•ˆç‡']
        
        # ä¸åŒæ–¹æ³•åœ¨å„æ ‡å‡†ä¸Šçš„è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰
        scores = {
            'MAML': [5, 4, 5, 4, 3],
            'Prototypical Networks': [3, 2, 3, 4, 5],
            'Meta-LSTM': [3, 5, 4, 3, 2],
            'Matching Networks': [3, 3, 3, 3, 4]
        }
        
        ax1 = plt.subplot(2, 2, 1, projection='polar')
        
        angles = np.linspace(0, 2*np.pi, len(criteria), endpoint=False).tolist()
        angles += angles[:1]
        
        colors = ['blue', 'red', 'green', 'orange']
        for i, (method, score) in enumerate(scores.items()):
            score += score[:1]  # é—­åˆå›¾å½¢
            ax1.plot(angles, score, 'o-', linewidth=2, label=method, color=colors[i])
            ax1.fill(angles, score, alpha=0.1, color=colors[i])
        
        ax1.set_xticks(angles[:-1])
        ax1.set_xticklabels(criteria)
        ax1.set_ylim(0, 5)
        ax1.set_title('å…ƒå­¦ä¹ æ–¹æ³•æ¯”è¾ƒ')
        ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1))
        
        # 2. é€‚ç”¨åœºæ™¯åˆ†å¸ƒ
        ax2 = axes[0, 1]
        
        scenarios = ['å›¾åƒåˆ†ç±»', 'æ–‡æœ¬åˆ†ç±»', 'å›å½’', 'å¼ºåŒ–å­¦ä¹ ', 'è¯­éŸ³è¯†åˆ«']
        maml_scores = [5, 4, 5, 4, 3]
        proto_scores = [5, 3, 2, 2, 4]
        
        x = np.arange(len(scenarios))
        width = 0.35
        
        bars1 = ax2.bar(x - width/2, maml_scores, width, label='MAML', alpha=0.7, color='blue')
        bars2 = ax2.bar(x + width/2, proto_scores, width, label='Prototypical', alpha=0.7, color='red')
        
        ax2.set_xlabel('åº”ç”¨åœºæ™¯')
        ax2.set_ylabel('é€‚ç”¨æ€§è¯„åˆ†')
        ax2.set_title('ä¸åŒåœºæ™¯ä¸‹çš„æ–¹æ³•é€‚ç”¨æ€§')
        ax2.set_xticks(x)
        ax2.set_xticklabels(scenarios, rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å­¦ä¹ æ›²çº¿æ¯”è¾ƒ
        ax3 = axes[1, 0]
        
        tasks = np.arange(1, 101)
        
        # æ¨¡æ‹Ÿä¸åŒæ–¹æ³•çš„å…ƒå­¦ä¹ æ›²çº¿
        maml_curve = 1 - 0.8 * np.exp(-tasks / 30) + np.random.normal(0, 0.02, len(tasks))
        proto_curve = 1 - 0.7 * np.exp(-tasks / 20) + np.random.normal(0, 0.02, len(tasks))
        scratch_curve = 1 - 0.6 * np.exp(-tasks / 50) + np.random.normal(0, 0.02, len(tasks))
        
        ax3.plot(tasks, maml_curve, label='MAML', linewidth=2, color='blue')
        ax3.plot(tasks, proto_curve, label='Prototypical Networks', linewidth=2, color='red')
        ax3.plot(tasks, scratch_curve, label='From Scratch', linewidth=2, color='gray', linestyle='--')
        
        ax3.set_xlabel('å…ƒè®­ç»ƒä»»åŠ¡æ•°')
        ax3.set_ylabel('æ–°ä»»åŠ¡é€‚åº”æ€§èƒ½')
        ax3.set_title('å…ƒå­¦ä¹ æ–¹æ³•çš„å­¦ä¹ æ›²çº¿')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. è®¡ç®—å¤æ‚åº¦æ¯”è¾ƒ
        ax4 = axes[1, 1]
        
        methods_comp = ['MAML', 'FOMAML', 'Prototypical', 'Matching', 'Meta-LSTM']
        training_time = [100, 40, 20, 25, 80]
        memory_usage = [100, 50, 30, 35, 70]
        
        scatter = ax4.scatter(training_time, memory_usage, s=200, alpha=0.7, 
                             c=range(len(methods_comp)), cmap='viridis')
        
        for i, method in enumerate(methods_comp):
            ax4.annotate(method, (training_time[i], memory_usage[i]), 
                        xytext=(5, 5), textcoords='offset points')
        
        ax4.set_xlabel('è®­ç»ƒæ—¶é—´ (ç›¸å¯¹)')
        ax4.set_ylabel('å†…å­˜ä½¿ç”¨ (ç›¸å¯¹)')
        ax4.set_title('è®¡ç®—å¤æ‚åº¦æ¯”è¾ƒ')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

class ModelAgnosticMetaLearning:
    """æ¨¡å‹æ— å…³å…ƒå­¦ä¹ (MAML)"""
    
    def __init__(self):
        pass
    
    def maml_theory(self):
        """MAMLç†è®º"""
        print("=== MAML (Model-Agnostic Meta-Learning) ç†è®º ===")
        
        print("æ ¸å¿ƒæ€æƒ³:")
        print("- å­¦ä¹ ä¸€ä¸ªå¥½çš„å‚æ•°åˆå§‹åŒ–")
        print("- ä½¿å¾—ä»è¯¥åˆå§‹åŒ–å¼€å§‹ï¼Œå°‘æ•°å‡ æ­¥æ¢¯åº¦ä¸‹é™å°±èƒ½é€‚åº”æ–°ä»»åŠ¡")
        print("- æ¨¡å‹æ— å…³ï¼šé€‚ç”¨äºä»»ä½•åŸºäºæ¢¯åº¦çš„å­¦ä¹ ç®—æ³•")
        print()
        
        print("ç®—æ³•æµç¨‹:")
        print("1. åˆå§‹åŒ–å…ƒå‚æ•° Î¸")
        print("2. å¯¹æ¯ä¸ªå…ƒè®­ç»ƒä»»åŠ¡ T_i:")
        print("   a. åœ¨æ”¯æŒé›†ä¸Šè®¡ç®—é€‚åº”å‚æ•°: Î¸_i' = Î¸ - Î±âˆ‡_Î¸ L_{T_i}(f_Î¸)")
        print("   b. åœ¨æŸ¥è¯¢é›†ä¸Šè®¡ç®—å…ƒæŸå¤±: L_{T_i}(f_{Î¸_i'})")
        print("3. æ›´æ–°å…ƒå‚æ•°: Î¸ = Î¸ - Î²âˆ‡_Î¸ Î£_i L_{T_i}(f_{Î¸_i'})")
        print("4. é‡å¤2-3ç›´åˆ°æ”¶æ•›")
        print()
        
        print("æ•°å­¦è¡¨è¿°:")
        print("min_Î¸ Î£_{T_i~p(T)} L_{T_i}(f_{Î¸ - Î±âˆ‡_Î¸ L_{T_i}(f_Î¸)})")
        print("å…¶ä¸­:")
        print("- Î¸: å…ƒå‚æ•°")
        print("- Î±: å†…å±‚å­¦ä¹ ç‡ï¼ˆä»»åŠ¡é€‚åº”ï¼‰")
        print("- Î²: å¤–å±‚å­¦ä¹ ç‡ï¼ˆå…ƒå­¦ä¹ ï¼‰")
        print("- f_Î¸: å‚æ•°åŒ–æ¨¡å‹")
        print()
        
        # MAMLçš„å…³é”®ç‰¹æ€§
        self.maml_properties()
        
        return self.implement_maml()
    
    def maml_properties(self):
        """MAMLå…³é”®ç‰¹æ€§"""
        print("=== MAMLå…³é”®ç‰¹æ€§ ===")
        
        properties = {
            "äºŒé˜¶å¯¼æ•°": {
                "æ¥æº": "å¯¹é€‚åº”åå‚æ•°æ±‚æ¢¯åº¦",
                "è®¡ç®—": "âˆ‡_Î¸ L(f_{Î¸'}), å…¶ä¸­ Î¸' = Î¸ - Î±âˆ‡_Î¸ L(f_Î¸)",
                "æŒ‘æˆ˜": "è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå†…å­˜éœ€æ±‚å¤§",
                "è¿‘ä¼¼": "FOMAMLå¿½ç•¥äºŒé˜¶é¡¹"
            },
            "é€šç”¨æ€§": {
                "ä¼˜åŠ¿": "é€‚ç”¨äºå„ç§ç›‘ç£å­¦ä¹ ä»»åŠ¡",
                "åº”ç”¨": "åˆ†ç±»ã€å›å½’ã€å¼ºåŒ–å­¦ä¹ ",
                "é™åˆ¶": "éœ€è¦åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–",
                "æ‰©å±•": "å¯ç»“åˆä¸åŒç½‘ç»œæ¶æ„"
            },
            "å¿«é€Ÿé€‚åº”": {
                "æœºåˆ¶": "è‰¯å¥½çš„åˆå§‹åŒ– + å°‘æ•°æ¢¯åº¦æ­¥",
                "æ•ˆæœ": "é€šå¸¸1-5æ­¥å°±èƒ½é€‚åº”",
                "åŸç†": "åœ¨æŸå¤±é¢ä¸Šå¯»æ‰¾å¹³å¦åŒºåŸŸ",
                "å‡ ä½•": "æ¥è¿‘å¤šä¸ªä»»åŠ¡çš„æœ€ä¼˜è§£"
            },
            "å…ƒæ¢¯åº¦": {
                "å®šä¹‰": "å…³äºå…ƒå‚æ•°çš„æ¢¯åº¦",
                "ä¼ æ’­": "é€šè¿‡é€‚åº”è¿‡ç¨‹åå‘ä¼ æ’­",
                "ç¨³å®šæ€§": "å¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸",
                "æ­£åˆ™åŒ–": "æ¢¯åº¦è£å‰ªã€æƒé‡è¡°å‡"
            }
        }
        
        for prop, details in properties.items():
            print(f"{prop}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        return properties
    
    def implement_maml(self):
        """å®ç°ç®€åŒ–ç‰ˆMAML"""
        print("=== ç®€åŒ–ç‰ˆMAMLå®ç° ===")
        
        # ç”Ÿæˆå…ƒå­¦ä¹ æ•°æ®
        np.random.seed(42)
        torch.manual_seed(42)
        
        def generate_sinusoid_task():
            """ç”Ÿæˆæ­£å¼¦å‡½æ•°å›å½’ä»»åŠ¡"""
            amplitude = np.random.uniform(0.1, 5.0)
            phase = np.random.uniform(0, np.pi)
            
            def task_function(x):
                return amplitude * np.sin(x + phase)
            
            return task_function
        
        def sample_task_data(task_func, n_samples=10):
            """é‡‡æ ·ä»»åŠ¡æ•°æ®"""
            x = np.random.uniform(-5, 5, n_samples)
            y = task_func(x) + np.random.normal(0, 0.1, n_samples)
            return x.reshape(-1, 1), y.reshape(-1, 1)
        
        # ç®€å•çš„ç¥ç»ç½‘ç»œ
        class SimpleNet(nn.Module):
            def __init__(self, input_dim=1, hidden_dim=40, output_dim=1):
                super().__init__()
                self.fc1 = nn.Linear(input_dim, hidden_dim)
                self.fc2 = nn.Linear(hidden_dim, hidden_dim)
                self.fc3 = nn.Linear(hidden_dim, output_dim)
                
            def forward(self, x):
                x = torch.relu(self.fc1(x))
                x = torch.relu(self.fc2(x))
                return self.fc3(x)
        
        # MAMLç®—æ³•
        def maml_update(model, support_x, support_y, query_x, query_y, 
                       inner_lr=0.01, inner_steps=5):
            """MAMLå†…å±‚æ›´æ–°"""
            # å¤åˆ¶æ¨¡å‹å‚æ•°
            fast_weights = {}
            for name, param in model.named_parameters():
                fast_weights[name] = param.clone()
            
            # å†…å±‚é€‚åº”
            for step in range(inner_steps):
                # å‰å‘ä¼ æ’­
                pred = model(support_x)
                loss = nn.MSELoss()(pred, support_y)
                
                # è®¡ç®—æ¢¯åº¦
                grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)
                
                # æ›´æ–°å¿«é€Ÿæƒé‡
                for (name, param), grad in zip(model.named_parameters(), grads):
                    fast_weights[name] = param - inner_lr * grad
                
                # æ›´æ–°æ¨¡å‹å‚æ•°
                for name, param in model.named_parameters():
                    param.data = fast_weights[name]
            
            # åœ¨æŸ¥è¯¢é›†ä¸Šè®¡ç®—æŸå¤±
            query_pred = model(query_x)
            query_loss = nn.MSELoss()(query_pred, query_y)
            
            return query_loss
        
        # å…ƒè®­ç»ƒ
        model = SimpleNet()
        meta_optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        print("å¼€å§‹MAMLå…ƒè®­ç»ƒ...")
        meta_losses = []
        
        for meta_iter in range(100):
            meta_optimizer.zero_grad()
            meta_loss = 0
            
            # é‡‡æ ·å…ƒæ‰¹æ¬¡
            for task_idx in range(5):  # æ¯æ¬¡5ä¸ªä»»åŠ¡
                # ç”Ÿæˆä»»åŠ¡
                task_func = generate_sinusoid_task()
                
                # é‡‡æ ·æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†
                support_x, support_y = sample_task_data(task_func, 5)
                query_x, query_y = sample_task_data(task_func, 10)
                
                # è½¬æ¢ä¸ºå¼ é‡
                support_x = torch.FloatTensor(support_x)
                support_y = torch.FloatTensor(support_y)
                query_x = torch.FloatTensor(query_x)
                query_y = torch.FloatTensor(query_y)
                
                # ä¿å­˜åŸå§‹å‚æ•°
                original_params = {}
                for name, param in model.named_parameters():
                    original_params[name] = param.clone()
                
                # MAMLæ›´æ–°
                task_loss = maml_update(model, support_x, support_y, query_x, query_y)
                meta_loss += task_loss
                
                # æ¢å¤åŸå§‹å‚æ•°
                for name, param in model.named_parameters():
                    param.data = original_params[name]
            
            # å…ƒæ¢¯åº¦æ›´æ–°
            meta_loss.backward()
            meta_optimizer.step()
            
            meta_losses.append(meta_loss.item() / 5)
            
            if meta_iter % 20 == 0:
                print(f"Meta-iteration {meta_iter}, Meta-loss: {meta_losses[-1]:.4f}")
        
        # å¯è§†åŒ–ç»“æœ
        self.visualize_maml_results(model, meta_losses)
        
        return model, meta_losses
    
    def visualize_maml_results(self, model, meta_losses):
        """å¯è§†åŒ–MAMLç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. å…ƒè®­ç»ƒæŸå¤±æ›²çº¿
        axes[0, 0].plot(meta_losses, linewidth=2, color='blue')
        axes[0, 0].set_xlabel('å…ƒè¿­ä»£æ¬¡æ•°')
        axes[0, 0].set_ylabel('å…ƒæŸå¤±')
        axes[0, 0].set_title('MAMLå…ƒè®­ç»ƒè¿‡ç¨‹')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æµ‹è¯•æ–°ä»»åŠ¡çš„å¿«é€Ÿé€‚åº”
        np.random.seed(123)
        
        # ç”Ÿæˆæµ‹è¯•ä»»åŠ¡
        test_amplitude = 3.0
        test_phase = np.pi/4
        test_func = lambda x: test_amplitude * np.sin(x + test_phase)
        
        # æ”¯æŒé›†
        support_x = np.array([-2, -1, 0, 1, 2]).reshape(-1, 1)
        support_y = test_func(support_x.flatten()).reshape(-1, 1)
        
        # æµ‹è¯•ç‚¹
        test_x = np.linspace(-5, 5, 100).reshape(-1, 1)
        test_y_true = test_func(test_x.flatten())
        
        # MAMLé€‚åº”å‰é¢„æµ‹
        model.eval()
        with torch.no_grad():
            pred_before = model(torch.FloatTensor(test_x)).numpy().flatten()
        
        # MAMLå¿«é€Ÿé€‚åº”
        support_x_tensor = torch.FloatTensor(support_x)
        support_y_tensor = torch.FloatTensor(support_y)
        
        # å‡ æ­¥æ¢¯åº¦ä¸‹é™
        optimizer = optim.SGD(model.parameters(), lr=0.01)
        for step in range(10):
            pred = model(support_x_tensor)
            loss = nn.MSELoss()(pred, support_y_tensor)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # é€‚åº”åé¢„æµ‹
        with torch.no_grad():
            pred_after = model(torch.FloatTensor(test_x)).numpy().flatten()
        
        axes[0, 1].plot(test_x, test_y_true, 'k-', linewidth=2, label='çœŸå®å‡½æ•°')
        axes[0, 1].plot(test_x, pred_before, 'r--', linewidth=2, label='é€‚åº”å‰')
        axes[0, 1].plot(test_x, pred_after, 'b-', linewidth=2, label='é€‚åº”å')
        axes[0, 1].scatter(support_x, support_y, color='red', s=50, zorder=5, label='æ”¯æŒé›†')
        axes[0, 1].set_xlabel('x')
        axes[0, 1].set_ylabel('y')
        axes[0, 1].set_title('MAMLå¿«é€Ÿé€‚åº”æ•ˆæœ')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ä¸åŒé€‚åº”æ­¥æ•°çš„æ•ˆæœ
        axes[1, 0].plot(test_x, test_y_true, 'k-', linewidth=3, label='çœŸå®å‡½æ•°')
        axes[1, 0].scatter(support_x, support_y, color='red', s=50, zorder=5, label='æ”¯æŒé›†')
        
        # é‡æ–°åˆå§‹åŒ–æ¨¡å‹è¿›è¡Œå¤šæ­¥é€‚åº”
        model_copy = SimpleNet()
        model_copy.load_state_dict(model.state_dict())
        
        adaptation_steps = [0, 1, 3, 5, 10]
        colors = plt.cm.viridis(np.linspace(0, 1, len(adaptation_steps)))
        
        for i, steps in enumerate(adaptation_steps):
            # é‡ç½®æ¨¡å‹
            model_copy.load_state_dict(model.state_dict())
            optimizer = optim.SGD(model_copy.parameters(), lr=0.01)
            
            # é€‚åº”æŒ‡å®šæ­¥æ•°
            for step in range(steps):
                pred = model_copy(support_x_tensor)
                loss = nn.MSELoss()(pred, support_y_tensor)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            
            # é¢„æµ‹
            with torch.no_grad():
                pred = model_copy(torch.FloatTensor(test_x)).numpy().flatten()
            
            axes[1, 0].plot(test_x, pred, color=colors[i], linewidth=2, 
                           alpha=0.7, label=f'{steps}æ­¥é€‚åº”')
        
        axes[1, 0].set_xlabel('x')
        axes[1, 0].set_ylabel('y')
        axes[1, 0].set_title('ä¸åŒé€‚åº”æ­¥æ•°çš„æ•ˆæœ')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. MAML vs éšæœºåˆå§‹åŒ–æ¯”è¾ƒ
        adaptation_errors = []
        random_errors = []
        steps_range = range(0, 11)
        
        for steps in steps_range:
            # MAMLé€‚åº”
            model_maml = SimpleNet()
            model_maml.load_state_dict(model.state_dict())
            optimizer_maml = optim.SGD(model_maml.parameters(), lr=0.01)
            
            for step in range(steps):
                pred = model_maml(support_x_tensor)
                loss = nn.MSELoss()(pred, support_y_tensor)
                optimizer_maml.zero_grad()
                loss.backward()
                optimizer_maml.step()
            
            with torch.no_grad():
                pred_maml = model_maml(torch.FloatTensor(test_x)).numpy().flatten()
            maml_error = np.mean((pred_maml - test_y_true)**2)
            adaptation_errors.append(maml_error)
            
            # éšæœºåˆå§‹åŒ–
            model_random = SimpleNet()
            optimizer_random = optim.SGD(model_random.parameters(), lr=0.01)
            
            for step in range(steps):
                pred = model_random(support_x_tensor)
                loss = nn.MSELoss()(pred, support_y_tensor)
                optimizer_random.zero_grad()
                loss.backward()
                optimizer_random.step()
            
            with torch.no_grad():
                pred_random = model_random(torch.FloatTensor(test_x)).numpy().flatten()
            random_error = np.mean((pred_random - test_y_true)**2)
            random_errors.append(random_error)
        
        axes[1, 1].plot(steps_range, adaptation_errors, 'b-o', linewidth=2, label='MAMLåˆå§‹åŒ–')
        axes[1, 1].plot(steps_range, random_errors, 'r-s', linewidth=2, label='éšæœºåˆå§‹åŒ–')
        axes[1, 1].set_xlabel('é€‚åº”æ­¥æ•°')
        axes[1, 1].set_ylabel('æµ‹è¯•MSE')
        axes[1, 1].set_title('MAML vs éšæœºåˆå§‹åŒ–')
        axes[1, 1].set_yscale('log')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def comprehensive_meta_learning_summary():
    """å…ƒå­¦ä¹ ç†è®ºç»¼åˆæ€»ç»“"""
    print("=== å…ƒå­¦ä¹ ç†è®ºç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "æ ¸å¿ƒæ¦‚å¿µ": {
            "å­¦ä¼šå­¦ä¹ ": "ä»å¤šä»»åŠ¡ç»éªŒä¸­å­¦ä¹ å­¦ä¹ ç­–ç•¥",
            "å¿«é€Ÿé€‚åº”": "ç”¨å°‘é‡æ ·æœ¬å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡",
            "ä»»åŠ¡åˆ†å¸ƒ": "å‡è®¾ä»»åŠ¡æ¥è‡ªå…±åŒåˆ†å¸ƒ",
            "å…ƒçŸ¥è¯†": "è·¨ä»»åŠ¡çš„å…±åŒæ¨¡å¼å’Œç»“æ„"
        },
        
        "ä¸»è¦æ–¹æ³•": {
            "åŸºäºåº¦é‡": "å­¦ä¹ ç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œåˆ†ç±»",
            "åŸºäºæ¨¡å‹": "å­¦ä¹ è‰¯å¥½çš„æ¨¡å‹åˆå§‹åŒ–",
            "åŸºäºä¼˜åŒ–": "å­¦ä¹ ä¼˜åŒ–ç®—æ³•æœ¬èº«",
            "åŸºäºè®°å¿†": "å¤–éƒ¨è®°å¿†å¢å¼ºå­¦ä¹ èƒ½åŠ›"
        },
        
        "ä»£è¡¨ç®—æ³•": {
            "MAML": "æ¨¡å‹æ— å…³å…ƒå­¦ä¹ ï¼Œå­¦ä¹ åˆå§‹åŒ–",
            "Prototypical Networks": "åŸå‹ç½‘ç»œï¼ŒåŸºäºè·ç¦»åˆ†ç±»",
            "Matching Networks": "åŒ¹é…ç½‘ç»œï¼Œæ³¨æ„åŠ›æœºåˆ¶",
            "Meta-LSTM": "åŸºäºLSTMçš„å…ƒä¼˜åŒ–å™¨"
        },
        
        "ç†è®ºåŸºç¡€": {
            "PAC-Bayes": "æ³›åŒ–ç†è®ºåˆ†æ",
            "ä¼˜åŒ–ç†è®º": "äºŒé˜¶ä¼˜åŒ–ã€æ”¶æ•›æ€§",
            "è¡¨ç¤ºå­¦ä¹ ": "ç‰¹å¾è¡¨ç¤ºçš„å¯è¿ç§»æ€§",
            "ä¿¡æ¯è®º": "ä»»åŠ¡é—´ä¿¡æ¯å…±äº«"
        },
        
        "åº”ç”¨é¢†åŸŸ": {
            "å°‘æ ·æœ¬å­¦ä¹ ": "å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹",
            "å¼ºåŒ–å­¦ä¹ ": "å¿«é€Ÿç­–ç•¥é€‚åº”",
            "ç¥ç»æ¶æ„æœç´¢": "è‡ªåŠ¨åŒ–æ¨¡å‹è®¾è®¡",
            "è¶…å‚æ•°ä¼˜åŒ–": "è‡ªåŠ¨åŒ–è°ƒå‚"
        },
        
        "æŒ‘æˆ˜ä¸é™åˆ¶": {
            "ä»»åŠ¡åˆ†å¸ƒ": "å‡è®¾å¯èƒ½ä¸æ»¡è¶³",
            "è®¡ç®—å¤æ‚åº¦": "äºŒé˜¶å¯¼æ•°è®¡ç®—å¼€é”€",
            "æ³›åŒ–èƒ½åŠ›": "è·¨åŸŸä»»åŠ¡é€‚åº”å›°éš¾",
            "è¯„ä¼°æ ‡å‡†": "å…ƒå­¦ä¹ è¯„ä¼°ç¼ºä¹æ ‡å‡†"
        },
        
        "æœªæ¥æ–¹å‘": {
            "ç†è®ºåˆ†æ": "æ›´ä¸¥æ ¼çš„ç†è®ºä¿è¯",
            "ç®—æ³•æ”¹è¿›": "æ›´é«˜æ•ˆçš„ä¼˜åŒ–æ–¹æ³•",
            "åº”ç”¨æ‹“å±•": "æ›´å¤šé¢†åŸŸçš„åº”ç”¨",
            "åŸºå‡†å»ºè®¾": "æ ‡å‡†åŒ–è¯„ä¼°ä½“ç³»"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("å…ƒå­¦ä¹ ç†è®ºæŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Finn et al. (2017): "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
- Vinyals et al. (2016): "Matching Networks for One Shot Learning"
- Snell et al. (2017): "Prototypical Networks for Few-shot Learning"
- Ravi & Larochelle (2017): "Optimization as a Model for Few-Shot Learning"
- Hospedales et al. (2021): "Meta-Learning in Neural Networks: A Survey"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [å°‘æ ·æœ¬å­¦ä¹ ](few_shot_learning.md) - å…·ä½“åº”ç”¨å’ŒæŠ€æœ¯
- [è¿ç§»å­¦ä¹ ](transfer_learning.md) - ç›¸å…³çš„çŸ¥è¯†è¿ç§»æ–¹æ³•
- [ç¥ç»æ¶æ„æœç´¢](neural_architecture_search.md) - å…ƒå­¦ä¹ åœ¨NASä¸­çš„åº”ç”¨