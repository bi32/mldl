# 线性模型原理详解 📐

## 1. 线性回归的数学本质

### 1.1 从一个故事开始

想象你是一个房产评估师，需要根据房屋面积预测价格。你收集了100套房子的数据，画在坐标系上，发现点大致呈一条直线分布。这条"最合适"的直线，就是线性回归要找的东西。

### 1.2 核心思想

线性回归假设输入特征和输出之间存在**线性关系**：

```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```

其中：
- **y**：我们要预测的目标（如房价）
- **x₁, x₂, ..., xₙ**：输入特征（如面积、房间数、位置等）
- **β₀**：截距项（偏置），表示当所有特征为0时的基准值
- **β₁, β₂, ..., βₙ**：回归系数，表示每个特征对结果的影响程度
- **ε**：误差项，表示模型无法解释的随机因素

### 1.3 为什么叫"线性"？

"线性"指的是模型关于**参数β是线性的**，而不是关于特征x。这意味着：
- ✅ `y = β₀ + β₁x` 是线性的
- ✅ `y = β₀ + β₁x²` 也是线性的（对β线性）
- ❌ `y = β₀ + x^β₁` 不是线性的（β在指数上）

### 1.4 最小二乘法原理

#### 目标：找到最佳的参数β

我们希望预测值ŷ尽可能接近真实值y。"接近"的程度用**损失函数**衡量：

**均方误差（MSE）**：
```
L(β) = (1/n) Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²
     = (1/n) Σᵢ₌₁ⁿ (yᵢ - β₀ - β₁x₁ᵢ - ... - βₙxₙᵢ)²
```

为什么用平方？
1. **放大大误差**：平方会让大的预测误差受到更重的惩罚
2. **数学性质好**：可导、凸函数，有唯一最优解
3. **几何意义**：等价于找到点到直线的最短距离

#### 求解过程

**方法1：解析解（正规方程）**

通过对损失函数求导并令其为0：
```
∂L/∂β = 0
```

可以得到闭式解：
```
β = (X^T X)^(-1) X^T y
```

这就是著名的**正规方程**！

**方法2：梯度下降**

当特征很多或样本很大时，计算矩阵逆很慢，我们使用迭代优化：

```
β := β - α · ∇L(β)
```

其中α是学习率，∇L(β)是损失函数的梯度。

### 1.5 几何解释

在二维空间中：
- 数据点散布在平面上
- 线性回归找一条直线
- 使所有点到直线的**垂直距离平方和**最小

在高维空间中：
- 数据点在n维空间中
- 线性回归找一个超平面
- 最小化所有点到超平面的距离

### 1.6 概率解释

从概率角度看，线性回归假设：
```
y = β^T x + ε, 其中 ε ~ N(0, σ²)
```

这意味着：
- 误差服从正态分布
- 在给定x的条件下，y服从正态分布：`y|x ~ N(β^T x, σ²)`

最小二乘法等价于**最大似然估计**！

## 2. 正则化的深入理解

### 2.1 过拟合问题

想象你在记忆一本书：
- **欠拟合**：只记住了"这是一本书"
- **合适**：记住了主要情节和人物关系
- **过拟合**：逐字背诵，包括错别字

在机器学习中，过拟合表现为：
- 训练误差很小
- 测试误差很大
- 模型过于复杂，记住了噪声

### 2.2 Ridge回归（L2正则化）

#### 核心思想：限制参数大小

Ridge在损失函数中加入L2惩罚项：
```
L_ridge(β) = MSE(β) + λ Σⱼ₌₁ᵖ βⱼ²
```

#### 为什么能防止过拟合？

1. **参数收缩**：大的参数受到更重的惩罚
2. **平滑性**：参数变小→模型变平滑→对噪声不敏感
3. **贝叶斯解释**：等价于给参数加上高斯先验

#### 几何理解

- 原始问题：在整个参数空间找最优解
- Ridge：在一个"球"内找最优解（Σβⱼ² ≤ t）
- 解在等高线和球的切点

### 2.3 Lasso回归（L1正则化）

#### 核心思想：稀疏化

Lasso使用L1惩罚：
```
L_lasso(β) = MSE(β) + λ Σⱼ₌₁ᵖ |βⱼ|
```

#### 为什么能做特征选择？

L1范数的几何形状是"菱形"（在2D中）或"超立方体"（在高维中）：
- 有尖角（角点）
- 等高线容易在角点相切
- 角点意味着某些βⱼ = 0

这就是**稀疏性**的来源！

#### Lasso vs Ridge

| 特性 | Lasso (L1) | Ridge (L2) |
|------|------------|------------|
| 几何形状 | 菱形/立方体 | 圆形/球体 |
| 解的特点 | 稀疏（很多0） | 密集（都很小） |
| 特征选择 | 自动选择 | 保留所有 |
| 共线性处理 | 随机选一个 | 平均分配 |
| 可微性 | 在0处不可微 | 处处可微 |

### 2.4 Elastic Net（弹性网）

结合L1和L2的优点：
```
L_elastic(β) = MSE(β) + λ₁ Σⱼ |βⱼ| + λ₂ Σⱼ βⱼ²
```

或用比例参数α：
```
L_elastic(β) = MSE(β) + λ [α Σⱼ |βⱼ| + (1-α) Σⱼ βⱼ²]
```

优势：
- 既能特征选择（L1）
- 又能处理共线性（L2）
- 在相关特征组中选择整组

## 3. 逻辑回归的本质

### 3.1 从线性到非线性

线性回归输出可以是任意实数，但分类需要概率（0到1之间）。

**Sigmoid函数**来拯救：
```
σ(z) = 1 / (1 + e^(-z))
```

性质：
- 输入：(-∞, +∞)
- 输出：(0, 1)
- S型曲线
- 中心对称：σ(-z) = 1 - σ(z)

### 3.2 概率建模

逻辑回归建模的是**对数几率**（log-odds）：
```
log(p/(1-p)) = β₀ + β₁x₁ + ... + βₙxₙ
```

其中p是属于正类的概率。

变换后：
```
p = 1 / (1 + exp(-(β₀ + β₁x₁ + ... + βₙxₙ)))
```

### 3.3 最大似然估计

给定n个样本，似然函数：
```
L(β) = Πᵢ₌₁ⁿ pᵢʸⁱ (1-pᵢ)^(1-yᵢ)
```

对数似然：
```
log L(β) = Σᵢ₌₁ⁿ [yᵢ log pᵢ + (1-yᵢ) log(1-pᵢ)]
```

这就是**交叉熵损失**的来源！

### 3.4 决策边界

- 线性边界：β^T x = 0
- 当β^T x > 0时，预测为正类
- 当β^T x < 0时，预测为负类

决策边界是一个超平面，将特征空间一分为二。

### 3.5 多分类扩展

**Softmax回归**（多项逻辑回归）：
```
P(y=k|x) = exp(βₖᵀx) / Σⱼ exp(βⱼᵀx)
```

- 每个类别有自己的参数向量βₖ
- Softmax确保所有概率和为1
- 交叉熵损失自然推广

## 4. 支持向量机（SVM）原理

### 4.1 核心思想：最大间隔

想象两群人站在操场上，你要画一条线把他们分开。SVM的想法是：
- 不仅要分开
- 还要离两边都尽可能远
- 这个"远"的距离叫**间隔**（margin）

### 4.2 硬间隔SVM

对于线性可分的数据：

**目标**：
```
max margin = 2/||w||
```

等价于：
```
min (1/2)||w||²
s.t. yᵢ(wᵀxᵢ + b) ≥ 1, ∀i
```

### 4.3 软间隔SVM

现实数据往往不是完全可分的，引入松弛变量ξ：

```
min (1/2)||w||² + C Σᵢ ξᵢ
s.t. yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ
     ξᵢ ≥ 0
```

C是权衡参数：
- C大：重视分类正确，间隔可以小
- C小：重视间隔大，允许一些错分

### 4.4 核技巧

**问题**：如果数据线性不可分怎么办？

**解决**：映射到高维空间！

核函数K(x, y)计算高维空间的内积，而不需要显式映射：
- 线性核：K(x, y) = xᵀy
- 多项式核：K(x, y) = (xᵀy + c)^d
- RBF核：K(x, y) = exp(-γ||x - y||²)

### 4.5 支持向量

- 只有离决策边界最近的点才重要
- 这些点叫**支持向量**
- 移除其他点，决策边界不变
- 这是SVM的稀疏性来源

## 5. 优化算法原理

### 5.1 梯度下降家族

#### 批量梯度下降（BGD）
- 使用所有数据计算梯度
- 稳定但慢
- 可能陷入局部最优

#### 随机梯度下降（SGD）
- 每次用一个样本
- 快但不稳定
- 噪声有助于跳出局部最优

#### 小批量梯度下降（Mini-batch GD）
- 折中方案
- 利用向量化加速
- 实践中最常用

### 5.2 动量法（Momentum）

物理类比：球滚下山坡
```
v := γv - α∇L
θ := θ + v
```

- 积累历史梯度
- 加速收敛
- 跨越局部最优

### 5.3 自适应学习率

#### AdaGrad
- 累积历史梯度平方
- 频繁更新的参数→学习率降低
- 稀疏数据友好

#### RMSprop
- 指数移动平均
- 解决AdaGrad学习率消失问题

#### Adam
- 结合动量和自适应学习率
- 一阶矩（动量）+ 二阶矩（自适应）
- 实践中的默认选择

## 6. 模型评估的统计学基础

### 6.1 偏差-方差权衡

总误差 = 偏差² + 方差 + 不可约误差

- **偏差**：模型的系统性错误
- **方差**：模型对数据扰动的敏感度
- **不可约误差**：数据本身的噪声

### 6.2 交叉验证的原理

为什么要交叉验证？
- 有限数据的充分利用
- 减少评估的方差
- 检测过拟合

### 6.3 评估指标的选择

- **准确率**：类别平衡时
- **精确率/召回率**：类别不平衡时
- **AUC-ROC**：需要概率输出时
- **对数损失**：概率校准重要时

## 思考题

1. 为什么线性回归的损失函数用平方误差而不是绝对值误差？
2. Ridge和Lasso都能防止过拟合，但机制有何不同？
3. 逻辑回归是"回归"还是"分类"算法？为什么？
4. SVM的核技巧本质上在做什么？
5. 为什么深度学习中Adam优化器如此流行？

## 延伸阅读

- 《The Elements of Statistical Learning》- 统计学习的圣经
- 《Pattern Recognition and Machine Learning》- Bishop的经典
- 《Convex Optimization》- Boyd，了解优化理论
- 《Understanding Machine Learning》- 学习理论的现代观点