# 树模型与集成学习原理深度剖析 🌲

## 1. 决策树：从ID3到CART

### 1.1 决策树的哲学思想

决策树模仿人类的决策过程。想象你是一位医生诊断病人：
- 首先问："发烧吗？"
- 如果是，再问："咳嗽吗？"
- 根据一系列问题，最终得出诊断

这就是决策树的本质：**通过一系列if-then规则进行决策**。

### 1.2 信息论基础

#### 熵（Entropy）
熵衡量系统的混乱程度：
```
H(S) = -Σᵢ pᵢ log₂(pᵢ)
```

直观理解：
- 硬币正反各50%：H = 1（最大不确定性）
- 硬币100%正面：H = 0（完全确定）

#### 信息增益（Information Gain）
分裂后不确定性的减少：
```
IG(S, A) = H(S) - Σᵥ (|Sᵥ|/|S|) × H(Sᵥ)
```

其中A是特征，v是特征的可能取值。

#### 信息增益率（Gain Ratio）
解决信息增益偏向多值特征的问题：
```
GR(S, A) = IG(S, A) / IV(A)
```

其中IV(A)是特征A的固有值（Intrinsic Value）。

### 1.3 三大经典算法

#### ID3算法
- **分裂准则**：信息增益
- **优点**：简单直观
- **缺点**：偏向多值属性，不能处理连续值

#### C4.5算法
- **分裂准则**：信息增益率
- **改进**：
  - 处理连续值（二分法）
  - 处理缺失值
  - 后剪枝

#### CART算法
- **分裂准则**：
  - 分类：基尼指数
  - 回归：均方误差
- **特点**：
  - 二叉树（每个节点只有两个分支）
  - 可以做回归
  - 使用代理分裂处理缺失值

### 1.4 基尼指数 vs 熵

基尼指数：
```
Gini(S) = 1 - Σᵢ pᵢ²
```

对比：
| 指标 | 计算复杂度 | 敏感性 | 使用场景 |
|------|-----------|--------|----------|
| 熵 | 有对数运算 | 对概率变化更敏感 | ID3, C4.5 |
| 基尼 | 只有平方运算 | 计算更快 | CART |

实践中，两者效果差异很小。

### 1.5 剪枝策略

#### 预剪枝
在构建过程中停止：
- 最小样本数
- 最大深度
- 最小信息增益

优点：快速，防止过拟合
缺点：可能欠拟合

#### 后剪枝
先构建完整树，再剪枝：
- 降低错误率剪枝（REP）
- 悲观错误剪枝（PEP）
- 最小错误剪枝（MEP）
- 代价复杂度剪枝（CCP）

CART的代价复杂度剪枝：
```
Rα(T) = R(T) + α|T|
```
其中R(T)是误差，|T|是叶节点数，α是复杂度参数。

## 2. 随机森林：集体智慧

### 2.1 Bagging的统计学原理

**Bootstrap**：有放回抽样

假设有n个样本，每次抽n个：
- 某样本被抽中概率：1/n
- 某样本不被抽中概率：(1-1/n)
- n次都不被抽中：(1-1/n)^n → 1/e ≈ 0.368

这意味着约36.8%的样本不会出现在Bootstrap样本中（OOB样本）。

### 2.2 为什么随机森林有效？

#### 方差减少
假设有M个独立同分布的模型，每个方差为σ²：
- 单个模型方差：σ²
- 平均后方差：σ²/M

但树之间不是独立的，假设相关系数为ρ：
```
Var(平均) = ρσ² + (1-ρ)σ²/M
```

随机森林通过**特征随机**减少ρ！

#### 特征随机性
每次分裂只考虑m个特征（m < 总特征数）：
- 分类：m ≈ √p
- 回归：m ≈ p/3

这进一步降低树之间的相关性。

### 2.3 OOB误差估计

Out-of-Bag样本的妙用：
- 不需要单独的验证集
- 每棵树都有自己的"测试集"
- OOB误差是泛化误差的无偏估计

### 2.4 特征重要性

两种计算方法：

#### 基于不纯度减少
```
Importance(Xⱼ) = Σ(所有使用Xⱼ的节点) 不纯度减少 × 样本比例
```

#### 基于OOB准确率
1. 计算原始OOB准确率
2. 随机打乱特征Xⱼ的值
3. 重新计算OOB准确率
4. 重要性 = 准确率下降

## 3. 梯度提升：错误中学习

### 3.1 Boosting的核心思想

不同于Bagging的并行思想，Boosting是**串行的**：
1. 训练第一个弱学习器
2. 找出错误，重点关注
3. 训练下一个学习器纠正错误
4. 重复直到满意

### 3.2 AdaBoost：自适应提升

#### 算法流程
1. 初始化样本权重：wᵢ = 1/n
2. 对于每轮t：
   - 训练弱分类器hₜ
   - 计算错误率：εₜ = Σᵢ wᵢ × I(hₜ(xᵢ) ≠ yᵢ)
   - 计算分类器权重：αₜ = 0.5 × log((1-εₜ)/εₜ)
   - 更新样本权重：wᵢ = wᵢ × exp(-αₜ × yᵢ × hₜ(xᵢ))
3. 最终分类器：H(x) = sign(Σₜ αₜ × hₜ(x))

#### 指数损失
AdaBoost优化的是指数损失：
```
L = Σᵢ exp(-yᵢ × f(xᵢ))
```

这解释了为什么AdaBoost对异常值敏感。

### 3.3 梯度提升（Gradient Boosting）

#### 函数空间的梯度下降

把提升看作在函数空间的优化：
```
F(x) = Σₘ γₘ × hₘ(x)
```

每步添加一个函数，使损失最小化。

#### 负梯度拟合

关键洞察：**负梯度就是残差的一般化**！

对于平方损失：
```
-∂L/∂F = y - F(x) （残差）
```

对于其他损失，负梯度给出了"伪残差"。

### 3.4 GBDT的细节

#### 为什么用决策树？
1. **非线性**：自然处理非线性关系
2. **交互**：自动发现特征交互
3. **鲁棒**：对异常值和缺失值鲁棒
4. **可解释**：可以输出特征重要性

#### 正则化技术
1. **收缩**（Shrinkage）：
   ```
   Fₘ = Fₘ₋₁ + ν × hₘ
   ```
   ν是学习率（通常0.01-0.1）

2. **子采样**：
   - 每轮只用部分数据（0.5-0.8）
   - 类似随机森林，增加随机性

3. **树的约束**：
   - 最大深度（3-10）
   - 最小叶节点样本数
   - 最大叶节点数

## 4. XGBoost：工程与理论的完美结合

### 4.1 目标函数

XGBoost优化的目标：
```
Obj = Σᵢ L(yᵢ, ŷᵢ) + Σₖ Ω(fₖ)
```

其中Ω(f)是正则项：
```
Ω(f) = γT + (λ/2) Σⱼ wⱼ²
```
- T：叶节点数
- wⱼ：叶节点权重

### 4.2 二阶泰勒展开

使用泰勒展开近似损失函数：
```
L ≈ Σᵢ [L(yᵢ, ŷᵢ⁽ᵗ⁻¹⁾) + gᵢfₜ(xᵢ) + (1/2)hᵢfₜ²(xᵢ)]
```

其中：
- gᵢ = ∂L/∂ŷ⁽ᵗ⁻¹⁾（一阶导数）
- hᵢ = ∂²L/∂ŷ²⁽ᵗ⁻¹⁾（二阶导数）

### 4.3 最优叶节点权重

给定树结构，叶节点j的最优权重：
```
wⱼ* = -Gⱼ/(Hⱼ + λ)
```

其中Gⱼ = Σᵢ∈ⱼ gᵢ，Hⱼ = Σᵢ∈ⱼ hᵢ

### 4.4 分裂增益

分裂的增益公式：
```
Gain = (1/2)[G²ₗ/(Hₗ+λ) + G²ᵣ/(Hᵣ+λ) - G²/(H+λ)] - γ
```

这个公式同时考虑了：
- 损失减少（前三项）
- 复杂度增加（γ）

### 4.5 系统优化

#### 列采样
三个层次的采样：
- **按树采样**：每棵树用部分特征
- **按层采样**：每层用部分特征
- **按节点采样**：每个节点用部分特征

#### 稀疏感知
自动处理缺失值和稀疏特征：
- 学习默认方向
- 只遍历非缺失值

#### 加权分位数草图
近似算法找分裂点：
- 不遍历所有可能
- 使用分位数
- 根据二阶导数加权

## 5. LightGBM：效率革命

### 5.1 Histogram算法

将连续值离散化为直方图：
- 内存：O(#data) → O(#bins)
- 速度：大幅提升
- 精度：轻微损失

### 5.2 GOSS（基于梯度的单边采样）

核心思想：**梯度大的样本更重要**

1. 保留top a%的大梯度样本
2. 随机采样b%的小梯度样本
3. 给小梯度样本乘以(1-a)/b的权重

理论保证：保持数据分布不变。

### 5.3 EFB（互斥特征捆绑）

很多特征是互斥的（不同时非零）：
- 独热编码的特征
- 稀疏特征

将互斥特征捆绑成一个特征，大幅减少特征数。

### 5.4 Leaf-wise生长

vs Level-wise（XGBoost）：
- Level-wise：同时分裂同一层所有节点
- Leaf-wise：只分裂增益最大的叶节点

Leaf-wise更高效但容易过拟合，需要限制深度。

## 6. CatBoost：类别特征的艺术

### 6.1 目标统计编码

对类别特征的革命性处理：
```
x̂ᵢ = (Σⱼ<ᵢ [xⱼ=xᵢ] × yⱼ + ap) / (Σⱼ<ᵢ [xⱼ=xᵢ] + a)
```

- 只使用之前的样本（避免泄露）
- 加入先验p和权重a（平滑）

### 6.2 有序提升

问题：传统GBDT存在预测偏移

解决：
1. 随机排列数据
2. 对每个样本，只用之前的样本训练
3. 使用多个排列，取平均

### 6.3 对称树

强制所有层使用相同的分裂特征：
- 减少过拟合
- 加速预测
- 便于GPU并行

## 7. 集成学习的理论基础

### 7.1 PAC学习理论

**强可学习 = 弱可学习**（Schapire, 1990）

只要弱学习器比随机猜测好一点点，就能通过集成达到任意精度。

### 7.2 Bias-Variance分解

- **Bagging**：主要减少方差
  - 适合高方差模型（如决策树）
  - 不改变偏差

- **Boosting**：同时减少偏差和方差
  - 适合高偏差模型
  - 可能增加方差（过拟合）

### 7.3 多样性的重要性

集成效果取决于：
1. **个体精度**：每个模型要足够好
2. **多样性**：模型之间要有差异

多样性来源：
- 数据采样（Bagging）
- 特征采样（Random Forest）
- 算法差异（Stacking）
- 参数差异（同算法不同参数）

## 8. 实践中的权衡

### 8.1 模型选择指南

| 场景 | 推荐模型 | 原因 |
|------|----------|------|
| 小数据集 | Random Forest | 不易过拟合 |
| 大数据集 | LightGBM | 速度快 |
| 类别特征多 | CatBoost | 自动处理 |
| 需要解释性 | 单棵决策树 | 可视化 |
| 竞赛 | XGBoost/LightGBM | 性能好，资料多 |

### 8.2 超参数重要性排序

1. **树的数量**（n_estimators）
2. **学习率**（learning_rate）
3. **树的深度**（max_depth）
4. **子采样比例**（subsample）
5. **特征采样**（colsample）
6. **正则化参数**（reg_lambda, reg_alpha）

### 8.3 过拟合诊断

征兆：
- 训练误差持续下降，验证误差上升
- 特征重要性集中在少数特征
- 树很深，叶节点样本很少

对策：
- 减少树的数量
- 降低学习率
- 增加正则化
- 增加子采样
- 减少树的深度

## 思考题

1. 为什么随机森林的树不需要剪枝？
2. GBDT为什么通常使用浅树（深度3-10）？
3. XGBoost的二阶导数相比一阶导数有什么优势？
4. LightGBM的直方图算法会损失精度，为什么实践中影响很小？
5. 什么情况下Bagging比Boosting更适合？

## 深入阅读

- 《The Elements of Statistical Learning》第9-10章
- 《Greedy Function Approximation: A Gradient Boosting Machine》- Friedman的原始论文
- 《XGBoost: A Scalable Tree Boosting System》- XGBoost论文
- 《LightGBM: A Highly Efficient Gradient Boosting Decision Tree》- LightGBM论文
- 《CatBoost: unbiased boosting with categorical features》- CatBoost论文