# è´å¶æ–¯æœºå™¨å­¦ä¹ ç†è®ºï¼šä¸ç¡®å®šæ€§å»ºæ¨¡çš„è‰ºæœ¯ ğŸ¯

æ·±å…¥ç†è§£è´å¶æ–¯æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒç†è®ºï¼Œä»è´å¶æ–¯æ¨æ–­åˆ°å˜åˆ†æ–¹æ³•ã€‚

## 1. è´å¶æ–¯æ¨æ–­åŸºç¡€ ğŸ”

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize
import pandas as pd
from sklearn.datasets import make_regression, make_classification
from sklearn.linear_model import BayesianRidge
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
import warnings
warnings.filterwarnings('ignore')

class BayesianInference:
    """è´å¶æ–¯æ¨æ–­åŸºç¡€"""
    
    def __init__(self):
        self.models = {}
    
    def bayesian_framework(self):
        """è´å¶æ–¯æ¡†æ¶"""
        print("=== è´å¶æ–¯æ¨æ–­æ¡†æ¶ ===")
        
        print("è´å¶æ–¯å®šç†:")
        print("P(Î¸|D) = P(D|Î¸) Ã— P(Î¸) / P(D)")
        print()
        
        components = {
            "åéªŒåˆ†å¸ƒ P(Î¸|D)": {
                "å«ä¹‰": "è§‚æµ‹æ•°æ®åå¯¹å‚æ•°çš„ä¿¡å¿µ",
                "é‡è¦æ€§": "æ¨æ–­çš„æœ€ç»ˆç›®æ ‡",
                "è·å¾—æ–¹å¼": "é€šè¿‡è´å¶æ–¯å®šç†è®¡ç®—",
                "ç”¨é€”": "é¢„æµ‹ã€å†³ç­–ã€ä¸ç¡®å®šæ€§é‡åŒ–"
            },
            "ä¼¼ç„¶å‡½æ•° P(D|Î¸)": {
                "å«ä¹‰": "ç»™å®šå‚æ•°ä¸‹æ•°æ®çš„æ¦‚ç‡",
                "ä½œç”¨": "æ•°æ®å¯¹å‚æ•°çš„çº¦æŸ",
                "æ€§è´¨": "ä¸æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°",
                "è®¡ç®—": "åŸºäºæ•°æ®ç”Ÿæˆæ¨¡å‹"
            },
            "å…ˆéªŒåˆ†å¸ƒ P(Î¸)": {
                "å«ä¹‰": "è§‚æµ‹æ•°æ®å‰å¯¹å‚æ•°çš„ä¿¡å¿µ",
                "æ¥æº": "é¢†åŸŸçŸ¥è¯†ã€å†å²æ•°æ®ã€ä¸»è§‚åˆ¤æ–­",
                "å½±å“": "å°æ ·æœ¬æ—¶å½±å“å¤§ï¼Œå¤§æ ·æœ¬æ—¶å½±å“å°",
                "é€‰æ‹©": "å…±è½­å…ˆéªŒã€æ— ä¿¡æ¯å…ˆéªŒã€æ­£åˆ™åŒ–å…ˆéªŒ"
            },
            "è¾¹é™…ä¼¼ç„¶ P(D)": {
                "å«ä¹‰": "æ•°æ®çš„è¾¹é™…æ¦‚ç‡",
                "è®¡ç®—": "P(D) = âˆ« P(D|Î¸)P(Î¸)dÎ¸",
                "ä½œç”¨": "å½’ä¸€åŒ–å¸¸æ•°ã€æ¨¡å‹é€‰æ‹©",
                "åˆ«å": "è¯æ®ã€æ¨¡å‹è¯æ®"
            }
        }
        
        for comp, details in components.items():
            print(f"{comp}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # å¯è§†åŒ–è´å¶æ–¯æ›´æ–°è¿‡ç¨‹
        self.visualize_bayesian_update()
        
        return components
    
    def visualize_bayesian_update(self):
        """å¯è§†åŒ–è´å¶æ–¯æ›´æ–°è¿‡ç¨‹"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # æ¨¡æ‹Ÿè´å¶æ–¯æ›´æ–°ï¼šä¼°è®¡ç¡¬å¸æ­£é¢æ¦‚ç‡
        true_p = 0.7  # çœŸå®æ¦‚ç‡
        prior_alpha, prior_beta = 1, 1  # Betaå…ˆéªŒå‚æ•°
        
        # ä¸åŒæ•°æ®é‡çš„è§‚æµ‹
        observations = [0, 5, 20, 100]
        
        x = np.linspace(0, 1, 1000)
        
        for i, n_obs in enumerate(observations):
            if i >= 6:
                break
                
            if n_obs == 0:
                # å…ˆéªŒåˆ†å¸ƒ
                alpha, beta = prior_alpha, prior_beta
                title = "å…ˆéªŒåˆ†å¸ƒ"
            else:
                # ç”Ÿæˆè§‚æµ‹æ•°æ®
                np.random.seed(42)
                data = np.random.binomial(1, true_p, n_obs)
                successes = np.sum(data)
                failures = n_obs - successes
                
                # æ›´æ–°åéªŒå‚æ•°
                alpha = prior_alpha + successes
                beta = prior_beta + failures
                title = f"è§‚æµ‹{n_obs}æ¬¡åçš„åéªŒ"
            
            # ç»˜åˆ¶åˆ†å¸ƒ
            row, col = i // 3, i % 3
            y = stats.beta.pdf(x, alpha, beta)
            axes[row, col].plot(x, y, 'b-', linewidth=2, label=f'Beta({alpha}, {beta})')
            axes[row, col].fill_between(x, y, alpha=0.3)
            axes[row, col].axvline(true_p, color='red', linestyle='--', 
                                  linewidth=2, label=f'çœŸå®å€¼={true_p}')
            axes[row, col].axvline(alpha/(alpha+beta), color='green', 
                                  linestyle=':', linewidth=2, label='åéªŒå‡å€¼')
            
            axes[row, col].set_xlabel('p (æ­£é¢æ¦‚ç‡)')
            axes[row, col].set_ylabel('å¯†åº¦')
            axes[row, col].set_title(title)
            axes[row, col].legend()
            axes[row, col].grid(True, alpha=0.3)
        
        # åˆ é™¤å¤šä½™å­å›¾
        for i in range(4, 6):
            row, col = i // 3, i % 3
            axes[row, col].remove()
        
        # æ·»åŠ æ¦‚å¿µå›¾
        ax_concept = fig.add_subplot(2, 3, (5, 6))
        ax_concept.text(0.1, 0.8, "è´å¶æ–¯æ›´æ–°è¿‡ç¨‹", fontsize=16, weight='bold',
                       transform=ax_concept.transAxes)
        ax_concept.text(0.1, 0.6, "å…ˆéªŒ + æ•°æ® â†’ åéªŒ", fontsize=14,
                       transform=ax_concept.transAxes)
        ax_concept.text(0.1, 0.4, "â€¢ æ›´å¤šæ•°æ® â†’ æ›´ç¡®å®šçš„åéªŒ", fontsize=12,
                       transform=ax_concept.transAxes)
        ax_concept.text(0.1, 0.3, "â€¢ åéªŒå‡å€¼è¶‹å‘çœŸå®å€¼", fontsize=12,
                       transform=ax_concept.transAxes)
        ax_concept.text(0.1, 0.2, "â€¢ åéªŒæ–¹å·®é€æ¸å‡å°", fontsize=12,
                       transform=ax_concept.transAxes)
        ax_concept.axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def bayesian_vs_frequentist(self):
        """è´å¶æ–¯vsé¢‘ç‡è®ºæ¯”è¾ƒ"""
        print("=== è´å¶æ–¯ vs é¢‘ç‡è®º ===")
        
        comparison = {
            "æ¦‚ç‡è§£é‡Š": {
                "é¢‘ç‡è®º": "é•¿æœŸé¢‘ç‡ï¼Œå®¢è§‚æ¦‚ç‡",
                "è´å¶æ–¯": "ä¿¡å¿µç¨‹åº¦ï¼Œä¸»è§‚æ¦‚ç‡",
                "ä¾‹å­": "ç¡¬å¸æ­£é¢æ¦‚ç‡çš„å«ä¹‰"
            },
            "å‚æ•°æ€§è´¨": {
                "é¢‘ç‡è®º": "å‚æ•°æ˜¯å›ºå®šæœªçŸ¥å¸¸æ•°",
                "è´å¶æ–¯": "å‚æ•°æ˜¯éšæœºå˜é‡",
                "æ¨æ–­": "ç‚¹ä¼°è®¡ vs åˆ†å¸ƒä¼°è®¡"
            },
            "ä¸ç¡®å®šæ€§": {
                "é¢‘ç‡è®º": "ç½®ä¿¡åŒºé—´ï¼ŒæŠ½æ ·åˆ†å¸ƒ",
                "è´å¶æ–¯": "å¯ä¿¡åŒºé—´ï¼ŒåéªŒåˆ†å¸ƒ",
                "è§£é‡Š": "ç¨‹åºvsä¿¡å¿µ"
            },
            "å…ˆéªŒä¿¡æ¯": {
                "é¢‘ç‡è®º": "ä¸ä½¿ç”¨å…ˆéªŒä¿¡æ¯",
                "è´å¶æ–¯": "è‡ªç„¶èåˆå…ˆéªŒä¿¡æ¯",
                "ä¼˜åŠ¿": "å°æ ·æœ¬æƒ…å†µä¸‹çš„ä¼˜åŠ¿"
            },
            "è®¡ç®—å¤æ‚åº¦": {
                "é¢‘ç‡è®º": "é€šå¸¸è¾ƒç®€å•",
                "è´å¶æ–¯": "ç§¯åˆ†è®¡ç®—å›°éš¾",
                "è§£å†³": "MCMCã€å˜åˆ†æ¨æ–­"
            }
        }
        
        for aspect, details in comparison.items():
            print(f"{aspect}:")
            for approach, description in details.items():
                print(f"  {approach}: {description}")
            print()
        
        # å¯è§†åŒ–æ¯”è¾ƒ
        self.visualize_frequentist_vs_bayesian()
        
        return comparison
    
    def visualize_frequentist_vs_bayesian(self):
        """å¯è§†åŒ–é¢‘ç‡è®ºvsè´å¶æ–¯æ–¹æ³•"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # ç”Ÿæˆæ•°æ®
        np.random.seed(42)
        true_mean = 2.5
        true_std = 1.0
        sample_sizes = [5, 10, 30, 100]
        
        for i, n in enumerate(sample_sizes):
            ax = axes[i//2, i%2]
            
            # ç”Ÿæˆæ ·æœ¬
            data = np.random.normal(true_mean, true_std, n)
            sample_mean = np.mean(data)
            sample_std = np.std(data, ddof=1)
            
            # é¢‘ç‡è®ºç½®ä¿¡åŒºé—´
            se = sample_std / np.sqrt(n)
            freq_ci_lower = sample_mean - 1.96 * se
            freq_ci_upper = sample_mean + 1.96 * se
            
            # è´å¶æ–¯å¯ä¿¡åŒºé—´ï¼ˆå‡è®¾å·²çŸ¥æ–¹å·®ï¼‰
            prior_mean = 0
            prior_var = 100
            posterior_var = 1 / (1/prior_var + n/true_std**2)
            posterior_mean = posterior_var * (prior_mean/prior_var + n*sample_mean/true_std**2)
            posterior_std = np.sqrt(posterior_var)
            bayes_ci_lower = posterior_mean - 1.96 * posterior_std
            bayes_ci_upper = posterior_mean + 1.96 * posterior_std
            
            # ç»˜åˆ¶ç»“æœ
            x_pos = [1, 2]
            estimates = [sample_mean, posterior_mean]
            ci_lowers = [freq_ci_lower, bayes_ci_lower]
            ci_uppers = [freq_ci_upper, bayes_ci_upper]
            colors = ['blue', 'red']
            labels = ['é¢‘ç‡è®º', 'è´å¶æ–¯']
            
            for j in range(2):
                ax.errorbar(x_pos[j], estimates[j], 
                           yerr=[[estimates[j] - ci_lowers[j]], [ci_uppers[j] - estimates[j]]],
                           fmt='o', color=colors[j], capsize=5, capthick=2, label=labels[j])
            
            ax.axhline(true_mean, color='green', linestyle='--', linewidth=2, label='çœŸå®å€¼')
            ax.set_xlim(0.5, 2.5)
            ax.set_xticks(x_pos)
            ax.set_xticklabels(labels)
            ax.set_ylabel('ä¼°è®¡å€¼')
            ax.set_title(f'æ ·æœ¬å¤§å° n={n}')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

class BayesianLinearRegression:
    """è´å¶æ–¯çº¿æ€§å›å½’"""
    
    def __init__(self):
        pass
    
    def bayesian_regression_theory(self):
        """è´å¶æ–¯å›å½’ç†è®º"""
        print("=== è´å¶æ–¯çº¿æ€§å›å½’ç†è®º ===")
        
        print("æ¨¡å‹å‡è®¾:")
        print("y = Xw + Îµ")
        print("å…¶ä¸­ Îµ ~ N(0, ÏƒÂ²I)")
        print()
        
        print("è´å¶æ–¯å¤„ç†:")
        print("1. æƒé‡å…ˆéªŒ: w ~ N(Î¼â‚€, Î£â‚€)")
        print("2. ä¼¼ç„¶å‡½æ•°: p(y|X,w,ÏƒÂ²) = N(Xw, ÏƒÂ²I)")
        print("3. åéªŒåˆ†å¸ƒ: p(w|X,y,ÏƒÂ²) = N(Î¼â‚™, Î£â‚™)")
        print()
        
        print("åéªŒå‚æ•°:")
        print("Î£â‚™ = (Î£â‚€â»Â¹ + Ïƒâ»Â²Xáµ€X)â»Â¹")
        print("Î¼â‚™ = Î£â‚™(Î£â‚€â»Â¹Î¼â‚€ + Ïƒâ»Â²Xáµ€y)")
        print()
        
        print("é¢„æµ‹åˆ†å¸ƒ:")
        print("p(y*|x*,X,y) = N(Î¼â‚™áµ€x*, x*áµ€Î£â‚™x* + ÏƒÂ²)")
        print("åŒ…å«å‚æ•°ä¸ç¡®å®šæ€§å’Œè§‚æµ‹å™ªå£°")
        
        # å®ç°å’Œå¯è§†åŒ–
        self.implement_bayesian_regression()
        
        return self.compare_with_frequentist()
    
    def implement_bayesian_regression(self):
        """å®ç°è´å¶æ–¯çº¿æ€§å›å½’"""
        print("\n=== è´å¶æ–¯çº¿æ€§å›å½’å®ç° ===")
        
        # ç”Ÿæˆæ•°æ®
        np.random.seed(42)
        n_samples = 50
        X = np.linspace(-3, 3, n_samples).reshape(-1, 1)
        true_w = [1.5, -0.3]  # çœŸå®æƒé‡ [æˆªè·, æ–œç‡]
        noise_std = 0.3
        
        # æ·»åŠ åç½®é¡¹
        X_design = np.column_stack([np.ones(n_samples), X.flatten()])
        y = X_design @ true_w + np.random.normal(0, noise_std, n_samples)
        
        # è´å¶æ–¯çº¿æ€§å›å½’
        # å…ˆéªŒå‚æ•°
        prior_mean = np.zeros(2)
        prior_cov = np.eye(2) * 5  # è¾ƒå®½æ¾çš„å…ˆéªŒ
        noise_precision = 1 / noise_std**2
        
        # åéªŒå‚æ•°
        posterior_precision = np.linalg.inv(prior_cov) + noise_precision * X_design.T @ X_design
        posterior_cov = np.linalg.inv(posterior_precision)
        posterior_mean = posterior_cov @ (np.linalg.inv(prior_cov) @ prior_mean + 
                                        noise_precision * X_design.T @ y)
        
        print(f"çœŸå®æƒé‡: {true_w}")
        print(f"åéªŒå‡å€¼: {posterior_mean}")
        print(f"åéªŒåæ–¹å·®å¯¹è§’çº¿: {np.diag(posterior_cov)}")
        
        # å¯è§†åŒ–
        self.visualize_bayesian_regression(X, y, X_design, posterior_mean, posterior_cov, noise_std)
        
        return posterior_mean, posterior_cov
    
    def visualize_bayesian_regression(self, X, y, X_design, posterior_mean, posterior_cov, noise_std):
        """å¯è§†åŒ–è´å¶æ–¯å›å½’"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. æ•°æ®å’Œé¢„æµ‹
        ax1 = axes[0, 0]
        ax1.scatter(X, y, alpha=0.7, color='blue', label='è§‚æµ‹æ•°æ®')
        
        # é¢„æµ‹
        X_test = np.linspace(-4, 4, 100).reshape(-1, 1)
        X_test_design = np.column_stack([np.ones(len(X_test)), X_test.flatten()])
        
        # é¢„æµ‹å‡å€¼
        y_pred_mean = X_test_design @ posterior_mean
        
        # é¢„æµ‹ä¸ç¡®å®šæ€§
        y_pred_var = []
        for i in range(len(X_test_design)):
            x_test = X_test_design[i]
            var = x_test @ posterior_cov @ x_test + noise_std**2
            y_pred_var.append(var)
        
        y_pred_std = np.sqrt(y_pred_var)
        
        ax1.plot(X_test, y_pred_mean, 'r-', linewidth=2, label='é¢„æµ‹å‡å€¼')
        ax1.fill_between(X_test.flatten(), 
                        y_pred_mean - 2*y_pred_std,
                        y_pred_mean + 2*y_pred_std,
                        alpha=0.3, color='red', label='95%é¢„æµ‹åŒºé—´')
        
        ax1.set_xlabel('x')
        ax1.set_ylabel('y')
        ax1.set_title('è´å¶æ–¯çº¿æ€§å›å½’')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æƒé‡åéªŒåˆ†å¸ƒ
        ax2 = axes[0, 1]
        n_samples = 1000
        weight_samples = np.random.multivariate_normal(posterior_mean, posterior_cov, n_samples)
        
        ax2.scatter(weight_samples[:, 0], weight_samples[:, 1], alpha=0.5, s=10, color='blue')
        ax2.scatter(posterior_mean[0], posterior_mean[1], color='red', s=100, 
                   marker='x', linewidth=3, label='åéªŒå‡å€¼')
        ax2.set_xlabel('wâ‚€ (æˆªè·)')
        ax2.set_ylabel('wâ‚ (æ–œç‡)')
        ax2.set_title('æƒé‡å‚æ•°åéªŒåˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å¤šæ¡é¢„æµ‹çº¿ï¼ˆå‚æ•°ä¸ç¡®å®šæ€§ï¼‰
        ax3 = axes[1, 0]
        ax3.scatter(X, y, alpha=0.7, color='blue', label='è§‚æµ‹æ•°æ®')
        
        # ä»åéªŒé‡‡æ ·å¤šç»„æƒé‡å¹¶ç»˜åˆ¶é¢„æµ‹çº¿
        for i in range(20):
            w_sample = weight_samples[i]
            y_sample = X_test_design @ w_sample
            ax3.plot(X_test, y_sample, 'r-', alpha=0.3, linewidth=1)
        
        ax3.plot(X_test, y_pred_mean, 'black', linewidth=2, label='å¹³å‡é¢„æµ‹')
        ax3.set_xlabel('x')
        ax3.set_ylabel('y')
        ax3.set_title('å‚æ•°ä¸ç¡®å®šæ€§å¯è§†åŒ–')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. é¢„æµ‹ä¸ç¡®å®šæ€§åˆ†è§£
        ax4 = axes[1, 1]
        
        # åˆ†è§£ä¸ç¡®å®šæ€§
        epistemic_var = []  # è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆå‚æ•°ä¸ç¡®å®šæ€§ï¼‰
        aleatoric_var = noise_std**2  # å¶ç„¶ä¸ç¡®å®šæ€§ï¼ˆè§‚æµ‹å™ªå£°ï¼‰
        
        for i in range(len(X_test_design)):
            x_test = X_test_design[i]
            epi_var = x_test @ posterior_cov @ x_test
            epistemic_var.append(epi_var)
        
        epistemic_std = np.sqrt(epistemic_var)
        aleatoric_std = np.sqrt(aleatoric_var)
        total_std = np.sqrt(np.array(epistemic_var) + aleatoric_var)
        
        ax4.plot(X_test, epistemic_std, label='è®¤çŸ¥ä¸ç¡®å®šæ€§', linewidth=2)
        ax4.axhline(aleatoric_std, color='red', linestyle='--', 
                   label=f'å¶ç„¶ä¸ç¡®å®šæ€§={aleatoric_std:.2f}')
        ax4.plot(X_test, total_std, label='æ€»ä¸ç¡®å®šæ€§', linewidth=2, color='black')
        
        ax4.set_xlabel('x')
        ax4.set_ylabel('é¢„æµ‹æ ‡å‡†å·®')
        ax4.set_title('ä¸ç¡®å®šæ€§åˆ†è§£')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def compare_with_frequentist(self):
        """ä¸é¢‘ç‡è®ºæ–¹æ³•æ¯”è¾ƒ"""
        print("\n=== è´å¶æ–¯ vs é¢‘ç‡è®ºå›å½’æ¯”è¾ƒ ===")
        
        comparison = {
            "å‚æ•°ä¼°è®¡": {
                "é¢‘ç‡è®º": "ç‚¹ä¼°è®¡ (MLE/OLS)",
                "è´å¶æ–¯": "åˆ†å¸ƒä¼°è®¡ (åéªŒåˆ†å¸ƒ)",
                "ä¼˜åŠ¿": "è´å¶æ–¯æä¾›ä¸ç¡®å®šæ€§ä¿¡æ¯"
            },
            "æ­£åˆ™åŒ–": {
                "é¢‘ç‡è®º": "æ˜¾å¼æ­£åˆ™åŒ– (Ridge, Lasso)",
                "è´å¶æ–¯": "éšå¼æ­£åˆ™åŒ– (å…ˆéªŒåˆ†å¸ƒ)",
                "è¿æ¥": "Ridgeå›å½’ â‰ˆ é«˜æ–¯å…ˆéªŒ"
            },
            "é¢„æµ‹": {
                "é¢‘ç‡è®º": "ç‚¹é¢„æµ‹ + ç½®ä¿¡åŒºé—´",
                "è´å¶æ–¯": "é¢„æµ‹åˆ†å¸ƒ",
                "ä¸ç¡®å®šæ€§": "è´å¶æ–¯è‡ªç„¶åŒ…å«å‚æ•°ä¸ç¡®å®šæ€§"
            },
            "æ¨¡å‹é€‰æ‹©": {
                "é¢‘ç‡è®º": "äº¤å‰éªŒè¯ã€ä¿¡æ¯å‡†åˆ™",
                "è´å¶æ–¯": "æ¨¡å‹è¯æ®ã€è´å¶æ–¯å› å­",
                "è‡ªåŠ¨æ€§": "è´å¶æ–¯è‡ªåŠ¨è¿›è¡Œæ¨¡å‹å¹³å‡"
            }
        }
        
        for aspect, details in comparison.items():
            print(f"{aspect}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        return comparison

class VariationalInference:
    """å˜åˆ†æ¨æ–­"""
    
    def __init__(self):
        pass
    
    def variational_theory(self):
        """å˜åˆ†æ¨æ–­ç†è®º"""
        print("=== å˜åˆ†æ¨æ–­ç†è®º ===")
        
        print("æ ¸å¿ƒæ€æƒ³:")
        print("- åéªŒåˆ†å¸ƒp(Î¸|D)é€šå¸¸éš¾ä»¥è®¡ç®—")
        print("- ç”¨ç®€å•åˆ†å¸ƒq(Î¸)è¿‘ä¼¼åéªŒåˆ†å¸ƒ")
        print("- æœ€å°åŒ–KLæ•£åº¦: KL(q(Î¸)||p(Î¸|D))")
        print()
        
        print("å˜åˆ†ä¸‹ç•Œ (ELBO):")
        print("log p(D) = ELBO + KL(q(Î¸)||p(Î¸|D))")
        print("ELBO = E_q[log p(D|Î¸)] - KL(q(Î¸)||p(Î¸))")
        print("     = é‡æ„é¡¹ - æ­£åˆ™åŒ–é¡¹")
        print()
        
        print("å˜åˆ†æ—é€‰æ‹©:")
        variational_families = {
            "å¹³å‡åœº": {
                "å‡è®¾": "q(Î¸) = âˆáµ¢ qáµ¢(Î¸áµ¢)",
                "ä¼˜ç‚¹": "è®¡ç®—ç®€å•ï¼Œæ˜“äºå®ç°",
                "ç¼ºç‚¹": "å¿½ç•¥å‚æ•°é—´ç›¸å…³æ€§",
                "åº”ç”¨": "å¤§å¤šæ•°å˜åˆ†æ–¹æ³•çš„åŸºç¡€"
            },
            "ç»“æ„åŒ–å˜åˆ†": {
                "å‡è®¾": "ä¿ç•™æŸäº›ç›¸å…³æ€§ç»“æ„",
                "ä¾‹å­": "åˆ†ç»„ç‹¬ç«‹ã€é©¬å°”ç§‘å¤«ç»“æ„",
                "æƒè¡¡": "è¡¨è¾¾èƒ½åŠ› vs è®¡ç®—å¤æ‚åº¦",
                "åº”ç”¨": "æ—¶åºæ¨¡å‹ã€ç©ºé—´æ¨¡å‹"
            },
            "ç¥ç»å˜åˆ†": {
                "å‡è®¾": "ç”¨ç¥ç»ç½‘ç»œå‚æ•°åŒ–q(Î¸)",
                "ä¼˜ç‚¹": "è¡¨è¾¾èƒ½åŠ›å¼º",
                "ç¼ºç‚¹": "è®¡ç®—å¤æ‚ï¼Œå±€éƒ¨æœ€ä¼˜",
                "åº”ç”¨": "æ·±åº¦å­¦ä¹ ã€å¤æ‚æ¨¡å‹"
            }
        }
        
        for family, details in variational_families.items():
            print(f"{family}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # å®ç°ç®€å•çš„å˜åˆ†æ¨æ–­
        self.implement_variational_inference()
        
        return variational_families
    
    def implement_variational_inference(self):
        """å®ç°å˜åˆ†æ¨æ–­ç¤ºä¾‹"""
        print("=== å˜åˆ†æ¨æ–­ç¤ºä¾‹ï¼šè´å¶æ–¯çº¿æ€§å›å½’ ===")
        
        # ç”Ÿæˆæ•°æ®
        np.random.seed(42)
        n_samples = 100
        X = np.random.randn(n_samples, 2)
        true_w = np.array([1.5, -0.8])
        noise_std = 0.2
        y = X @ true_w + np.random.normal(0, noise_std, n_samples)
        
        # å˜åˆ†æ¨æ–­
        # å‡è®¾å˜åˆ†åéªŒä¸ºé«˜æ–¯åˆ†å¸ƒï¼šq(w) = N(Î¼, Î£)
        
        def elbo(params):
            """è®¡ç®—ELBO"""
            mu = params[:2]
            log_sigma = params[2:4]
            sigma = np.exp(log_sigma)
            
            # å…ˆéªŒå‚æ•°
            prior_mean = np.zeros(2)
            prior_std = 2.0
            
            # é‡æ„é¡¹ï¼šE_q[log p(y|X,w)]
            reconstruction = 0
            n_mc_samples = 100
            for _ in range(n_mc_samples):
                w_sample = mu + sigma * np.random.randn(2)
                y_pred = X @ w_sample
                reconstruction += -0.5 * np.sum((y - y_pred)**2) / noise_std**2
            reconstruction /= n_mc_samples
            
            # KLé¡¹ï¼šKL(q(w)||p(w))
            kl_term = 0.5 * (np.sum(sigma**2 / prior_std**2) + 
                             np.sum((mu - prior_mean)**2 / prior_std**2) -
                             len(mu) - 2 * np.sum(log_sigma) + 
                             len(mu) * np.log(prior_std**2))
            
            return -(reconstruction - kl_term)  # è´Ÿå·å› ä¸ºè¦æœ€å°åŒ–
        
        # ä¼˜åŒ–
        initial_params = np.array([0.0, 0.0, 0.0, 0.0])
        result = minimize(elbo, initial_params, method='L-BFGS-B')
        
        # æå–ç»“æœ
        mu_opt = result.x[:2]
        sigma_opt = np.exp(result.x[2:4])
        
        print(f"çœŸå®æƒé‡: {true_w}")
        print(f"å˜åˆ†åéªŒå‡å€¼: {mu_opt}")
        print(f"å˜åˆ†åéªŒæ ‡å‡†å·®: {sigma_opt}")
        
        # å¯è§†åŒ–ç»“æœ
        self.visualize_variational_inference(true_w, mu_opt, sigma_opt)
        
        return mu_opt, sigma_opt
    
    def visualize_variational_inference(self, true_w, mu_opt, sigma_opt):
        """å¯è§†åŒ–å˜åˆ†æ¨æ–­ç»“æœ"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # 1. æƒé‡åˆ†å¸ƒæ¯”è¾ƒ
        ax1 = axes[0]
        
        # å˜åˆ†åéªŒæ ·æœ¬
        n_samples = 1000
        w_samples = mu_opt[:, np.newaxis] + sigma_opt[:, np.newaxis] * np.random.randn(2, n_samples)
        
        ax1.scatter(w_samples[0], w_samples[1], alpha=0.5, s=10, label='å˜åˆ†åéªŒæ ·æœ¬')
        ax1.scatter(mu_opt[0], mu_opt[1], color='red', s=100, marker='x', 
                   linewidth=3, label='å˜åˆ†åéªŒå‡å€¼')
        ax1.scatter(true_w[0], true_w[1], color='green', s=100, marker='o',
                   label='çœŸå®æƒé‡')
        
        ax1.set_xlabel('wâ‚')
        ax1.set_ylabel('wâ‚‚')
        ax1.set_title('æƒé‡åˆ†å¸ƒ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ELBOæ”¶æ•›æ›²çº¿ï¼ˆæ¨¡æ‹Ÿï¼‰
        ax2 = axes[1]
        iterations = np.arange(1, 101)
        elbo_values = -1000 * np.exp(-iterations/20) + np.random.normal(0, 10, 100)
        
        ax2.plot(iterations, elbo_values, 'b-', linewidth=2)
        ax2.set_xlabel('è¿­ä»£æ¬¡æ•°')
        ax2.set_ylabel('ELBO')
        ax2.set_title('ELBOæ”¶æ•›æ›²çº¿')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

class BayesianNeuralNetworks:
    """è´å¶æ–¯ç¥ç»ç½‘ç»œ"""
    
    def __init__(self):
        pass
    
    def bnn_theory(self):
        """è´å¶æ–¯ç¥ç»ç½‘ç»œç†è®º"""
        print("=== è´å¶æ–¯ç¥ç»ç½‘ç»œç†è®º ===")
        
        print("æ ¸å¿ƒæ€æƒ³:")
        print("- å°†ç¥ç»ç½‘ç»œæƒé‡è§†ä¸ºéšæœºå˜é‡")
        print("- å­¦ä¹ æƒé‡çš„åˆ†å¸ƒè€Œéç‚¹ä¼°è®¡")
        print("- é¢„æµ‹æ—¶è€ƒè™‘æƒé‡ä¸ç¡®å®šæ€§")
        print()
        
        print("æŒ‘æˆ˜:")
        challenges = {
            "é«˜ç»´åéªŒ": {
                "é—®é¢˜": "ç¥ç»ç½‘ç»œå‚æ•°æ•°é‡å·¨å¤§",
                "å›°éš¾": "ç²¾ç¡®åéªŒæ¨æ–­ä¸å¯è¡Œ",
                "è§£å†³": "å˜åˆ†æ¨æ–­ã€é‡‡æ ·æ–¹æ³•"
            },
            "è®¡ç®—å¤æ‚åº¦": {
                "é—®é¢˜": "è®­ç»ƒå’Œæ¨æ–­æˆæœ¬é«˜",
                "åŸå› ": "éœ€è¦å¤„ç†å‚æ•°åˆ†å¸ƒ",
                "ç¼“è§£": "è¿‘ä¼¼æ–¹æ³•ã€ç½‘ç»œå‰ªæ"
            },
            "å…ˆéªŒé€‰æ‹©": {
                "é—®é¢˜": "å¦‚ä½•é€‰æ‹©åˆé€‚çš„æƒé‡å…ˆéªŒ",
                "å½±å“": "å½±å“æ­£åˆ™åŒ–æ•ˆæœ",
                "æ–¹æ³•": "åˆ†å±‚è´å¶æ–¯ã€ç»éªŒè´å¶æ–¯"
            },
            "ä¸ç¡®å®šæ€§æ ¡å‡†": {
                "é—®é¢˜": "é¢„æµ‹ä¸ç¡®å®šæ€§æ˜¯å¦å‡†ç¡®",
                "è¯„ä¼°": "æ ¡å‡†å›¾ã€å¯é æ€§åˆ†æ",
                "é‡è¦æ€§": "å®‰å…¨å…³é”®åº”ç”¨"
            }
        }
        
        for challenge, details in challenges.items():
            print(f"{challenge}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # ä»‹ç»ä¸»è¦æ–¹æ³•
        self.bnn_methods()
        
        return challenges
    
    def bnn_methods(self):
        """BNNä¸»è¦æ–¹æ³•"""
        print("=== BNNä¸»è¦æ–¹æ³• ===")
        
        methods = {
            "å˜åˆ†è´å¶æ–¯": {
                "æ€æƒ³": "ç”¨ç®€å•åˆ†å¸ƒè¿‘ä¼¼æƒé‡åéªŒ",
                "å®ç°": "Bayes by Backprop",
                "ä¼˜ç‚¹": "è®­ç»ƒç›¸å¯¹ç®€å•",
                "ç¼ºç‚¹": "åéªŒè¿‘ä¼¼è´¨é‡æœ‰é™"
            },
            "MC Dropout": {
                "æ€æƒ³": "Dropoutè¿‘ä¼¼è´å¶æ–¯æ¨æ–­",
                "åŸç†": "éšæœºå¤±æ´»ç­‰ä»·äºå˜åˆ†æ¨æ–­",
                "ä¼˜ç‚¹": "æ˜“äºå®ç°ï¼Œè®¡ç®—é«˜æ•ˆ",
                "å±€é™": "è¿‘ä¼¼è´¨é‡æœ‰äº‰è®®"
            },
            "æ·±åº¦é›†æˆ": {
                "æ€æƒ³": "è®­ç»ƒå¤šä¸ªç¥ç»ç½‘ç»œ",
                "é¢„æµ‹": "é›†æˆé¢„æµ‹è·å¾—ä¸ç¡®å®šæ€§",
                "ä¼˜ç‚¹": "å®ç”¨æœ‰æ•ˆ",
                "ç¼ºç‚¹": "è®¡ç®—æˆæœ¬é«˜"
            },
            "æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼": {
                "æ€æƒ³": "åéªŒåˆ†å¸ƒçš„äºŒé˜¶è¿‘ä¼¼",
                "è®¡ç®—": "åŸºäºHessiançŸ©é˜µ",
                "ä¼˜ç‚¹": "ç†è®ºåŸºç¡€å¼º",
                "ç¼ºç‚¹": "Hessianè®¡ç®—å›°éš¾"
            },
            "MCMC": {
                "æ€æƒ³": "é‡‡æ ·è·å¾—åéªŒæ ·æœ¬",
                "æ–¹æ³•": "HMCã€SGLDç­‰",
                "ä¼˜ç‚¹": "ç†è®ºä¿è¯",
                "ç¼ºç‚¹": "è®¡ç®—æˆæœ¬æé«˜"
            }
        }
        
        for method, details in methods.items():
            print(f"{method}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # å¯è§†åŒ–æ–¹æ³•æ¯”è¾ƒ
        self.visualize_bnn_methods()
        
        return methods
    
    def visualize_bnn_methods(self):
        """å¯è§†åŒ–BNNæ–¹æ³•æ¯”è¾ƒ"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. æ–¹æ³•æ€§èƒ½æ¯”è¾ƒ
        methods = ['å˜åˆ†è´å¶æ–¯', 'MC Dropout', 'æ·±åº¦é›†æˆ', 'MCMC']
        accuracy = [85, 83, 87, 88]
        uncertainty_quality = [70, 60, 80, 95]
        computational_cost = [60, 20, 80, 100]
        
        x = np.arange(len(methods))
        width = 0.25
        
        bars1 = axes[0, 0].bar(x - width, accuracy, width, label='é¢„æµ‹å‡†ç¡®ç‡', alpha=0.7)
        bars2 = axes[0, 0].bar(x, uncertainty_quality, width, label='ä¸ç¡®å®šæ€§è´¨é‡', alpha=0.7)
        bars3 = axes[0, 0].bar(x + width, computational_cost, width, label='è®¡ç®—æˆæœ¬', alpha=0.7)
        
        axes[0, 0].set_xlabel('æ–¹æ³•')
        axes[0, 0].set_ylabel('è¯„åˆ†')
        axes[0, 0].set_title('BNNæ–¹æ³•æ¯”è¾ƒ')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(methods, rotation=45, ha='right')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ä¸ç¡®å®šæ€§vså‡†ç¡®æ€§
        axes[1, 0].scatter(uncertainty_quality, accuracy, s=100, alpha=0.7)
        for i, method in enumerate(methods):
            axes[1, 0].annotate(method, (uncertainty_quality[i], accuracy[i]),
                               xytext=(5, 5), textcoords='offset points')
        
        axes[1, 0].set_xlabel('ä¸ç¡®å®šæ€§è´¨é‡')
        axes[1, 0].set_ylabel('é¢„æµ‹å‡†ç¡®ç‡')
        axes[1, 0].set_title('ä¸ç¡®å®šæ€§è´¨é‡ vs é¢„æµ‹å‡†ç¡®ç‡')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 3. è®­ç»ƒæ—¶é—´vsæ€§èƒ½
        training_time = [100, 30, 300, 1000]  # ç›¸å¯¹è®­ç»ƒæ—¶é—´
        
        axes[0, 1].scatter(training_time, accuracy, s=100, alpha=0.7, color='red')
        for i, method in enumerate(methods):
            axes[0, 1].annotate(method, (training_time[i], accuracy[i]),
                               xytext=(5, 5), textcoords='offset points')
        
        axes[0, 1].set_xlabel('è®­ç»ƒæ—¶é—´ (ç›¸å¯¹)')
        axes[0, 1].set_ylabel('é¢„æµ‹å‡†ç¡®ç‡')
        axes[0, 1].set_title('è®­ç»ƒæ—¶é—´ vs æ€§èƒ½')
        axes[0, 1].set_xscale('log')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 4. åº”ç”¨åœºæ™¯é›·è¾¾å›¾
        applications = ['è®¡ç®—æœºè§†è§‰', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'åŒ»ç–—è¯Šæ–­', 'è‡ªåŠ¨é©¾é©¶', 'é‡‘èé¢„æµ‹']
        n_apps = len(applications)
        
        # ä¸åŒæ–¹æ³•åœ¨å„åº”ç”¨åœºæ™¯çš„é€‚ç”¨æ€§
        variational_scores = [3, 4, 3, 2, 4]
        dropout_scores = [4, 4, 2, 2, 3]
        ensemble_scores = [5, 4, 4, 4, 4]
        mcmc_scores = [2, 2, 5, 1, 3]
        
        angles = np.linspace(0, 2*np.pi, n_apps, endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        ax_radar = plt.subplot(2, 2, 4, projection='polar')
        
        for scores, label, color in zip([variational_scores, dropout_scores, ensemble_scores, mcmc_scores],
                                       ['å˜åˆ†è´å¶æ–¯', 'MC Dropout', 'æ·±åº¦é›†æˆ', 'MCMC'],
                                       ['blue', 'green', 'red', 'orange']):
            scores += scores[:1]  # é—­åˆå›¾å½¢
            ax_radar.plot(angles, scores, 'o-', linewidth=2, label=label, color=color)
            ax_radar.fill(angles, scores, alpha=0.1, color=color)
        
        ax_radar.set_xticks(angles[:-1])
        ax_radar.set_xticklabels(applications)
        ax_radar.set_ylim(0, 5)
        ax_radar.set_title('åº”ç”¨åœºæ™¯é€‚ç”¨æ€§')
        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.2, 1))
        
        plt.tight_layout()
        plt.show()

def comprehensive_bayesian_ml_summary():
    """è´å¶æ–¯æœºå™¨å­¦ä¹ ç»¼åˆæ€»ç»“"""
    print("=== è´å¶æ–¯æœºå™¨å­¦ä¹ ç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "æ ¸å¿ƒç†å¿µ": {
            "æ¦‚ç‡å»ºæ¨¡": "ä¸€åˆ‡çš†æ¦‚ç‡åˆ†å¸ƒ",
            "ä¸ç¡®å®šæ€§": "å‚æ•°å’Œé¢„æµ‹çš„ä¸ç¡®å®šæ€§é‡åŒ–",
            "å…ˆéªŒçŸ¥è¯†": "è‡ªç„¶èåˆé¢†åŸŸçŸ¥è¯†",
            "ä¸€è‡´æ€§": "ç†è®ºæ¡†æ¶çš„æ•°å­¦ä¸€è‡´æ€§"
        },
        
        "ä¸»è¦æ–¹æ³•": {
            "ç²¾ç¡®æ¨æ–­": "å…±è½­å…ˆéªŒã€è§£æè§£",
            "è¿‘ä¼¼æ¨æ–­": "å˜åˆ†æ¨æ–­ã€æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼",
            "é‡‡æ ·æ–¹æ³•": "MCMCã€é‡è¦æ€§é‡‡æ ·",
            "æ·±åº¦é›†æˆ": "å¤šæ¨¡å‹é›†æˆè·å¾—ä¸ç¡®å®šæ€§"
        },
        
        "åº”ç”¨é¢†åŸŸ": {
            "å›å½’åˆ†æ": "è´å¶æ–¯çº¿æ€§/éçº¿æ€§å›å½’",
            "åˆ†ç±»é—®é¢˜": "è´å¶æ–¯é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯",
            "ç¥ç»ç½‘ç»œ": "è´å¶æ–¯ç¥ç»ç½‘ç»œã€ä¸ç¡®å®šæ€§é‡åŒ–",
            "é«˜æ–¯è¿‡ç¨‹": "éå‚æ•°è´å¶æ–¯ã€å‡½æ•°åˆ†å¸ƒ"
        },
        
        "ä¼˜åŠ¿ç‰¹ç‚¹": {
            "ä¸ç¡®å®šæ€§": "è‡ªç„¶çš„ä¸ç¡®å®šæ€§é‡åŒ–",
            "æ­£åˆ™åŒ–": "å…ˆéªŒä½œä¸ºè‡ªç„¶æ­£åˆ™åŒ–",
            "å°æ ·æœ¬": "å…ˆéªŒä¿¡æ¯åœ¨å°æ ·æœ¬æ—¶æœ‰æ•ˆ",
            "æ¨¡å‹é€‰æ‹©": "è´å¶æ–¯å› å­ã€æ¨¡å‹å¹³å‡"
        },
        
        "è®¡ç®—æŒ‘æˆ˜": {
            "ç§¯åˆ†å›°éš¾": "é«˜ç»´ç§¯åˆ†é€šå¸¸æ— è§£æè§£",
            "è®¡ç®—å¤æ‚": "é‡‡æ ·å’Œå˜åˆ†æ¨æ–­æˆæœ¬é«˜",
            "è¿‘ä¼¼è´¨é‡": "è¿‘ä¼¼æ–¹æ³•çš„å‡†ç¡®æ€§é—®é¢˜",
            "æ”¶æ•›åˆ¤æ–­": "MCMCæ”¶æ•›è¯Šæ–­"
        },
        
        "ç°ä»£å‘å±•": {
            "å˜åˆ†è‡ªç¼–ç å™¨": "VAEä¸­çš„å˜åˆ†æ¨æ–­",
            "ç¥ç»åéªŒä¼°è®¡": "ç¥ç»ç½‘ç»œè¿‘ä¼¼åéªŒ",
            "æ¦‚ç‡ç¼–ç¨‹": "Stanã€PyMC3ç­‰å·¥å…·",
            "è¿‘ä¼¼è´å¶æ–¯è®¡ç®—": "ABCæ–¹æ³•"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("è´å¶æ–¯æœºå™¨å­¦ä¹ ç†è®ºæŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Murphy (2012): "Machine Learning: A Probabilistic Perspective"
- Bishop (2006): "Pattern Recognition and Machine Learning"
- Gelman et al. (2013): "Bayesian Data Analysis"
- Blei et al. (2017): "Variational Inference: A Review for Statisticians"
- MacKay (1992): "A Practical Bayesian Framework for Backpropagation Networks"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [é«˜æ–¯è¿‡ç¨‹](gaussian_processes.md) - éå‚æ•°è´å¶æ–¯æ–¹æ³•
- [å˜åˆ†è‡ªç¼–ç å™¨](../generative_models.md) - æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­çš„å˜åˆ†æ¨æ–­
- [æ¦‚ç‡ç¼–ç¨‹](probabilistic_programming.md) - ç°ä»£è´å¶æ–¯å»ºæ¨¡å·¥å…·