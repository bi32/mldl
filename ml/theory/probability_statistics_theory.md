# æ¦‚ç‡ç»Ÿè®¡ç†è®ºï¼šæœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ ğŸ“Š

æ·±å…¥ç†è§£æœºå™¨å­¦ä¹ ä¸­çš„æ¦‚ç‡ç»Ÿè®¡ç†è®ºï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§åº”ç”¨ã€‚

## 1. æ¦‚ç‡è®ºåŸºç¡€ ğŸ²

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.naive_bayes import GaussianNB
from sklearn.mixture import GaussianMixture
import warnings
warnings.filterwarnings('ignore')

class ProbabilityFoundations:
    """æ¦‚ç‡è®ºåŸºç¡€"""
    
    def __init__(self):
        self.distributions = {}
    
    def probability_axioms(self):
        """æ¦‚ç‡å…¬ç†"""
        print("=== æ¦‚ç‡å…¬ç† (Kolmogorov Axioms) ===")
        
        axioms = {
            "éè´Ÿæ€§": {
                "å…¬å¼": "P(A) â‰¥ 0",
                "å«ä¹‰": "ä»»ä½•äº‹ä»¶çš„æ¦‚ç‡éƒ½éè´Ÿ",
                "MLåº”ç”¨": "æ¦‚ç‡é¢„æµ‹ã€åˆ†ç±»å™¨è¾“å‡º"
            },
            "è§„èŒƒæ€§": {
                "å…¬å¼": "P(Î©) = 1",
                "å«ä¹‰": "æ ·æœ¬ç©ºé—´çš„æ¦‚ç‡ä¸º1",
                "MLåº”ç”¨": "æ¦‚ç‡åˆ†å¸ƒå½’ä¸€åŒ–"
            },
            "å¯åŠ æ€§": {
                "å…¬å¼": "P(Aâ‚ âˆª Aâ‚‚ âˆª ...) = P(Aâ‚) + P(Aâ‚‚) + ...",
                "å«ä¹‰": "äº’æ–¥äº‹ä»¶æ¦‚ç‡å¯åŠ ",
                "MLåº”ç”¨": "å¤šç±»åˆ†ç±»æ¦‚ç‡æ±‚å’Œ"
            }
        }
        
        for axiom, details in axioms.items():
            print(f"\n{axiom}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # æ¦‚ç‡å¯è§†åŒ–æ¼”ç¤º
        self.demonstrate_probability_axioms()
        
        return axioms
    
    def demonstrate_probability_axioms(self):
        """æ¼”ç¤ºæ¦‚ç‡å…¬ç†"""
        # æŠ•æ·éª°å­çš„æ¦‚ç‡åˆ†å¸ƒ
        outcomes = np.arange(1, 7)
        probabilities = np.ones(6) / 6
        
        plt.figure(figsize=(15, 5))
        
        # å…¬ç†1: éè´Ÿæ€§
        plt.subplot(1, 3, 1)
        plt.bar(outcomes, probabilities, alpha=0.7, color='blue')
        plt.title('å…¬ç†1: éè´Ÿæ€§\nP(X) â‰¥ 0')
        plt.xlabel('éª°å­ç‚¹æ•°')
        plt.ylabel('æ¦‚ç‡')
        plt.ylim(0, 0.3)
        
        # å…¬ç†2: è§„èŒƒæ€§
        plt.subplot(1, 3, 2)
        plt.bar(outcomes, probabilities, alpha=0.7, color='green')
        plt.axhline(y=probabilities.sum(), color='red', linestyle='--', 
                   label=f'æ€»æ¦‚ç‡ = {probabilities.sum():.1f}')
        plt.title('å…¬ç†2: è§„èŒƒæ€§\nÎ£P(X) = 1')
        plt.xlabel('éª°å­ç‚¹æ•°')
        plt.ylabel('æ¦‚ç‡')
        plt.legend()
        
        # å…¬ç†3: å¯åŠ æ€§
        plt.subplot(1, 3, 3)
        even_prob = probabilities[[1, 3, 5]].sum()  # å¶æ•°æ¦‚ç‡
        odd_prob = probabilities[[0, 2, 4]].sum()   # å¥‡æ•°æ¦‚ç‡
        
        plt.bar(['å¶æ•°', 'å¥‡æ•°'], [even_prob, odd_prob], alpha=0.7, color='orange')
        plt.title('å…¬ç†3: å¯åŠ æ€§\nP(å¶æ•°) + P(å¥‡æ•°) = 1')
        plt.ylabel('æ¦‚ç‡')
        
        plt.tight_layout()
        plt.show()
    
    def conditional_probability(self):
        """æ¡ä»¶æ¦‚ç‡ä¸è´å¶æ–¯å®šç†"""
        print("=== æ¡ä»¶æ¦‚ç‡ä¸è´å¶æ–¯å®šç† ===")
        
        print("æ¡ä»¶æ¦‚ç‡å…¬å¼:")
        print("P(A|B) = P(A âˆ© B) / P(B)")
        print()
        
        print("è´å¶æ–¯å®šç†:")
        print("P(A|B) = P(B|A) Ã— P(A) / P(B)")
        print("å…¶ä¸­:")
        print("- P(A|B): åéªŒæ¦‚ç‡")
        print("- P(B|A): ä¼¼ç„¶å‡½æ•°")
        print("- P(A): å…ˆéªŒæ¦‚ç‡")
        print("- P(B): è¾¹é™…æ¦‚ç‡")
        print()
        
        # è´å¶æ–¯å®šç†å®é™…åº”ç”¨ï¼šåƒåœ¾é‚®ä»¶åˆ†ç±»
        self.bayesian_spam_classification()
        
        return self.visualize_bayes_theorem()
    
    def bayesian_spam_classification(self):
        """è´å¶æ–¯åƒåœ¾é‚®ä»¶åˆ†ç±»ç¤ºä¾‹"""
        print("=== è´å¶æ–¯åƒåœ¾é‚®ä»¶åˆ†ç±»ç¤ºä¾‹ ===")
        
        # å‡è®¾æ•°æ®
        prob_spam = 0.3  # å…ˆéªŒæ¦‚ç‡ï¼š30%çš„é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶
        prob_ham = 0.7   # å…ˆéªŒæ¦‚ç‡ï¼š70%çš„é‚®ä»¶æ˜¯æ­£å¸¸é‚®ä»¶
        
        # ä¼¼ç„¶æ¦‚ç‡ï¼šåŒ…å«"å…è´¹"è¿™ä¸ªè¯çš„æ¦‚ç‡
        prob_free_given_spam = 0.8   # åƒåœ¾é‚®ä»¶ä¸­åŒ…å«"å…è´¹"çš„æ¦‚ç‡
        prob_free_given_ham = 0.1    # æ­£å¸¸é‚®ä»¶ä¸­åŒ…å«"å…è´¹"çš„æ¦‚ç‡
        
        # è¾¹é™…æ¦‚ç‡ï¼šé‚®ä»¶åŒ…å«"å…è´¹"çš„æ€»æ¦‚ç‡
        prob_free = (prob_free_given_spam * prob_spam + 
                    prob_free_given_ham * prob_ham)
        
        # åéªŒæ¦‚ç‡ï¼šåŒ…å«"å…è´¹"çš„é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶çš„æ¦‚ç‡
        prob_spam_given_free = (prob_free_given_spam * prob_spam) / prob_free
        
        print(f"å…ˆéªŒæ¦‚ç‡ P(åƒåœ¾é‚®ä»¶) = {prob_spam}")
        print(f"ä¼¼ç„¶æ¦‚ç‡ P('å…è´¹'|åƒåœ¾é‚®ä»¶) = {prob_free_given_spam}")
        print(f"è¾¹é™…æ¦‚ç‡ P('å…è´¹') = {prob_free:.3f}")
        print(f"åéªŒæ¦‚ç‡ P(åƒåœ¾é‚®ä»¶|'å…è´¹') = {prob_spam_given_free:.3f}")
        
        # å¯è§†åŒ–è´å¶æ–¯æ›´æ–°è¿‡ç¨‹
        self.visualize_bayesian_update(prob_spam, prob_spam_given_free)
    
    def visualize_bayesian_update(self, prior, posterior):
        """å¯è§†åŒ–è´å¶æ–¯æ›´æ–°"""
        plt.figure(figsize=(10, 6))
        
        categories = ['å…ˆéªŒæ¦‚ç‡\nP(åƒåœ¾é‚®ä»¶)', 'åéªŒæ¦‚ç‡\nP(åƒåœ¾é‚®ä»¶|"å…è´¹")']
        probabilities = [prior, posterior]
        colors = ['lightblue', 'orange']
        
        bars = plt.bar(categories, probabilities, color=colors, alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, prob in zip(bars, probabilities):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{prob:.3f}', ha='center', va='bottom', fontsize=12)
        
        plt.ylabel('æ¦‚ç‡')
        plt.title('è´å¶æ–¯æ›´æ–°ï¼šè§‚å¯Ÿåˆ°è¯æ®åæ¦‚ç‡çš„å˜åŒ–')
        plt.ylim(0, 1)
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def visualize_bayes_theorem(self):
        """å¯è§†åŒ–è´å¶æ–¯å®šç†"""
        print("\n=== è´å¶æ–¯å®šç†å¯è§†åŒ– ===")
        
        # åˆ›å»ºäºŒç»´æ¦‚ç‡åˆ†å¸ƒ
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # è”åˆæ¦‚ç‡åˆ†å¸ƒ
        x = np.linspace(-3, 3, 100)
        y = np.linspace(-3, 3, 100)
        X, Y = np.meshgrid(x, y)
        
        # äºŒå…ƒæ­£æ€åˆ†å¸ƒ
        mean = [0, 0]
        cov = [[1, 0.5], [0.5, 1]]
        rv = stats.multivariate_normal(mean, cov)
        Z = rv.pdf(np.dstack((X, Y)))
        
        # è”åˆåˆ†å¸ƒ
        im1 = axes[0, 0].contourf(X, Y, Z, levels=20, cmap='Blues')
        axes[0, 0].set_title('è”åˆæ¦‚ç‡åˆ†å¸ƒ P(X,Y)')
        axes[0, 0].set_xlabel('X')
        axes[0, 0].set_ylabel('Y')
        
        # è¾¹é™…åˆ†å¸ƒ P(X)
        marginal_x = np.sum(Z, axis=0)
        marginal_x = marginal_x / np.sum(marginal_x)
        axes[0, 1].plot(x, marginal_x, 'b-', linewidth=2)
        axes[0, 1].fill_between(x, marginal_x, alpha=0.3)
        axes[0, 1].set_title('è¾¹é™…åˆ†å¸ƒ P(X)')
        axes[0, 1].set_xlabel('X')
        axes[0, 1].set_ylabel('æ¦‚ç‡å¯†åº¦')
        
        # æ¡ä»¶åˆ†å¸ƒ P(Y|X=0)
        x_idx = np.argmin(np.abs(x))
        conditional_y = Z[:, x_idx]
        conditional_y = conditional_y / np.sum(conditional_y)
        axes[1, 0].plot(y, conditional_y, 'r-', linewidth=2)
        axes[1, 0].fill_between(y, conditional_y, alpha=0.3, color='red')
        axes[1, 0].set_title('æ¡ä»¶åˆ†å¸ƒ P(Y|X=0)')
        axes[1, 0].set_xlabel('Y')
        axes[1, 0].set_ylabel('æ¦‚ç‡å¯†åº¦')
        
        # è´å¶æ–¯å®šç†ç¤ºæ„å›¾
        axes[1, 1].text(0.1, 0.8, 'P(A|B) = P(B|A) Ã— P(A) / P(B)', 
                        fontsize=16, transform=axes[1, 1].transAxes,
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        axes[1, 1].text(0.1, 0.6, 'åéªŒ = ä¼¼ç„¶ Ã— å…ˆéªŒ / è¯æ®', 
                        fontsize=14, transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('è´å¶æ–¯å®šç†')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return mean, cov

class StatisticalDistributions:
    """ç»Ÿè®¡åˆ†å¸ƒ"""
    
    def __init__(self):
        self.distributions = {}
    
    def common_distributions(self):
        """å¸¸è§åˆ†å¸ƒæ—"""
        print("=== æœºå™¨å­¦ä¹ ä¸­çš„å¸¸è§åˆ†å¸ƒ ===")
        
        distributions = {
            "ä¼¯åŠªåˆ©åˆ†å¸ƒ": {
                "å‚æ•°": "p (æˆåŠŸæ¦‚ç‡)",
                "PMF": "P(X=k) = p^k (1-p)^(1-k)",
                "åº”ç”¨": "äºŒåˆ†ç±»ã€ç¡¬å¸æŠ•æ·",
                "MLç”¨é€”": "é€»è¾‘å›å½’è¾“å‡º"
            },
            "äºŒé¡¹åˆ†å¸ƒ": {
                "å‚æ•°": "n (è¯•éªŒæ¬¡æ•°), p (æˆåŠŸæ¦‚ç‡)",
                "PMF": "P(X=k) = C(n,k) p^k (1-p)^(n-k)",
                "åº”ç”¨": "é‡å¤è¯•éªŒæˆåŠŸæ¬¡æ•°",
                "MLç”¨é€”": "åˆ†ç±»å‡†ç¡®ç‡åˆ†æ"
            },
            "æ³Šæ¾åˆ†å¸ƒ": {
                "å‚æ•°": "Î» (å¹³å‡å‘ç”Ÿç‡)",
                "PMF": "P(X=k) = Î»^k e^(-Î») / k!",
                "åº”ç”¨": "å•ä½æ—¶é—´å†…äº‹ä»¶å‘ç”Ÿæ¬¡æ•°",
                "MLç”¨é€”": "è®¡æ•°æ•°æ®å»ºæ¨¡"
            },
            "æ­£æ€åˆ†å¸ƒ": {
                "å‚æ•°": "Î¼ (å‡å€¼), ÏƒÂ² (æ–¹å·®)",
                "PDF": "f(x) = 1/âˆš(2Ï€ÏƒÂ²) exp(-(x-Î¼)Â²/2ÏƒÂ²)",
                "åº”ç”¨": "è¿ç»­æ•°æ®å»ºæ¨¡",
                "MLç”¨é€”": "çº¿æ€§å›å½’è¯¯å·®ã€ç‰¹å¾åˆ†å¸ƒ"
            },
            "æŒ‡æ•°åˆ†å¸ƒ": {
                "å‚æ•°": "Î» (ç‡å‚æ•°)",
                "PDF": "f(x) = Î»e^(-Î»x), x â‰¥ 0",
                "åº”ç”¨": "ç­‰å¾…æ—¶é—´å»ºæ¨¡",
                "MLç”¨é€”": "ç”Ÿå­˜åˆ†æã€å¯é æ€§"
            },
            "Betaåˆ†å¸ƒ": {
                "å‚æ•°": "Î±, Î² (å½¢çŠ¶å‚æ•°)",
                "PDF": "f(x) = x^(Î±-1)(1-x)^(Î²-1) / B(Î±,Î²)",
                "åº”ç”¨": "æ¦‚ç‡çš„æ¦‚ç‡",
                "MLç”¨é€”": "è´å¶æ–¯æ¨æ–­å…ˆéªŒ"
            }
        }
        
        for dist, details in distributions.items():
            print(f"\n{dist}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # å¯è§†åŒ–å¸¸è§åˆ†å¸ƒ
        self.visualize_distributions()
        
        return distributions
    
    def visualize_distributions(self):
        """å¯è§†åŒ–å¸¸è§åˆ†å¸ƒ"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        # 1. ä¼¯åŠªåˆ©åˆ†å¸ƒ
        x_bern = [0, 1]
        p_bern = 0.3
        y_bern = [1-p_bern, p_bern]
        axes[0].bar(x_bern, y_bern, alpha=0.7, color='blue')
        axes[0].set_title(f'ä¼¯åŠªåˆ©åˆ†å¸ƒ (p={p_bern})')
        axes[0].set_xlabel('X')
        axes[0].set_ylabel('P(X)')
        
        # 2. äºŒé¡¹åˆ†å¸ƒ
        n, p = 20, 0.3
        x_binom = np.arange(0, n+1)
        y_binom = stats.binom.pmf(x_binom, n, p)
        axes[1].bar(x_binom, y_binom, alpha=0.7, color='green')
        axes[1].set_title(f'äºŒé¡¹åˆ†å¸ƒ (n={n}, p={p})')
        axes[1].set_xlabel('X')
        axes[1].set_ylabel('P(X)')
        
        # 3. æ³Šæ¾åˆ†å¸ƒ
        lam = 3
        x_poisson = np.arange(0, 15)
        y_poisson = stats.poisson.pmf(x_poisson, lam)
        axes[2].bar(x_poisson, y_poisson, alpha=0.7, color='red')
        axes[2].set_title(f'æ³Šæ¾åˆ†å¸ƒ (Î»={lam})')
        axes[2].set_xlabel('X')
        axes[2].set_ylabel('P(X)')
        
        # 4. æ­£æ€åˆ†å¸ƒ
        x_norm = np.linspace(-4, 4, 100)
        y_norm1 = stats.norm.pdf(x_norm, 0, 1)
        y_norm2 = stats.norm.pdf(x_norm, 0, 0.5)
        axes[3].plot(x_norm, y_norm1, label='Ïƒ=1', linewidth=2)
        axes[3].plot(x_norm, y_norm2, label='Ïƒ=0.5', linewidth=2)
        axes[3].fill_between(x_norm, y_norm1, alpha=0.3)
        axes[3].set_title('æ­£æ€åˆ†å¸ƒ (Î¼=0)')
        axes[3].set_xlabel('X')
        axes[3].set_ylabel('f(x)')
        axes[3].legend()
        
        # 5. æŒ‡æ•°åˆ†å¸ƒ
        x_exp = np.linspace(0, 5, 100)
        y_exp1 = stats.expon.pdf(x_exp, scale=1)
        y_exp2 = stats.expon.pdf(x_exp, scale=0.5)
        axes[4].plot(x_exp, y_exp1, label='Î»=1', linewidth=2)
        axes[4].plot(x_exp, y_exp2, label='Î»=2', linewidth=2)
        axes[4].fill_between(x_exp, y_exp1, alpha=0.3)
        axes[4].set_title('æŒ‡æ•°åˆ†å¸ƒ')
        axes[4].set_xlabel('X')
        axes[4].set_ylabel('f(x)')
        axes[4].legend()
        
        # 6. Betaåˆ†å¸ƒ
        x_beta = np.linspace(0, 1, 100)
        y_beta1 = stats.beta.pdf(x_beta, 2, 2)
        y_beta2 = stats.beta.pdf(x_beta, 0.5, 0.5)
        axes[5].plot(x_beta, y_beta1, label='Î±=2, Î²=2', linewidth=2)
        axes[5].plot(x_beta, y_beta2, label='Î±=0.5, Î²=0.5', linewidth=2)
        axes[5].fill_between(x_beta, y_beta1, alpha=0.3)
        axes[5].set_title('Betaåˆ†å¸ƒ')
        axes[5].set_xlabel('X')
        axes[5].set_ylabel('f(x)')
        axes[5].legend()
        
        plt.tight_layout()
        plt.show()
    
    def central_limit_theorem(self):
        """ä¸­å¿ƒæé™å®šç†"""
        print("=== ä¸­å¿ƒæé™å®šç† ===")
        
        print("å®šç†è¡¨è¿°:")
        print("è®¾Xâ‚, Xâ‚‚, ..., Xâ‚™æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œ")
        print("å‡å€¼ä¸ºÎ¼ï¼Œæ–¹å·®ä¸ºÏƒÂ²ï¼Œåˆ™å½“nè¶³å¤Ÿå¤§æ—¶ï¼š")
        print("(XÌ„ - Î¼) / (Ïƒ/âˆšn) â†’ N(0,1)")
        print()
        
        # æ¼”ç¤ºä¸­å¿ƒæé™å®šç†
        self.demonstrate_clt()
        
        print("æœºå™¨å­¦ä¹ æ„ä¹‰:")
        print("- è§£é‡Šä¸ºä»€ä¹ˆå¾ˆå¤šç»Ÿè®¡é‡æœä»æ­£æ€åˆ†å¸ƒ")
        print("- ä¸ºå¤§æ ·æœ¬ç»Ÿè®¡æ¨æ–­æä¾›ç†è®ºåŸºç¡€")
        print("- ç¥ç»ç½‘ç»œæƒé‡åˆå§‹åŒ–çš„ç†è®ºä¾æ®")
        print("- Bootstrapæ–¹æ³•çš„ç†è®ºåŸºç¡€")
    
    def demonstrate_clt(self):
        """æ¼”ç¤ºä¸­å¿ƒæé™å®šç†"""
        # åŸå§‹åˆ†å¸ƒï¼šå‡åŒ€åˆ†å¸ƒ
        np.random.seed(42)
        
        # ä¸åŒæ ·æœ¬å¤§å°
        sample_sizes = [1, 5, 30, 100]
        n_experiments = 1000
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for i, n in enumerate(sample_sizes):
            # è¿›è¡Œå¤šæ¬¡å®éªŒï¼Œæ¯æ¬¡å–nä¸ªæ ·æœ¬çš„å‡å€¼
            sample_means = []
            for _ in range(n_experiments):
                # ä»å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–nä¸ªæ ·æœ¬
                samples = np.random.uniform(0, 1, n)
                sample_means.append(np.mean(samples))
            
            # ç»˜åˆ¶æ ·æœ¬å‡å€¼çš„åˆ†å¸ƒ
            axes[i].hist(sample_means, bins=50, density=True, alpha=0.7, 
                        color='skyblue', edgecolor='black')
            
            # ç†è®ºæ­£æ€åˆ†å¸ƒ
            mu = 0.5  # å‡åŒ€åˆ†å¸ƒ[0,1]çš„å‡å€¼
            sigma = np.sqrt(1/12 / n)  # æ ·æœ¬å‡å€¼çš„æ ‡å‡†å·®
            x_theory = np.linspace(min(sample_means), max(sample_means), 100)
            y_theory = stats.norm.pdf(x_theory, mu, sigma)
            axes[i].plot(x_theory, y_theory, 'r-', linewidth=2, 
                        label='ç†è®ºæ­£æ€åˆ†å¸ƒ')
            
            axes[i].set_title(f'æ ·æœ¬å¤§å° n = {n}')
            axes[i].set_xlabel('æ ·æœ¬å‡å€¼')
            axes[i].set_ylabel('å¯†åº¦')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.suptitle('ä¸­å¿ƒæé™å®šç†æ¼”ç¤ºï¼šæ ·æœ¬å‡å€¼è¶‹å‘æ­£æ€åˆ†å¸ƒ', fontsize=16)
        plt.tight_layout()
        plt.show()

class BayesianMachineLearning:
    """è´å¶æ–¯æœºå™¨å­¦ä¹ """
    
    def __init__(self):
        pass
    
    def bayesian_inference(self):
        """è´å¶æ–¯æ¨æ–­"""
        print("=== è´å¶æ–¯æ¨æ–­ ===")
        
        print("è´å¶æ–¯æ¨æ–­æ¡†æ¶:")
        print("P(Î¸|D) = P(D|Î¸) Ã— P(Î¸) / P(D)")
        print("å…¶ä¸­:")
        print("- Î¸: æ¨¡å‹å‚æ•°")
        print("- D: è§‚æµ‹æ•°æ®")
        print("- P(Î¸): å…ˆéªŒåˆ†å¸ƒ")
        print("- P(D|Î¸): ä¼¼ç„¶å‡½æ•°")
        print("- P(Î¸|D): åéªŒåˆ†å¸ƒ")
        print("- P(D): è¾¹é™…ä¼¼ç„¶")
        print()
        
        # è´å¶æ–¯çº¿æ€§å›å½’ç¤ºä¾‹
        self.bayesian_linear_regression()
        
        return self.demonstrate_conjugate_priors()
    
    def bayesian_linear_regression(self):
        """è´å¶æ–¯çº¿æ€§å›å½’"""
        print("=== è´å¶æ–¯çº¿æ€§å›å½’ ===")
        
        # ç”Ÿæˆæ•°æ®
        np.random.seed(42)
        n_points = 20
        true_w = [2, -1]  # çœŸå®æƒé‡
        true_sigma = 0.3   # å™ªå£°æ ‡å‡†å·®
        
        x = np.linspace(-1, 1, n_points)
        X = np.column_stack([np.ones(n_points), x])  # æ·»åŠ åç½®é¡¹
        y = X @ true_w + np.random.normal(0, true_sigma, n_points)
        
        # è´å¶æ–¯æ¨æ–­
        # å…ˆéªŒï¼šw ~ N(0, ÏƒÂ²I)
        prior_sigma = 1.0
        prior_precision = 1 / prior_sigma**2 * np.eye(2)
        prior_mean = np.zeros(2)
        
        # ä¼¼ç„¶ï¼šy = Xw + Îµ, Îµ ~ N(0, ÏƒÂ²I)
        noise_precision = 1 / true_sigma**2
        
        # åéªŒå‚æ•°
        posterior_precision = prior_precision + noise_precision * X.T @ X
        posterior_covariance = np.linalg.inv(posterior_precision)
        posterior_mean = posterior_covariance @ (noise_precision * X.T @ y)
        
        print(f"çœŸå®æƒé‡: {true_w}")
        print(f"åéªŒå‡å€¼: {posterior_mean}")
        print(f"åéªŒåæ–¹å·®å¯¹è§’çº¿: {np.diag(posterior_covariance)}")
        
        # å¯è§†åŒ–
        self.visualize_bayesian_regression(x, y, X, posterior_mean, posterior_covariance)
    
    def visualize_bayesian_regression(self, x, y, X, posterior_mean, posterior_cov):
        """å¯è§†åŒ–è´å¶æ–¯å›å½’"""
        plt.figure(figsize=(12, 8))
        
        # æ•°æ®ç‚¹
        plt.subplot(2, 2, 1)
        plt.scatter(x, y, alpha=0.7, color='blue', label='è§‚æµ‹æ•°æ®')
        
        # åéªŒé¢„æµ‹
        x_test = np.linspace(-1.5, 1.5, 100)
        X_test = np.column_stack([np.ones(len(x_test)), x_test])
        
        # é¢„æµ‹å‡å€¼
        y_pred_mean = X_test @ posterior_mean
        
        # é¢„æµ‹ä¸ç¡®å®šæ€§
        y_pred_var = np.array([X_test[i] @ posterior_cov @ X_test[i] 
                              for i in range(len(X_test))])
        y_pred_std = np.sqrt(y_pred_var + 0.3**2)  # åŠ ä¸Šå™ªå£°æ–¹å·®
        
        plt.plot(x_test, y_pred_mean, 'r-', linewidth=2, label='åéªŒé¢„æµ‹å‡å€¼')
        plt.fill_between(x_test, 
                        y_pred_mean - 2*y_pred_std,
                        y_pred_mean + 2*y_pred_std,
                        alpha=0.3, color='red', label='95%ç½®ä¿¡åŒºé—´')
        
        plt.xlabel('x')
        plt.ylabel('y')
        plt.title('è´å¶æ–¯çº¿æ€§å›å½’')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æƒé‡çš„åéªŒåˆ†å¸ƒ
        plt.subplot(2, 2, 2)
        # é‡‡æ ·æƒé‡
        n_samples = 1000
        weight_samples = np.random.multivariate_normal(posterior_mean, posterior_cov, n_samples)
        
        plt.scatter(weight_samples[:, 0], weight_samples[:, 1], alpha=0.5, s=10)
        plt.scatter(posterior_mean[0], posterior_mean[1], color='red', s=100, 
                   marker='x', linewidth=3, label='åéªŒå‡å€¼')
        plt.xlabel('wâ‚€ (åç½®)')
        plt.ylabel('wâ‚ (æ–œç‡)')
        plt.title('æƒé‡å‚æ•°çš„åéªŒåˆ†å¸ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å…ˆéªŒvsåéªŒå¯¹æ¯”
        plt.subplot(2, 2, 3)
        prior_samples = np.random.multivariate_normal([0, 0], np.eye(2), n_samples)
        plt.scatter(prior_samples[:, 0], prior_samples[:, 1], alpha=0.3, s=5, 
                   color='gray', label='å…ˆéªŒæ ·æœ¬')
        plt.scatter(weight_samples[:, 0], weight_samples[:, 1], alpha=0.5, s=5,
                   color='blue', label='åéªŒæ ·æœ¬')
        plt.xlabel('wâ‚€')
        plt.ylabel('wâ‚')
        plt.title('å…ˆéªŒ vs åéªŒåˆ†å¸ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # é¢„æµ‹ä¸ç¡®å®šæ€§å˜åŒ–
        plt.subplot(2, 2, 4)
        plt.plot(x_test, y_pred_std, 'purple', linewidth=2)
        plt.axvline(x=x.min(), color='gray', linestyle='--', alpha=0.5, label='æ•°æ®èŒƒå›´')
        plt.axvline(x=x.max(), color='gray', linestyle='--', alpha=0.5)
        plt.xlabel('x')
        plt.ylabel('é¢„æµ‹æ ‡å‡†å·®')
        plt.title('é¢„æµ‹ä¸ç¡®å®šæ€§')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def demonstrate_conjugate_priors(self):
        """æ¼”ç¤ºå…±è½­å…ˆéªŒ"""
        print("\n=== å…±è½­å…ˆéªŒ ===")
        
        conjugate_pairs = {
            "ä¼¯åŠªåˆ©-Beta": {
                "ä¼¼ç„¶": "Bernoulli(p)",
                "å…ˆéªŒ": "Beta(Î±, Î²)",
                "åéªŒ": "Beta(Î± + Î£xáµ¢, Î² + n - Î£xáµ¢)",
                "åº”ç”¨": "ç‚¹å‡»ç‡ä¼°è®¡ã€A/Bæµ‹è¯•"
            },
            "æ­£æ€-æ­£æ€": {
                "ä¼¼ç„¶": "N(Î¼, ÏƒÂ²) (ÏƒÂ²å·²çŸ¥)",
                "å…ˆéªŒ": "N(Î¼â‚€, Ïƒâ‚€Â²)",
                "åéªŒ": "N(Î¼â‚™, Ïƒâ‚™Â²)",
                "åº”ç”¨": "çº¿æ€§å›å½’ã€é«˜æ–¯è¿‡ç¨‹"
            },
            "æ³Šæ¾-Gamma": {
                "ä¼¼ç„¶": "Poisson(Î»)",
                "å…ˆéªŒ": "Gamma(Î±, Î²)",
                "åéªŒ": "Gamma(Î± + Î£xáµ¢, Î² + n)",
                "åº”ç”¨": "è®¡æ•°æ•°æ®å»ºæ¨¡"
            }
        }
        
        for pair, details in conjugate_pairs.items():
            print(f"\n{pair}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # Beta-Bernoulliå…±è½­æ¼”ç¤º
        self.beta_bernoulli_demo()
        
        return conjugate_pairs
    
    def beta_bernoulli_demo(self):
        """Beta-Bernoulliå…±è½­æ¼”ç¤º"""
        print("\n=== Beta-Bernoulliå…±è½­æ¼”ç¤º ===")
        
        # å‚æ•°è®¾ç½®
        true_p = 0.3  # çœŸå®æˆåŠŸæ¦‚ç‡
        alpha_0, beta_0 = 1, 1  # å…ˆéªŒå‚æ•°ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        
        # æ¨¡æ‹Ÿæ•°æ®æ”¶é›†è¿‡ç¨‹
        np.random.seed(42)
        n_experiments = [0, 5, 20, 100]
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        x = np.linspace(0, 1, 1000)
        
        for i, n in enumerate(n_experiments):
            if n == 0:
                # å…ˆéªŒåˆ†å¸ƒ
                alpha_n, beta_n = alpha_0, beta_0
                title = "å…ˆéªŒåˆ†å¸ƒ"
            else:
                # ç”Ÿæˆæ•°æ®
                data = np.random.binomial(1, true_p, n)
                successes = np.sum(data)
                
                # æ›´æ–°åéªŒå‚æ•°
                alpha_n = alpha_0 + successes
                beta_n = beta_0 + n - successes
                title = f"è§‚æµ‹ {n} æ¬¡åçš„åéªŒåˆ†å¸ƒ"
            
            # ç»˜åˆ¶åˆ†å¸ƒ
            y = stats.beta.pdf(x, alpha_n, beta_n)
            axes[i].plot(x, y, linewidth=2, color='blue')
            axes[i].fill_between(x, y, alpha=0.3, color='blue')
            axes[i].axvline(true_p, color='red', linestyle='--', 
                           linewidth=2, label=f'çœŸå®å€¼ p={true_p}')
            axes[i].axvline(alpha_n/(alpha_n + beta_n), color='green', 
                           linestyle=':', linewidth=2, label='åéªŒå‡å€¼')
            
            axes[i].set_xlabel('p (æˆåŠŸæ¦‚ç‡)')
            axes[i].set_ylabel('å¯†åº¦')
            axes[i].set_title(title)
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.suptitle('Beta-Bernoulliå…±è½­ï¼šå…ˆéªŒåˆ°åéªŒçš„æ›´æ–°', fontsize=16)
        plt.tight_layout()
        plt.show()

def comprehensive_probability_summary():
    """æ¦‚ç‡ç»Ÿè®¡ç†è®ºç»¼åˆæ€»ç»“"""
    print("=== æ¦‚ç‡ç»Ÿè®¡ç†è®ºç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "åŸºç¡€æ¦‚å¿µ": {
            "æ¦‚ç‡ç©ºé—´": "(Î©, F, P) - æ ·æœ¬ç©ºé—´ã€äº‹ä»¶åŸŸã€æ¦‚ç‡æµ‹åº¦",
            "éšæœºå˜é‡": "æ˜ å°„å‡½æ•° X: Î© â†’ â„",
            "åˆ†å¸ƒå‡½æ•°": "CDF: F(x) = P(X â‰¤ x)",
            "æ¦‚ç‡è´¨é‡/å¯†åº¦": "PMF/PDF: æè¿°æ¦‚ç‡åˆ†å¸ƒ"
        },
        
        "é‡è¦å®šç†": {
            "è´å¶æ–¯å®šç†": "P(A|B) = P(B|A)P(A)/P(B)",
            "å…¨æ¦‚ç‡å®šç†": "P(B) = Î£áµ¢ P(B|Aáµ¢)P(Aáµ¢)",
            "ä¸­å¿ƒæé™å®šç†": "æ ·æœ¬å‡å€¼æ¸è¿‘æ­£æ€åˆ†å¸ƒ",
            "å¤§æ•°å®šå¾‹": "æ ·æœ¬å‡å€¼æ”¶æ•›åˆ°æœŸæœ›å€¼"
        },
        
        "ç»Ÿè®¡æ¨æ–­": {
            "ç‚¹ä¼°è®¡": "å‚æ•°çš„æœ€ä½³çŒœæµ‹å€¼",
            "åŒºé—´ä¼°è®¡": "ç½®ä¿¡åŒºé—´ã€credible interval",
            "å‡è®¾æ£€éªŒ": "på€¼ã€ç¬¬ä¸€ç±»/äºŒç±»é”™è¯¯",
            "è´å¶æ–¯æ¨æ–­": "å…ˆéªŒâ†’ä¼¼ç„¶â†’åéªŒ"
        },
        
        "MLåº”ç”¨": {
            "æ¦‚ç‡æ¨¡å‹": "æœ´ç´ è´å¶æ–¯ã€é«˜æ–¯æ··åˆæ¨¡å‹",
            "ä¸ç¡®å®šæ€§é‡åŒ–": "é¢„æµ‹åŒºé—´ã€æ¨¡å‹ç½®ä¿¡åº¦",
            "è´å¶æ–¯æ·±åº¦å­¦ä¹ ": "æƒé‡ä¸ç¡®å®šæ€§ã€å˜åˆ†æ¨æ–­",
            "å¼ºåŒ–å­¦ä¹ ": "ç­–ç•¥æ¢¯åº¦ã€ä»·å€¼å‡½æ•°"
        },
        
        "å®è·µæŠ€å·§": {
            "æ•°å€¼ç¨³å®šæ€§": "log-sum-exp trick",
            "é‡‡æ ·æ–¹æ³•": "MCMCã€å˜åˆ†æ¨æ–­",
            "æ¨¡å‹é€‰æ‹©": "AICã€BICã€äº¤å‰éªŒè¯",
            "æ­£åˆ™åŒ–": "å…ˆéªŒä½œä¸ºæ­£åˆ™åŒ–é¡¹"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("æ¦‚ç‡ç»Ÿè®¡ç†è®ºåŸºç¡€æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Ross (2014): "A First Course in Probability"
- Casella & Berger (2002): "Statistical Inference"
- Murphy (2012): "Machine Learning: A Probabilistic Perspective"
- Bishop (2006): "Pattern Recognition and Machine Learning"
- Gelman et al. (2013): "Bayesian Data Analysis"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [è´å¶æ–¯æœºå™¨å­¦ä¹ ](bayesian_ml_theory.md) - è´å¶æ–¯æ–¹æ³•æ·±å…¥
- [ä¿¡æ¯ç†è®º](information_theory.md) - ç†µä¸ä¿¡æ¯æµ‹åº¦
- [å› æœæ¨ç†](causal_inference_theory.md) - å› æœå…³ç³»å»ºæ¨¡