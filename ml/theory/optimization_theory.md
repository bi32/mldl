# ä¼˜åŒ–ç®—æ³•ç†è®ºï¼šä»æ¢¯åº¦ä¸‹é™åˆ°ç°ä»£ä¼˜åŒ–å™¨ ğŸ¯

æ·±å…¥ç†è§£æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŒ–ç†è®ºã€ç®—æ³•å’Œå®è·µæŠ€å·§ã€‚

## 1. ä¼˜åŒ–åŸºç¡€ç†è®º ğŸ“

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import torch
import torch.optim as optim
from scipy.optimize import minimize
import seaborn as sns

class OptimizationFoundations:
    """ä¼˜åŒ–åŸºç¡€ç†è®º"""
    
    def __init__(self):
        self.functions = {}
        self.optimizers = {}
    
    def convex_analysis(self):
        """å‡¸ä¼˜åŒ–åˆ†æ"""
        print("=== å‡¸ä¼˜åŒ–ç†è®ºåŸºç¡€ ===")
        
        # å‡¸å‡½æ•°å®šä¹‰å’Œæ€§è´¨
        print("å‡¸å‡½æ•°å®šä¹‰:")
        print("f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y), âˆ€Î» âˆˆ [0,1]")
        print()
        
        # å¯è§†åŒ–å‡¸å‡½æ•°å’Œéå‡¸å‡½æ•°
        x = np.linspace(-3, 3, 100)
        
        # å‡¸å‡½æ•°ç¤ºä¾‹
        convex_funcs = {
            'xÂ²': x**2,
            'e^x': np.exp(x),
            '|x|': np.abs(x),
            'max(0,x)': np.maximum(0, x)
        }
        
        # éå‡¸å‡½æ•°ç¤ºä¾‹  
        nonconvex_funcs = {
            'xÂ³': x**3,
            'sin(x)': np.sin(x),
            'xâ´ - 2xÂ²': x**4 - 2*x**2
        }
        
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        
        # ç»˜åˆ¶å‡¸å‡½æ•°
        for i, (name, y) in enumerate(convex_funcs.items()):
            axes[0, i].plot(x, y, 'b-', linewidth=2)
            axes[0, i].set_title(f'å‡¸å‡½æ•°: {name}')
            axes[0, i].grid(True, alpha=0.3)
        
        # ç»˜åˆ¶éå‡¸å‡½æ•°
        for i, (name, y) in enumerate(nonconvex_funcs.items()):
            if i < 3:
                axes[1, i].plot(x, y, 'r-', linewidth=2)
                axes[1, i].set_title(f'éå‡¸å‡½æ•°: {name}')
                axes[1, i].grid(True, alpha=0.3)
        
        # å±€éƒ¨æœ€ä¼˜vså…¨å±€æœ€ä¼˜
        x_nonconvex = np.linspace(-2, 2, 100)
        y_nonconvex = x_nonconvex**4 - 2*x_nonconvex**2 + 0.5
        
        axes[1, 3].plot(x_nonconvex, y_nonconvex, 'r-', linewidth=2)
        
        # æ ‡æ³¨å±€éƒ¨æœ€ä¼˜ç‚¹
        local_minima = [-1, 1]
        for xmin in local_minima:
            ymin = xmin**4 - 2*xmin**2 + 0.5
            axes[1, 3].plot(xmin, ymin, 'go', markersize=8, label='å±€éƒ¨æœ€ä¼˜')
        
        # æ ‡æ³¨å…¨å±€æœ€ä¼˜ç‚¹
        global_min_x = 0
        global_min_y = 0.5
        axes[1, 3].plot(global_min_x, global_min_y, 'ro', markersize=8, label='å…¨å±€æœ€ä¼˜')
        axes[1, 3].set_title('å±€éƒ¨vså…¨å±€æœ€ä¼˜')
        axes[1, 3].legend()
        axes[1, 3].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # å‡¸ä¼˜åŒ–çš„ä¼˜åŠ¿
        print("å‡¸ä¼˜åŒ–ä¼˜åŠ¿:")
        print("1. å±€éƒ¨æœ€ä¼˜å³å…¨å±€æœ€ä¼˜")
        print("2. å¯ä»¥ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜è§£")
        print("3. æœ‰æˆç†Ÿçš„ç†è®ºå’Œç®—æ³•")
        print("4. è®¡ç®—å¤æ‚åº¦é€šå¸¸è¾ƒä½")
        print()
        
        return convex_funcs, nonconvex_funcs
    
    def gradient_analysis(self):
        """æ¢¯åº¦åˆ†æ"""
        print("=== æ¢¯åº¦ç†è®ºåˆ†æ ===")
        
        print("æ¢¯åº¦çš„å‡ ä½•æ„ä¹‰:")
        print("1. æ¢¯åº¦æ–¹å‘æ˜¯å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘")
        print("2. æ¢¯åº¦å¤§å°è¡¨ç¤ºå˜åŒ–ç‡") 
        print("3. è´Ÿæ¢¯åº¦æ–¹å‘æ˜¯ä¸‹é™æœ€å¿«çš„æ–¹å‘")
        print()
        
        # 2Då‡½æ•°æ¢¯åº¦å¯è§†åŒ–
        x = np.linspace(-2, 2, 20)
        y = np.linspace(-2, 2, 20)
        X, Y = np.meshgrid(x, y)
        
        # ç¤ºä¾‹å‡½æ•°: f(x,y) = xÂ² + yÂ²
        Z = X**2 + Y**2
        
        # è®¡ç®—æ¢¯åº¦: âˆ‡f = (2x, 2y)
        grad_x = 2 * X
        grad_y = 2 * Y
        
        plt.figure(figsize=(12, 5))
        
        # ç­‰é«˜çº¿å’Œæ¢¯åº¦åœº
        plt.subplot(1, 2, 1)
        contour = plt.contour(X, Y, Z, levels=15, alpha=0.6)
        plt.clabel(contour, inline=True, fontsize=8)
        plt.quiver(X[::3, ::3], Y[::3, ::3], 
                  grad_x[::3, ::3], grad_y[::3, ::3], 
                  scale=50, color='red', alpha=0.7)
        plt.title('æ¢¯åº¦åœºå¯è§†åŒ–')
        plt.xlabel('x')
        plt.ylabel('y')
        plt.grid(True, alpha=0.3)
        
        # 3Dè¡¨é¢å›¾
        ax = plt.subplot(1, 2, 2, projection='3d')
        surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)
        
        # æ·»åŠ ä¸€äº›æ¢¯åº¦å‘é‡
        for i in range(0, 20, 4):
            for j in range(0, 20, 4):
                ax.quiver(X[i,j], Y[i,j], Z[i,j], 
                         grad_x[i,j], grad_y[i,j], 0,
                         length=0.3, color='red', alpha=0.8)
        
        ax.set_title('3Då‡½æ•°åŠå…¶æ¢¯åº¦')
        ax.set_xlabel('x')
        ax.set_ylabel('y') 
        ax.set_zlabel('f(x,y)')
        
        plt.tight_layout()
        plt.show()
        
        return X, Y, Z, grad_x, grad_y
    
    def hessian_analysis(self):
        """HessiançŸ©é˜µåˆ†æ"""
        print("=== HessiançŸ©é˜µåˆ†æ ===")
        
        print("HessiançŸ©é˜µå®šä¹‰:")
        print("H_ij = âˆ‚Â²f / âˆ‚x_iâˆ‚x_j")
        print()
        print("äºŒé˜¶æ¡ä»¶:")
        print("- H > 0 (æ­£å®š): å±€éƒ¨æœ€å°å€¼")
        print("- H < 0 (è´Ÿå®š): å±€éƒ¨æœ€å¤§å€¼") 
        print("- H ä¸å®š: éç‚¹")
        print("- H â‰¥ 0 (åŠæ­£å®š): å¯èƒ½çš„æœ€å°å€¼")
        print()
        
        # ä¸åŒHessianç‰¹å¾å€¼å¯¹åº”çš„å‡½æ•°å½¢çŠ¶
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        x = np.linspace(-2, 2, 100)
        y = np.linspace(-2, 2, 100)
        X, Y = np.meshgrid(x, y)
        
        # 1. æ­£å®š Hessian (æ¤­åœ†å½¢)
        Z1 = X**2 + 4*Y**2
        im1 = axes[0, 0].contour(X, Y, Z1, levels=15)
        axes[0, 0].set_title('æ­£å®šHessian\n(å±€éƒ¨æœ€å°å€¼)')
        axes[0, 0].set_aspect('equal')
        
        # 2. è´Ÿå®š Hessian (å€’æ¤­åœ†å½¢)
        Z2 = -(X**2 + 4*Y**2) + 10
        axes[0, 1].contour(X, Y, Z2, levels=15)
        axes[0, 1].set_title('è´Ÿå®šHessian\n(å±€éƒ¨æœ€å¤§å€¼)')
        axes[0, 1].set_aspect('equal')
        
        # 3. ä¸å®š Hessian (éç‚¹)
        Z3 = X**2 - Y**2
        axes[1, 0].contour(X, Y, Z3, levels=15)
        axes[1, 0].set_title('ä¸å®šHessian\n(éç‚¹)')
        axes[1, 0].set_aspect('equal')
        
        # 4. åŠæ­£å®š Hessian
        Z4 = X**2
        axes[1, 1].contour(X, Y, Z4, levels=15)
        axes[1, 1].set_title('åŠæ­£å®šHessian\n(å¹³å¦æ–¹å‘)')
        axes[1, 1].set_aspect('equal')
        
        plt.tight_layout()
        plt.show()
        
        # è®¡ç®—å…·ä½“çš„HessiançŸ©é˜µ
        print("ç¤ºä¾‹: f(x,y) = xÂ² + 4yÂ²")
        print("HessiançŸ©é˜µ:")
        print("H = [[2, 0],")
        print("     [0, 8]]")
        print("ç‰¹å¾å€¼: Î»â‚ = 2, Î»â‚‚ = 8 (éƒ½ä¸ºæ­£ï¼Œæ­£å®š)")
        print()
        
        return Z1, Z2, Z3, Z4
```

## 2. æ¢¯åº¦ä¸‹é™ç®—æ³•æ— ğŸƒâ€â™‚ï¸

```python
class GradientDescentFamily:
    """æ¢¯åº¦ä¸‹é™ç®—æ³•æ—"""
    
    def __init__(self):
        self.optimizers = {}
        self.history = {}
    
    def vanilla_gradient_descent(self, func, grad_func, x0, lr=0.01, max_iter=1000, tol=1e-6):
        """æ ‡å‡†æ¢¯åº¦ä¸‹é™"""
        print("=== æ ‡å‡†æ¢¯åº¦ä¸‹é™ ===")
        
        x = x0.copy()
        history = {'x': [x.copy()], 'f': [func(x)]}
        
        for i in range(max_iter):
            grad = grad_func(x)
            x_new = x - lr * grad
            
            # æ£€æŸ¥æ”¶æ•›
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def momentum_gradient_descent(self, func, grad_func, x0, lr=0.01, momentum=0.9, 
                                 max_iter=1000, tol=1e-6):
        """åŠ¨é‡æ¢¯åº¦ä¸‹é™"""
        print("=== åŠ¨é‡æ¢¯åº¦ä¸‹é™ ===")
        
        x = x0.copy()
        v = np.zeros_like(x)  # é€Ÿåº¦é¡¹
        history = {'x': [x.copy()], 'f': [func(x)]}
        
        for i in range(max_iter):
            grad = grad_func(x)
            v = momentum * v + lr * grad  # æ›´æ–°é€Ÿåº¦
            x_new = x - v  # æ›´æ–°ä½ç½®
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def nesterov_momentum(self, func, grad_func, x0, lr=0.01, momentum=0.9,
                         max_iter=1000, tol=1e-6):
        """NesterovåŠ é€Ÿæ¢¯åº¦ä¸‹é™"""
        print("=== NesterovåŠ é€Ÿæ¢¯åº¦ä¸‹é™ ===")
        
        x = x0.copy()
        v = np.zeros_like(x)
        history = {'x': [x.copy()], 'f': [func(x)]}
        
        for i in range(max_iter):
            # å‘å‰çœ‹ä¸€æ­¥
            x_lookahead = x - momentum * v
            grad = grad_func(x_lookahead)  # åœ¨lookaheadä½ç½®è®¡ç®—æ¢¯åº¦
            
            v = momentum * v + lr * grad
            x_new = x - v
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def compare_gradient_methods(self):
        """æ¯”è¾ƒä¸åŒæ¢¯åº¦ä¸‹é™æ–¹æ³•"""
        print("=== æ¢¯åº¦ä¸‹é™æ–¹æ³•æ¯”è¾ƒ ===")
        
        # å®šä¹‰æµ‹è¯•å‡½æ•°: Rosenbrockå‡½æ•°
        def rosenbrock(x):
            return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2
        
        def rosenbrock_grad(x):
            grad_x0 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])
            grad_x1 = 200 * (x[1] - x[0]**2)
            return np.array([grad_x0, grad_x1])
        
        # åˆå§‹ç‚¹
        x0 = np.array([-1.2, 1.0])
        
        # è¿è¡Œä¸åŒç®—æ³•
        methods = {
            'Vanilla GD': lambda: self.vanilla_gradient_descent(
                rosenbrock, rosenbrock_grad, x0, lr=0.001, max_iter=5000),
            'Momentum': lambda: self.momentum_gradient_descent(
                rosenbrock, rosenbrock_grad, x0, lr=0.001, momentum=0.9, max_iter=5000),
            'Nesterov': lambda: self.nesterov_momentum(
                rosenbrock, rosenbrock_grad, x0, lr=0.001, momentum=0.9, max_iter=5000)
        }
        
        results = {}
        for name, method in methods.items():
            print(f"\nè¿è¡Œ {name}:")
            x_final, history = method()
            results[name] = history
            print(f"æœ€ç»ˆç‚¹: [{x_final[0]:.4f}, {x_final[1]:.4f}]")
            print(f"æœ€ç»ˆå‡½æ•°å€¼: {rosenbrock(x_final):.6f}")
            print(f"è¿­ä»£æ¬¡æ•°: {len(history['x'])}")
        
        # å¯è§†åŒ–æ¯”è¾ƒ
        self.visualize_optimization_paths(rosenbrock, results)
        
        return results
    
    def visualize_optimization_paths(self, func, results):
        """å¯è§†åŒ–ä¼˜åŒ–è·¯å¾„"""
        # åˆ›å»ºå‡½æ•°çš„ç­‰é«˜çº¿å›¾
        x = np.linspace(-1.5, 1.5, 100)
        y = np.linspace(-0.5, 1.5, 100)
        X, Y = np.meshgrid(x, y)
        Z = np.array([[func(np.array([xi, yi])) for xi in x] for yi in y])
        
        plt.figure(figsize=(12, 8))
        
        # ç­‰é«˜çº¿å›¾
        levels = np.logspace(0, 3, 20)
        contour = plt.contour(X, Y, Z, levels=levels, alpha=0.6)
        plt.clabel(contour, inline=True, fontsize=8, fmt='%1.0f')
        
        # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„
        colors = ['red', 'blue', 'green']
        markers = ['o-', 's-', '^-']
        
        for i, (name, history) in enumerate(results.items()):
            path = np.array(history['x'])
            plt.plot(path[:, 0], path[:, 1], markers[i], 
                    color=colors[i], label=name, markersize=3, alpha=0.7)
            
            # æ ‡æ³¨èµ·ç‚¹å’Œç»ˆç‚¹
            plt.plot(path[0, 0], path[0, 1], 'ko', markersize=8, label='èµ·ç‚¹' if i == 0 else '')
            plt.plot(path[-1, 0], path[-1, 1], colors[i], marker='*', 
                    markersize=12, label=f'{name} ç»ˆç‚¹')
        
        # æ ‡æ³¨å…¨å±€æœ€ä¼˜ç‚¹
        plt.plot(1, 1, 'gold', marker='*', markersize=15, label='å…¨å±€æœ€ä¼˜ç‚¹')
        
        plt.xlabel('xâ‚')
        plt.ylabel('xâ‚‚')
        plt.title('ä¼˜åŒ–è·¯å¾„æ¯”è¾ƒ (Rosenbrockå‡½æ•°)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        # æ”¶æ•›æ›²çº¿
        plt.figure(figsize=(10, 6))
        
        for name, history in results.items():
            plt.semilogy(history['f'], label=name, linewidth=2)
        
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('å‡½æ•°å€¼ (log scale)')
        plt.title('æ”¶æ•›æ›²çº¿æ¯”è¾ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
```

## 3. è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³• ğŸ›ï¸

```python
class AdaptiveLearningRateOptimizers:
    """è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        pass
    
    def adagrad_implementation(self, func, grad_func, x0, lr=0.1, eps=1e-8, 
                              max_iter=1000, tol=1e-6):
        """AdaGradå®ç°"""
        print("=== AdaGradä¼˜åŒ–å™¨ ===")
        print("ç‰¹ç‚¹: ç´¯ç§¯å†å²æ¢¯åº¦çš„å¹³æ–¹ï¼Œè‡ªåŠ¨è°ƒèŠ‚å­¦ä¹ ç‡")
        print("å…¬å¼: x_t = x_{t-1} - lr / âˆš(G_t + Îµ) * g_t")
        print("å…¶ä¸­ G_t = G_{t-1} + g_tÂ²")
        print()
        
        x = x0.copy()
        G = np.zeros_like(x)  # æ¢¯åº¦å¹³æ–¹å’Œç´¯ç§¯
        history = {'x': [x.copy()], 'f': [func(x)], 'lr': []}
        
        for i in range(max_iter):
            grad = grad_func(x)
            G += grad**2  # ç´¯ç§¯æ¢¯åº¦å¹³æ–¹
            
            # è‡ªé€‚åº”å­¦ä¹ ç‡
            adapted_lr = lr / np.sqrt(G + eps)
            x_new = x - adapted_lr * grad
            
            history['lr'].append(adapted_lr.copy())
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def rmsprop_implementation(self, func, grad_func, x0, lr=0.01, decay_rate=0.9,
                              eps=1e-8, max_iter=1000, tol=1e-6):
        """RMSpropå®ç°"""
        print("=== RMSpropä¼˜åŒ–å™¨ ===")
        print("ç‰¹ç‚¹: æŒ‡æ•°ç§»åŠ¨å¹³å‡çš„æ¢¯åº¦å¹³æ–¹ï¼Œè§£å†³AdaGradå­¦ä¹ ç‡è¡°å‡è¿‡å¿«é—®é¢˜")
        print("å…¬å¼: x_t = x_{t-1} - lr / âˆš(v_t + Îµ) * g_t")
        print("å…¶ä¸­ v_t = Î² * v_{t-1} + (1-Î²) * g_tÂ²")
        print()
        
        x = x0.copy()
        v = np.zeros_like(x)  # æ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡
        history = {'x': [x.copy()], 'f': [func(x)], 'lr': []}
        
        for i in range(max_iter):
            grad = grad_func(x)
            v = decay_rate * v + (1 - decay_rate) * grad**2
            
            adapted_lr = lr / np.sqrt(v + eps)
            x_new = x - adapted_lr * grad
            
            history['lr'].append(adapted_lr.copy())
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def adam_implementation(self, func, grad_func, x0, lr=0.001, beta1=0.9, beta2=0.999,
                           eps=1e-8, max_iter=1000, tol=1e-6):
        """Adamä¼˜åŒ–å™¨å®ç°"""
        print("=== Adamä¼˜åŒ–å™¨ ===")
        print("ç‰¹ç‚¹: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡")
        print("å…¬å¼: m_t = Î²â‚*m_{t-1} + (1-Î²â‚)*g_t")
        print("      v_t = Î²â‚‚*v_{t-1} + (1-Î²â‚‚)*g_tÂ²")
        print("      mÌ‚_t = m_t / (1-Î²â‚áµ—)")
        print("      vÌ‚_t = v_t / (1-Î²â‚‚áµ—)")
        print("      x_t = x_{t-1} - lr * mÌ‚_t / (âˆšvÌ‚_t + Îµ)")
        print()
        
        x = x0.copy()
        m = np.zeros_like(x)  # ä¸€é˜¶çŸ©ä¼°è®¡
        v = np.zeros_like(x)  # äºŒé˜¶çŸ©ä¼°è®¡
        history = {'x': [x.copy()], 'f': [func(x)], 'lr': []}
        
        for t in range(1, max_iter + 1):
            grad = grad_func(x)
            
            # æ›´æ–°æœ‰åä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * grad**2
            
            # åå·®ä¿®æ­£
            m_hat = m / (1 - beta1**t)
            v_hat = v / (1 - beta2**t)
            
            # æ›´æ–°å‚æ•°
            adapted_lr = lr / (np.sqrt(v_hat) + eps)
            x_new = x - adapted_lr * m_hat
            
            history['lr'].append(adapted_lr.copy())
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {t} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def adamw_implementation(self, func, grad_func, x0, lr=0.001, beta1=0.9, beta2=0.999,
                            eps=1e-8, weight_decay=0.01, max_iter=1000, tol=1e-6):
        """AdamWä¼˜åŒ–å™¨å®ç°"""
        print("=== AdamWä¼˜åŒ–å™¨ ===")
        print("ç‰¹ç‚¹: Adam + æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–çš„æ­£ç¡®å®ç°ï¼‰")
        print("åŒºåˆ«: æƒé‡è¡°å‡ç›´æ¥åº”ç”¨äºå‚æ•°ï¼Œä¸ç»è¿‡åŠ¨é‡")
        print()
        
        x = x0.copy()
        m = np.zeros_like(x)
        v = np.zeros_like(x)
        history = {'x': [x.copy()], 'f': [func(x)], 'lr': []}
        
        for t in range(1, max_iter + 1):
            grad = grad_func(x)
            
            # æƒé‡è¡°å‡
            grad = grad + weight_decay * x
            
            # Adamæ­¥éª¤
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * grad**2
            
            m_hat = m / (1 - beta1**t)
            v_hat = v / (1 - beta2**t)
            
            adapted_lr = lr / (np.sqrt(v_hat) + eps)
            x_new = x - adapted_lr * m_hat
            
            history['lr'].append(adapted_lr.copy())
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {t} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def compare_adaptive_optimizers(self):
        """æ¯”è¾ƒè‡ªé€‚åº”ä¼˜åŒ–å™¨"""
        print("=== è‡ªé€‚åº”ä¼˜åŒ–å™¨æ¯”è¾ƒ ===")
        
        # æµ‹è¯•å‡½æ•°ï¼šBealeå‡½æ•° (å¤šå³°å‡½æ•°)
        def beale(x):
            return (1.5 - x[0] + x[0]*x[1])**2 + \
                   (2.25 - x[0] + x[0]*x[1]**2)**2 + \
                   (2.625 - x[0] + x[0]*x[1]**3)**2
        
        def beale_grad(x):
            t1 = 1.5 - x[0] + x[0]*x[1]
            t2 = 2.25 - x[0] + x[0]*x[1]**2  
            t3 = 2.625 - x[0] + x[0]*x[1]**3
            
            grad_x0 = 2*t1*(-1 + x[1]) + 2*t2*(-1 + x[1]**2) + 2*t3*(-1 + x[1]**3)
            grad_x1 = 2*t1*x[0] + 2*t2*x[0]*2*x[1] + 2*t3*x[0]*3*x[1]**2
            
            return np.array([grad_x0, grad_x1])
        
        x0 = np.array([1.0, 1.0])
        
        # è¿è¡Œä¸åŒä¼˜åŒ–å™¨
        optimizers = {
            'AdaGrad': lambda: self.adagrad_implementation(beale, beale_grad, x0, lr=0.1),
            'RMSprop': lambda: self.rmsprop_implementation(beale, beale_grad, x0, lr=0.01),
            'Adam': lambda: self.adam_implementation(beale, beale_grad, x0, lr=0.01),
            'AdamW': lambda: self.adamw_implementation(beale, beale_grad, x0, lr=0.01)
        }
        
        results = {}
        for name, optimizer in optimizers.items():
            print(f"\nè¿è¡Œ {name}:")
            try:
                x_final, history = optimizer()
                results[name] = history
                print(f"æœ€ç»ˆç‚¹: [{x_final[0]:.4f}, {x_final[1]:.4f}]")
                print(f"æœ€ç»ˆå‡½æ•°å€¼: {beale(x_final):.6f}")
                print(f"è¿­ä»£æ¬¡æ•°: {len(history['x'])}")
            except Exception as e:
                print(f"ä¼˜åŒ–å¤±è´¥: {e}")
        
        # å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–
        self.visualize_adaptive_learning_rates(results)
        
        return results
    
    def visualize_adaptive_learning_rates(self, results):
        """å¯è§†åŒ–è‡ªé€‚åº”å­¦ä¹ ç‡å˜åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å‡½æ•°å€¼æ”¶æ•›
        axes[0, 0].set_title('æ”¶æ•›æ›²çº¿')
        for name, history in results.items():
            if 'f' in history:
                axes[0, 0].semilogy(history['f'], label=name, linewidth=2)
        axes[0, 0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 0].set_ylabel('å‡½æ•°å€¼ (log)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡å˜åŒ–ï¼ˆç¬¬ä¸€ä¸ªç»´åº¦ï¼‰
        axes[0, 1].set_title('å­¦ä¹ ç‡å˜åŒ– (ç¬¬ä¸€ç»´åº¦)')
        for name, history in results.items():
            if 'lr' in history and len(history['lr']) > 0:
                lr_dim0 = [lr[0] if isinstance(lr, np.ndarray) else lr for lr in history['lr']]
                axes[0, 1].plot(lr_dim0, label=name, linewidth=2)
        axes[0, 1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 1].set_ylabel('å­¦ä¹ ç‡')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # å‚æ•°è½¨è¿¹
        axes[1, 0].set_title('å‚æ•°ç©ºé—´è½¨è¿¹')
        for name, history in results.items():
            if 'x' in history:
                path = np.array(history['x'])
                axes[1, 0].plot(path[:, 0], path[:, 1], 'o-', 
                               label=name, markersize=3, alpha=0.7)
        axes[1, 0].set_xlabel('xâ‚')
        axes[1, 0].set_ylabel('xâ‚‚')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ¢¯åº¦èŒƒæ•°
        axes[1, 1].set_title('æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ')
        for name, history in results.items():
            if 'f' in history:
                # è®¡ç®—æ”¶æ•›é€Ÿåº¦ï¼ˆå‡½æ•°å€¼å˜åŒ–ç‡ï¼‰
                f_values = np.array(history['f'])
                if len(f_values) > 1:
                    convergence_rate = np.abs(np.diff(f_values))
                    axes[1, 1].semilogy(convergence_rate, label=name, linewidth=2)
        axes[1, 1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[1, 1].set_ylabel('å‡½æ•°å€¼å˜åŒ– (log)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
```

## 4. äºŒé˜¶ä¼˜åŒ–æ–¹æ³• ğŸ“ˆ

```python
class SecondOrderOptimizers:
    """äºŒé˜¶ä¼˜åŒ–æ–¹æ³•"""
    
    def __init__(self):
        pass
    
    def newton_method(self, func, grad_func, hess_func, x0, max_iter=100, tol=1e-6):
        """ç‰›é¡¿æ³•"""
        print("=== ç‰›é¡¿æ³• ===")
        print("åŸç†: åˆ©ç”¨äºŒé˜¶ä¿¡æ¯å¿«é€Ÿæ”¶æ•›")
        print("å…¬å¼: x_{k+1} = x_k - Hâ»Â¹(x_k) * âˆ‡f(x_k)")
        print("ä¼˜ç‚¹: äºŒæ¬¡æ”¶æ•›é€Ÿåº¦")
        print("ç¼ºç‚¹: éœ€è¦è®¡ç®—å’Œæ±‚é€†HessiançŸ©é˜µ")
        print()
        
        x = x0.copy()
        history = {'x': [x.copy()], 'f': [func(x)]}
        
        for i in range(max_iter):
            grad = grad_func(x)
            hess = hess_func(x)
            
            # æ£€æŸ¥Hessianæ˜¯å¦æ­£å®š
            try:
                # ç‰›é¡¿æ­¥é•¿
                delta_x = np.linalg.solve(hess, grad)
                x_new = x - delta_x
            except np.linalg.LinAlgError:
                print(f"HessiançŸ©é˜µå¥‡å¼‚ï¼Œåœ¨ç¬¬ {i} æ­¥åœæ­¢")
                break
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def quasi_newton_bfgs(self, func, grad_func, x0, max_iter=1000, tol=1e-6):
        """BFGSæ‹Ÿç‰›é¡¿æ³•"""
        print("=== BFGSæ‹Ÿç‰›é¡¿æ³• ===")
        print("åŸç†: ç”¨æ­£å®šçŸ©é˜µè¿‘ä¼¼Hessiané€†çŸ©é˜µ")
        print("ä¼˜ç‚¹: è¶…çº¿æ€§æ”¶æ•›ï¼Œä¸éœ€è¦è®¡ç®—çœŸå®Hessian")
        print("ç¼ºç‚¹: éœ€è¦å­˜å‚¨nÃ—nçŸ©é˜µ")
        print()
        
        n = len(x0)
        x = x0.copy()
        H = np.eye(n)  # Hessiané€†çŸ©é˜µçš„è¿‘ä¼¼
        grad = grad_func(x)
        
        history = {'x': [x.copy()], 'f': [func(x)]}
        
        for i in range(max_iter):
            # è®¡ç®—æœç´¢æ–¹å‘
            p = -H @ grad
            
            # çº¿æœç´¢ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å›ºå®šæ­¥é•¿ï¼‰
            alpha = self.line_search(func, grad_func, x, p)
            
            # æ›´æ–°ä½ç½®
            x_new = x + alpha * p
            grad_new = grad_func(x_new)
            
            # BFGSæ›´æ–°å…¬å¼
            s = x_new - x
            y = grad_new - grad
            
            if np.dot(s, y) > 1e-10:  # ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
                rho = 1.0 / np.dot(y, s)
                I = np.eye(n)
                H = (I - rho * np.outer(s, y)) @ H @ (I - rho * np.outer(y, s)) + \
                    rho * np.outer(s, s)
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            grad = grad_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def limited_memory_bfgs(self, func, grad_func, x0, m=10, max_iter=1000, tol=1e-6):
        """L-BFGSæœ‰é™å†…å­˜BFGS"""
        print("=== L-BFGSæœ‰é™å†…å­˜BFGS ===")
        print(f"åŸç†: åªä¿å­˜æœ€è¿‘{m}æ­¥çš„ä¿¡æ¯è¿‘ä¼¼Hessian")
        print("ä¼˜ç‚¹: å†…å­˜éœ€æ±‚ä½ï¼Œé€‚åˆå¤§è§„æ¨¡é—®é¢˜")
        print("ç¼ºç‚¹: æ”¶æ•›é€Ÿåº¦ç•¥æ…¢äºBFGS")
        print()
        
        x = x0.copy()
        grad = grad_func(x)
        
        # å†å²ä¿¡æ¯å­˜å‚¨
        s_list = []  # xçš„å˜åŒ–
        y_list = []  # æ¢¯åº¦çš„å˜åŒ–
        rho_list = []  # 1/(s^T y)
        
        history = {'x': [x.copy()], 'f': [func(x)]}
        
        for i in range(max_iter):
            # ä¸¤å¾ªç¯é€’å½’è®¡ç®—æœç´¢æ–¹å‘
            q = grad.copy()
            alpha_list = []
            
            # ç¬¬ä¸€ä¸ªå¾ªç¯ï¼ˆåå‘ï¼‰
            for j in range(len(s_list)-1, -1, -1):
                alpha = rho_list[j] * np.dot(s_list[j], q)
                q = q - alpha * y_list[j]
                alpha_list.insert(0, alpha)
            
            # åˆå§‹Hessianè¿‘ä¼¼
            if len(s_list) > 0:
                gamma = np.dot(s_list[-1], y_list[-1]) / np.dot(y_list[-1], y_list[-1])
                r = gamma * q
            else:
                r = q
            
            # ç¬¬äºŒä¸ªå¾ªç¯ï¼ˆæ­£å‘ï¼‰
            for j in range(len(s_list)):
                beta = rho_list[j] * np.dot(y_list[j], r)
                r = r + (alpha_list[j] - beta) * s_list[j]
            
            p = -r  # æœç´¢æ–¹å‘
            
            # çº¿æœç´¢
            alpha = self.line_search(func, grad_func, x, p)
            
            # æ›´æ–°
            x_new = x + alpha * p
            grad_new = grad_func(x_new)
            
            s = x_new - x
            y = grad_new - grad
            
            if np.dot(s, y) > 1e-10:
                # æ›´æ–°å†å²ä¿¡æ¯
                if len(s_list) >= m:
                    s_list.pop(0)
                    y_list.pop(0)
                    rho_list.pop(0)
                
                s_list.append(s)
                y_list.append(y)
                rho_list.append(1.0 / np.dot(s, y))
            
            if np.linalg.norm(x_new - x) < tol:
                print(f"æ”¶æ•›äºç¬¬ {i+1} æ­¥")
                break
            
            x = x_new
            grad = grad_new
            history['x'].append(x.copy())
            history['f'].append(func(x))
        
        return x, history
    
    def line_search(self, func, grad_func, x, p, alpha0=1.0, c1=1e-4, c2=0.9):
        """Wolfeæ¡ä»¶çº¿æœç´¢"""
        alpha = alpha0
        phi0 = func(x)
        dphi0 = np.dot(grad_func(x), p)
        
        # ç®€åŒ–çš„å›é€€çº¿æœç´¢
        for _ in range(20):
            phi_alpha = func(x + alpha * p)
            
            # Armijoæ¡ä»¶
            if phi_alpha <= phi0 + c1 * alpha * dphi0:
                return alpha
            
            alpha *= 0.5
        
        return alpha
    
    def compare_second_order_methods(self):
        """æ¯”è¾ƒäºŒé˜¶ä¼˜åŒ–æ–¹æ³•"""
        print("=== äºŒé˜¶ä¼˜åŒ–æ–¹æ³•æ¯”è¾ƒ ===")
        
        # æµ‹è¯•å‡½æ•°ï¼šäºŒæ¬¡å‡½æ•° f(x) = 1/2 * x^T A x - b^T x
        A = np.array([[4, 1], [1, 2]])
        b = np.array([1, 2])
        
        def quadratic(x):
            return 0.5 * x.T @ A @ x - b.T @ x
        
        def quadratic_grad(x):
            return A @ x - b
        
        def quadratic_hess(x):
            return A
        
        x0 = np.array([2.0, 2.0])
        
        # è¿è¡Œä¸åŒæ–¹æ³•
        methods = {
            'Newton': lambda: self.newton_method(quadratic, quadratic_grad, quadratic_hess, x0),
            'BFGS': lambda: self.quasi_newton_bfgs(quadratic, quadratic_grad, x0),
            'L-BFGS': lambda: self.limited_memory_bfgs(quadratic, quadratic_grad, x0, m=5)
        }
        
        results = {}
        for name, method in methods.items():
            print(f"\nè¿è¡Œ {name}:")
            x_final, history = method()
            results[name] = history
            print(f"æœ€ç»ˆç‚¹: [{x_final[0]:.6f}, {x_final[1]:.6f}]")
            print(f"æœ€ç»ˆå‡½æ•°å€¼: {quadratic(x_final):.8f}")
            print(f"è¿­ä»£æ¬¡æ•°: {len(history['x'])}")
        
        # å¯è§†åŒ–æ¯”è¾ƒ
        self.visualize_second_order_comparison(quadratic, results)
        
        return results
    
    def visualize_second_order_comparison(self, func, results):
        """å¯è§†åŒ–äºŒé˜¶æ–¹æ³•æ¯”è¾ƒ"""
        plt.figure(figsize=(15, 5))
        
        # ä¼˜åŒ–è·¯å¾„
        plt.subplot(1, 3, 1)
        
        # åˆ›å»ºç­‰é«˜çº¿
        x = np.linspace(-0.5, 2.5, 50)
        y = np.linspace(-0.5, 2.5, 50)
        X, Y = np.meshgrid(x, y)
        Z = np.array([[func(np.array([xi, yi])) for xi in x] for yi in y])
        
        contour = plt.contour(X, Y, Z, levels=15, alpha=0.6)
        plt.clabel(contour, inline=True, fontsize=8)
        
        colors = ['red', 'blue', 'green']
        markers = ['o-', 's-', '^-']
        
        for i, (name, history) in enumerate(results.items()):
            path = np.array(history['x'])
            plt.plot(path[:, 0], path[:, 1], markers[i], 
                    color=colors[i], label=name, markersize=4, alpha=0.7)
        
        plt.xlabel('xâ‚')
        plt.ylabel('xâ‚‚')
        plt.title('ä¼˜åŒ–è·¯å¾„æ¯”è¾ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ”¶æ•›æ›²çº¿
        plt.subplot(1, 3, 2)
        for name, history in results.items():
            plt.semilogy(history['f'], label=name, linewidth=2)
        
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('å‡½æ•°å€¼ (log)')
        plt.title('æ”¶æ•›æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ”¶æ•›é€Ÿåº¦
        plt.subplot(1, 3, 3)
        for name, history in results.items():
            errors = [np.linalg.norm(x - np.array([0.2, 0.8])) 
                     for x in history['x']]  # è·ç¦»çœŸå®è§£çš„è¯¯å·®
            if len(errors) > 1:
                plt.semilogy(errors, label=name, linewidth=2)
        
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('è¯¯å·® (log)')
        plt.title('æ”¶æ•›åˆ°æœ€ä¼˜è§£çš„è¯¯å·®')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
```

## 5. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ ğŸ“Š

```python
class LearningRateScheduling:
    """å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
    
    def __init__(self):
        self.schedules = {}
    
    def step_decay_schedule(self, initial_lr=0.1, drop_rate=0.5, epochs_drop=10, epochs=100):
        """æ­¥é˜¶è¡°å‡"""
        schedule = []
        for epoch in range(epochs):
            lr = initial_lr * (drop_rate ** (epoch // epochs_drop))
            schedule.append(lr)
        return schedule
    
    def exponential_decay_schedule(self, initial_lr=0.1, decay_rate=0.95, epochs=100):
        """æŒ‡æ•°è¡°å‡"""
        schedule = []
        for epoch in range(epochs):
            lr = initial_lr * (decay_rate ** epoch)
            schedule.append(lr)
        return schedule
    
    def cosine_annealing_schedule(self, initial_lr=0.1, min_lr=1e-5, epochs=100):
        """ä½™å¼¦é€€ç«"""
        schedule = []
        for epoch in range(epochs):
            lr = min_lr + (initial_lr - min_lr) * \
                (1 + np.cos(np.pi * epoch / epochs)) / 2
            schedule.append(lr)
        return schedule
    
    def warmup_cosine_schedule(self, initial_lr=0.1, warmup_epochs=10, 
                              min_lr=1e-5, epochs=100):
        """é¢„çƒ­+ä½™å¼¦é€€ç«"""
        schedule = []
        for epoch in range(epochs):
            if epoch < warmup_epochs:
                # çº¿æ€§é¢„çƒ­
                lr = initial_lr * epoch / warmup_epochs
            else:
                # ä½™å¼¦é€€ç«
                lr = min_lr + (initial_lr - min_lr) * \
                    (1 + np.cos(np.pi * (epoch - warmup_epochs) / 
                               (epochs - warmup_epochs))) / 2
            schedule.append(lr)
        return schedule
    
    def cyclic_lr_schedule(self, base_lr=0.001, max_lr=0.1, step_size=20, epochs=100):
        """å¾ªç¯å­¦ä¹ ç‡"""
        schedule = []
        for epoch in range(epochs):
            cycle = np.floor(1 + epoch / (2 * step_size))
            x = np.abs(epoch / step_size - 2 * cycle + 1)
            lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))
            schedule.append(lr)
        return schedule
    
    def one_cycle_schedule(self, max_lr=0.1, epochs=100, pct_start=0.3):
        """One Cycleå­¦ä¹ ç‡"""
        schedule = []
        step_up = int(epochs * pct_start)
        step_down = epochs - step_up
        
        for epoch in range(epochs):
            if epoch <= step_up:
                # ä¸Šå‡é˜¶æ®µ
                lr = max_lr * epoch / step_up
            else:
                # ä¸‹é™é˜¶æ®µ
                lr = max_lr * (1 - (epoch - step_up) / step_down)
            schedule.append(lr)
        return schedule
    
    def visualize_lr_schedules(self):
        """å¯è§†åŒ–å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
        epochs = 100
        
        schedules = {
            'Step Decay': self.step_decay_schedule(epochs=epochs),
            'Exponential': self.exponential_decay_schedule(epochs=epochs),
            'Cosine Annealing': self.cosine_annealing_schedule(epochs=epochs),
            'Warmup+Cosine': self.warmup_cosine_schedule(epochs=epochs),
            'Cyclic LR': self.cyclic_lr_schedule(epochs=epochs),
            'One Cycle': self.one_cycle_schedule(epochs=epochs)
        }
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, (name, schedule) in enumerate(schedules.items()):
            axes[i].plot(range(epochs), schedule, linewidth=2)
            axes[i].set_title(name)
            axes[i].set_xlabel('Epoch')
            axes[i].set_ylabel('Learning Rate')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰€æœ‰ç­–ç•¥çš„æ¯”è¾ƒ
        plt.figure(figsize=(12, 8))
        
        colors = plt.cm.tab10(np.linspace(0, 1, len(schedules)))
        for (name, schedule), color in zip(schedules.items(), colors):
            plt.plot(range(epochs), schedule, label=name, linewidth=2, color=color)
        
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.title('å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥æ¯”è¾ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return schedules
    
    def analyze_schedule_effects(self):
        """åˆ†æä¸åŒè°ƒåº¦ç­–ç•¥çš„æ•ˆæœ"""
        print("=== å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥åˆ†æ ===")
        
        strategies = {
            "Step Decay": {
                "ä¼˜ç‚¹": ["ç®€å•æ˜“å®ç°", "é˜¶æ®µæ€§è°ƒæ•´"],
                "ç¼ºç‚¹": ["éœ€è¦æ‰‹åŠ¨è®¾å®šè¡°å‡ç‚¹", "å¯èƒ½è¿‡æ—©è¡°å‡"],
                "é€‚ç”¨": ["ä¼ ç»ŸCNNè®­ç»ƒ", "å›ºå®šè®­ç»ƒå‘¨æœŸ"]
            },
            "Exponential Decay": {
                "ä¼˜ç‚¹": ["å¹³æ»‘è¡°å‡", "è‡ªåŠ¨è°ƒæ•´"],
                "ç¼ºç‚¹": ["å¯èƒ½è¡°å‡è¿‡å¿«", "åæœŸå­¦ä¹ ç‡è¿‡å°"],
                "é€‚ç”¨": ["é•¿æœŸè®­ç»ƒ", "ç¨³å®šæ”¶æ•›åœºæ™¯"]
            },
            "Cosine Annealing": {
                "ä¼˜ç‚¹": ["å¹³æ»‘å˜åŒ–", "é¿å…å±€éƒ¨æœ€ä¼˜"],
                "ç¼ºç‚¹": ["éœ€è¦é¢„çŸ¥æ€»è®­ç»ƒè½®æ•°"],
                "é€‚ç”¨": ["ç°ä»£æ·±åº¦å­¦ä¹ ", "å›¾åƒåˆ†ç±»ä»»åŠ¡"]
            },
            "Warmup + Cosine": {
                "ä¼˜ç‚¹": ["ç¨³å®šçš„åˆå§‹åŒ–", "å¹³æ»‘æ”¶æ•›"],
                "ç¼ºç‚¹": ["å‚æ•°è¾ƒå¤š", "è°ƒå‚å¤æ‚"],
                "é€‚ç”¨": ["Transformerè®­ç»ƒ", "å¤§æ¨¡å‹è®­ç»ƒ"]
            },
            "Cyclic LR": {
                "ä¼˜ç‚¹": ["è·³å‡ºå±€éƒ¨æœ€ä¼˜", "åŠ é€Ÿæ”¶æ•›"],
                "ç¼ºç‚¹": ["å¯èƒ½ä¸ç¨³å®š", "éœ€è¦è°ƒå‚"],
                "é€‚ç”¨": ["æ¢ç´¢æ€§è®­ç»ƒ", "å¿«é€ŸåŸå‹"]
            },
            "One Cycle": {
                "ä¼˜ç‚¹": ["å¿«é€Ÿæ”¶æ•›", "é«˜å‡†ç¡®ç‡"],
                "ç¼ºç‚¹": ["éœ€è¦é¢„è®¾æœ€å¤§å­¦ä¹ ç‡"],
                "é€‚ç”¨": ["å¿«é€Ÿè®­ç»ƒ", "èµ„æºå—é™åœºæ™¯"]
            }
        }
        
        for strategy, details in strategies.items():
            print(f"\n{strategy}:")
            for key, values in details.items():
                print(f"  {key}: {', '.join(values)}")
        
        return strategies
```

## 6. ä¼˜åŒ–ç†è®ºæ·±å…¥ ğŸ”¬

```python
class OptimizationTheoryDeep:
    """ä¼˜åŒ–ç†è®ºæ·±å…¥åˆ†æ"""
    
    @staticmethod
    def convergence_analysis():
        """æ”¶æ•›æ€§åˆ†æ"""
        print("=== æ”¶æ•›æ€§ç†è®ºåˆ†æ ===")
        
        convergence_types = {
            "çº¿æ€§æ”¶æ•›": {
                "å®šä¹‰": "||x_{k+1} - x*|| â‰¤ c||x_k - x*||, 0 < c < 1",
                "æ”¶æ•›ç‡": "æŒ‡æ•°çº§",
                "ä»£è¡¨ç®—æ³•": ["æ¢¯åº¦ä¸‹é™(å¼ºå‡¸)", "åæ ‡ä¸‹é™"],
                "æ”¶æ•›é€Ÿåº¦": "è¾ƒæ…¢"
            },
            "è¶…çº¿æ€§æ”¶æ•›": {
                "å®šä¹‰": "lim(kâ†’âˆ) ||x_{k+1} - x*|| / ||x_k - x*|| = 0", 
                "æ”¶æ•›ç‡": "è¶…è¿‡çº¿æ€§",
                "ä»£è¡¨ç®—æ³•": ["BFGS", "SR1"],
                "æ”¶æ•›é€Ÿåº¦": "å¿«"
            },
            "äºŒæ¬¡æ”¶æ•›": {
                "å®šä¹‰": "||x_{k+1} - x*|| â‰¤ c||x_k - x*||Â²",
                "æ”¶æ•›ç‡": "å¹³æ–¹çº§",
                "ä»£è¡¨ç®—æ³•": ["ç‰›é¡¿æ³•"],
                "æ”¶æ•›é€Ÿåº¦": "éå¸¸å¿«"
            },
            "æ¬¡çº¿æ€§æ”¶æ•›": {
                "å®šä¹‰": "||x_{k+1} - x*|| / ||x_k - x*|| â†’ 1",
                "æ”¶æ•›ç‡": "æ…¢äºçº¿æ€§",
                "ä»£è¡¨ç®—æ³•": ["æ¢¯åº¦ä¸‹é™(éå¼ºå‡¸)"],
                "æ”¶æ•›é€Ÿåº¦": "æ…¢"
            }
        }
        
        for conv_type, details in convergence_types.items():
            print(f"\n{conv_type}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # å¯è§†åŒ–ä¸åŒæ”¶æ•›é€Ÿåº¦
        iterations = np.arange(0, 20)
        
        plt.figure(figsize=(12, 6))
        
        # çº¿æ€§æ”¶æ•›
        linear_conv = 0.8 ** iterations
        plt.subplot(1, 2, 1)
        plt.semilogy(iterations, linear_conv, 'b-', linewidth=2, label='çº¿æ€§æ”¶æ•› (c=0.8)')
        
        # è¶…çº¿æ€§æ”¶æ•›ï¼ˆè¿‘ä¼¼ï¼‰
        superlinear_conv = 0.8 ** (iterations ** 1.2)
        plt.semilogy(iterations, superlinear_conv, 'g-', linewidth=2, label='è¶…çº¿æ€§æ”¶æ•›')
        
        # äºŒæ¬¡æ”¶æ•›
        quadratic_conv = [1.0]
        for i in range(1, len(iterations)):
            quadratic_conv.append(min(quadratic_conv[-1]**2, 1e-15))
        plt.semilogy(iterations, quadratic_conv, 'r-', linewidth=2, label='äºŒæ¬¡æ”¶æ•›')
        
        # æ¬¡çº¿æ€§æ”¶æ•›
        sublinear_conv = 1.0 / (iterations + 1)
        plt.semilogy(iterations, sublinear_conv, 'm-', linewidth=2, label='æ¬¡çº¿æ€§æ”¶æ•›')
        
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('è¯¯å·® (log scale)')
        plt.title('ä¸åŒæ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å®é™…è¿­ä»£æ¬¡æ•°æ¯”è¾ƒ
        plt.subplot(1, 2, 2)
        target_error = 1e-6
        
        # è®¡ç®—è¾¾åˆ°ç›®æ ‡è¯¯å·®éœ€è¦çš„è¿­ä»£æ¬¡æ•°
        linear_iters = np.log(target_error) / np.log(0.8)
        sublinear_iters = 1.0 / target_error - 1
        
        methods = ['çº¿æ€§\n(å¼ºå‡¸)', 'è¶…çº¿æ€§\n(BFGS)', 'äºŒæ¬¡\n(ç‰›é¡¿)', 'æ¬¡çº¿æ€§\n(éå¼ºå‡¸)']
        iterations_needed = [linear_iters, linear_iters*0.3, 10, min(sublinear_iters, 10000)]
        
        bars = plt.bar(methods, iterations_needed, 
                      color=['blue', 'green', 'red', 'magenta'], alpha=0.7)
        plt.ylabel('è¾¾åˆ°1e-6è¯¯å·®çš„è¿­ä»£æ¬¡æ•°')
        plt.title('æ”¶æ•›é€Ÿåº¦å®é™…æ¯”è¾ƒ')
        plt.yscale('log')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, iters in zip(bars, iterations_needed):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(iters)}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        return convergence_types
    
    @staticmethod
    def lipschitz_smoothness():
        """Lipschitzå¹³æ»‘æ€§åˆ†æ"""
        print("=== Lipschitzå¹³æ»‘æ€§ ===")
        
        print("å®šä¹‰:")
        print("å‡½æ•°fæ˜¯L-smoothçš„ï¼Œå¦‚æœ:")
        print("||âˆ‡f(x) - âˆ‡f(y)|| â‰¤ L||x - y||")
        print()
        
        print("æ„ä¹‰:")
        print("- Læ˜¯æ¢¯åº¦çš„Lipschitzå¸¸æ•°")
        print("- æ¢¯åº¦å˜åŒ–ä¸ä¼šå¤ªå‰§çƒˆ")
        print("- å…è®¸ä½¿ç”¨å›ºå®šæ­¥é•¿")
        print("- æ­¥é•¿ä¸Šç•Œ: Î± â‰¤ 2/L")
        print()
        
        # å¯è§†åŒ–Lipschitzå¹³æ»‘å‡½æ•°
        x = np.linspace(-2, 2, 1000)
        
        plt.figure(figsize=(15, 5))
        
        # L-smoothå‡½æ•°ç¤ºä¾‹
        plt.subplot(1, 3, 1)
        f1 = x**2  # L = 2
        plt.plot(x, f1, 'b-', linewidth=2, label='f(x) = xÂ² (L=2)')
        plt.title('L-smoothå‡½æ•°')
        plt.xlabel('x')
        plt.ylabel('f(x)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ésmoothå‡½æ•°ç¤ºä¾‹
        plt.subplot(1, 3, 2)
        f2 = np.abs(x)**1.5  # ä¸å¯å¾®
        plt.plot(x, f2, 'r-', linewidth=2, label='f(x) = |x|^1.5')
        plt.title('ésmoothå‡½æ•°')
        plt.xlabel('x')
        plt.ylabel('f(x)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ¢¯åº¦æ¯”è¾ƒ
        plt.subplot(1, 3, 3)
        grad1 = 2 * x  # smoothå‡½æ•°çš„æ¢¯åº¦
        grad2 = 1.5 * np.sign(x) * np.abs(x)**0.5  # ésmoothå‡½æ•°çš„æ¢¯åº¦
        
        plt.plot(x, grad1, 'b-', linewidth=2, label="âˆ‡f(x) = 2x")
        plt.plot(x, grad2, 'r-', linewidth=2, label="âˆ‡f(x) = 1.5|x|^0.5Â·sign(x)")
        plt.title('æ¢¯åº¦æ¯”è¾ƒ')
        plt.xlabel('x')
        plt.ylabel("âˆ‡f(x)")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return None
    
    @staticmethod
    def strong_convexity():
        """å¼ºå‡¸æ€§åˆ†æ"""
        print("=== å¼ºå‡¸æ€§ ===")
        
        print("å®šä¹‰:")
        print("å‡½æ•°fæ˜¯Î¼-å¼ºå‡¸çš„ï¼Œå¦‚æœ:")
        print("f(y) â‰¥ f(x) + âˆ‡f(x)áµ€(y-x) + Î¼/2||y-x||Â²")
        print()
        
        print("ç­‰ä»·æ¡ä»¶:")
        print("- HessiançŸ©é˜µ: H âª° Î¼I (æ‰€æœ‰ç‰¹å¾å€¼ â‰¥ Î¼)")
        print("- æ¢¯åº¦æ¡ä»¶: (âˆ‡f(x)-âˆ‡f(y))áµ€(x-y) â‰¥ Î¼||x-y||Â²")
        print()
        
        print("æ”¶æ•›ä¿è¯:")
        print("- æ¢¯åº¦ä¸‹é™çº¿æ€§æ”¶æ•›")
        print("- æ”¶æ•›ç‡: (1 - Î¼/L)^k")
        print("- æ¡ä»¶æ•°: Îº = L/Î¼")
        print()
        
        # å¯è§†åŒ–å¼ºå‡¸å‡½æ•°
        x = np.linspace(-2, 2, 100)
        y = np.linspace(-2, 2, 100)
        X, Y = np.meshgrid(x, y)
        
        # å¼ºå‡¸å‡½æ•°ï¼šf(x,y) = 2xÂ² + 3yÂ²
        Z1 = 2*X**2 + 3*Y**2
        
        # å‡¸ä½†éå¼ºå‡¸ï¼šf(x,y) = xÂ² + 0.1yÂ²  
        Z2 = X**2 + 0.1*Y**2
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # å¼ºå‡¸å‡½æ•°
        contour1 = axes[0].contour(X, Y, Z1, levels=20)
        axes[0].clabel(contour1, inline=True, fontsize=8)
        axes[0].set_title('å¼ºå‡¸å‡½æ•°\n(Î¼=2, L=3, Îº=1.5)')
        axes[0].set_xlabel('x')
        axes[0].set_ylabel('y')
        axes[0].set_aspect('equal')
        
        # å‡¸ä½†éå¼ºå‡¸å‡½æ•°
        contour2 = axes[1].contour(X, Y, Z2, levels=20)
        axes[1].clabel(contour2, inline=True, fontsize=8)
        axes[1].set_title('å‡¸ä½†éå¼ºå‡¸\n(Î¼â‰ˆ0, L=1, Îºâ†’âˆ)')
        axes[1].set_xlabel('x')
        axes[1].set_ylabel('y')
        axes[1].set_aspect('equal')
        
        plt.tight_layout()
        plt.show()
        
        return None
    
    @staticmethod
    def condition_number_analysis():
        """æ¡ä»¶æ•°åˆ†æ"""
        print("=== æ¡ä»¶æ•°åˆ†æ ===")
        
        print("å®šä¹‰:")
        print("Îº = L/Î¼ (æœ€å¤§ç‰¹å¾å€¼/æœ€å°ç‰¹å¾å€¼)")
        print()
        
        print("å½±å“:")
        print("- Îº = 1: çƒå½¢å‡½æ•°ï¼Œæ”¶æ•›æœ€å¿«") 
        print("- Îº >> 1: æ¤­åœ†å½¢å‡½æ•°ï¼Œæ”¶æ•›æ…¢")
        print("- Îº â†’ âˆ: ç—…æ€é—®é¢˜ï¼Œéš¾ä»¥ä¼˜åŒ–")
        print()
        
        # å¯è§†åŒ–ä¸åŒæ¡ä»¶æ•°çš„å½±å“
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        condition_numbers = [1, 5, 20]
        x = np.linspace(-2, 2, 100)
        y = np.linspace(-2, 2, 100)
        X, Y = np.meshgrid(x, y)
        
        for i, kappa in enumerate(condition_numbers):
            # åˆ›å»ºå…·æœ‰æŒ‡å®šæ¡ä»¶æ•°çš„å‡½æ•°
            Z = X**2 + kappa * Y**2
            
            # ç­‰é«˜çº¿å›¾
            axes[0, i].contour(X, Y, Z, levels=15, alpha=0.7)
            axes[0, i].set_title(f'æ¡ä»¶æ•° Îº = {kappa}')
            axes[0, i].set_xlabel('x')
            axes[0, i].set_ylabel('y')
            axes[0, i].set_aspect('equal')
            
            # æ¨¡æ‹Ÿæ¢¯åº¦ä¸‹é™è½¨è¿¹
            def grad_func(pos):
                return np.array([2*pos[0], 2*kappa*pos[1]])
            
            # æ¢¯åº¦ä¸‹é™
            pos = np.array([1.5, 1.5])
            trajectory = [pos.copy()]
            lr = 0.1 / kappa  # è°ƒæ•´å­¦ä¹ ç‡é¿å…å‘æ•£
            
            for _ in range(50):
                grad = grad_func(pos)
                pos = pos - lr * grad
                trajectory.append(pos.copy())
                
                if np.linalg.norm(grad) < 1e-3:
                    break
            
            trajectory = np.array(trajectory)
            axes[0, i].plot(trajectory[:, 0], trajectory[:, 1], 'r-o', 
                          markersize=3, alpha=0.7, label='æ¢¯åº¦ä¸‹é™è·¯å¾„')
            axes[0, i].legend()
            
            # æ”¶æ•›æ›²çº¿
            distances = [np.linalg.norm(p) for p in trajectory]
            axes[1, i].semilogy(distances, 'b-', linewidth=2)
            axes[1, i].set_xlabel('è¿­ä»£æ¬¡æ•°')
            axes[1, i].set_ylabel('è·ç¦»åŸç‚¹ (log)')
            axes[1, i].set_title(f'æ”¶æ•›æ›²çº¿ (Îº = {kappa})')
            axes[1, i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return condition_numbers

def comprehensive_optimization_summary():
    """ç»¼åˆä¼˜åŒ–ç†è®ºæ€»ç»“"""
    print("=== ä¼˜åŒ–ç®—æ³•ç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "ç®—æ³•åˆ†ç±»": {
            "é›¶é˜¶æ–¹æ³•": "ä¸ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ (é—ä¼ ç®—æ³•ã€æ¨¡æ‹Ÿé€€ç«)",
            "ä¸€é˜¶æ–¹æ³•": "ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ (SGDã€Adamç­‰)",
            "äºŒé˜¶æ–¹æ³•": "ä½¿ç”¨Hessianä¿¡æ¯ (ç‰›é¡¿æ³•ã€BFGS)"
        },
        
        "æ”¶æ•›é€Ÿåº¦": {
            "æ¬¡çº¿æ€§": "O(1/k) - æ¢¯åº¦ä¸‹é™(éå¼ºå‡¸)",
            "çº¿æ€§": "O(Ïáµ) - æ¢¯åº¦ä¸‹é™(å¼ºå‡¸)", 
            "è¶…çº¿æ€§": "æ¯”çº¿æ€§å¿« - BFGS",
            "äºŒæ¬¡": "O(ÏáµÂ²) - ç‰›é¡¿æ³•"
        },
        
        "é€‚ç”¨åœºæ™¯": {
            "SGD": "å¤§è§„æ¨¡ã€éšæœºä¼˜åŒ–",
            "Adam": "æ·±åº¦å­¦ä¹ é»˜è®¤é€‰æ‹©",
            "L-BFGS": "å°è§„æ¨¡ã€ç²¾ç¡®ä¼˜åŒ–",
            "ç‰›é¡¿æ³•": "å°è§„æ¨¡ã€äºŒæ¬¡å‡½æ•°"
        },
        
        "è°ƒå‚å»ºè®®": {
            "å­¦ä¹ ç‡": "æœ€é‡è¦çš„è¶…å‚æ•°",
            "æ‰¹æ¬¡å¤§å°": "å½±å“å™ªå£°å’Œå¹¶è¡Œæ€§",
            "åŠ¨é‡ç³»æ•°": "åŠ é€Ÿæ”¶æ•›ï¼Œé¿å…éœ‡è¡",
            "æ­£åˆ™åŒ–": "é˜²æ­¢è¿‡æ‹Ÿåˆ"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("ä¼˜åŒ–ç®—æ³•ç†è®ºæŒ‡å—åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Nocedal & Wright (2006): "Numerical Optimization"
- Boyd & Vandenberghe (2004): "Convex Optimization"
- Ruder (2016): "An overview of gradient descent optimization algorithms"
- Kingma & Ba (2014): "Adam: A Method for Stochastic Optimization"
- Robbins & Monro (1951): "A Stochastic Approximation Method"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [æ·±åº¦å­¦ä¹ åŸºç¡€](../dl/basics.md) - ç¥ç»ç½‘ç»œä¼˜åŒ–åº”ç”¨
- [æ¨¡å‹è®­ç»ƒæŠ€å·§](../dl/training_tricks.md) - å®è·µä¼˜åŒ–æŠ€æœ¯
- [è¶…å‚æ•°è°ƒä¼˜](hyperparameter_tuning.md) - è‡ªåŠ¨åŒ–å‚æ•°æœç´¢