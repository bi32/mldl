# æœºå™¨å­¦ä¹ åŸºç¡€ï¼šä»é›¶å¼€å§‹ ğŸš€

å…¨é¢æŒæ¡æœºå™¨å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µã€ç†è®ºå’Œå®è·µã€‚

## 1. æœºå™¨å­¦ä¹ æ¦‚è¿° ğŸŒŸ

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®å¯è§†åŒ–é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12

class MachineLearningBasics:
    """æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ"""
    
    def __init__(self):
        self.concepts = {
            "ç›‘ç£å­¦ä¹ ": "ä»æ ‡æ³¨æ•°æ®å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„",
            "æ— ç›‘ç£å­¦ä¹ ": "ä»æ— æ ‡æ³¨æ•°æ®å‘ç°æ¨¡å¼",
            "å¼ºåŒ–å­¦ä¹ ": "é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥",
            "åŠç›‘ç£å­¦ä¹ ": "ç»“åˆå°‘é‡æ ‡æ³¨æ•°æ®å’Œå¤§é‡æ— æ ‡æ³¨æ•°æ®",
            "è¿ç§»å­¦ä¹ ": "å°†ä¸€ä¸ªä»»åŠ¡çš„çŸ¥è¯†è¿ç§»åˆ°å¦ä¸€ä¸ªä»»åŠ¡"
        }
    
    def demonstrate_ml_workflow(self):
        """æ¼”ç¤ºæœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹"""
        print("=" * 50)
        print("æœºå™¨å­¦ä¹ æ ‡å‡†å·¥ä½œæµç¨‹")
        print("=" * 50)
        
        # 1. æ•°æ®æ”¶é›†
        print("\n1. æ•°æ®æ”¶é›†")
        print("   - ç¡®å®šæ•°æ®æº")
        print("   - æ”¶é›†åŸå§‹æ•°æ®")
        print("   - æ•°æ®è´¨é‡æ£€æŸ¥")
        
        # 2. æ•°æ®é¢„å¤„ç†
        print("\n2. æ•°æ®é¢„å¤„ç†")
        print("   - å¤„ç†ç¼ºå¤±å€¼")
        print("   - å¤„ç†å¼‚å¸¸å€¼")
        print("   - ç‰¹å¾ç¼–ç ")
        print("   - ç‰¹å¾ç¼©æ”¾")
        
        # 3. ç‰¹å¾å·¥ç¨‹
        print("\n3. ç‰¹å¾å·¥ç¨‹")
        print("   - ç‰¹å¾é€‰æ‹©")
        print("   - ç‰¹å¾æå–")
        print("   - ç‰¹å¾åˆ›å»º")
        
        # 4. æ¨¡å‹é€‰æ‹©
        print("\n4. æ¨¡å‹é€‰æ‹©")
        print("   - é€‰æ‹©ç®—æ³•")
        print("   - è®¾ç½®åŸºå‡†æ¨¡å‹")
        
        # 5. æ¨¡å‹è®­ç»ƒ
        print("\n5. æ¨¡å‹è®­ç»ƒ")
        print("   - åˆ†å‰²æ•°æ®é›†")
        print("   - è®­ç»ƒæ¨¡å‹")
        print("   - äº¤å‰éªŒè¯")
        
        # 6. æ¨¡å‹è¯„ä¼°
        print("\n6. æ¨¡å‹è¯„ä¼°")
        print("   - é€‰æ‹©è¯„ä¼°æŒ‡æ ‡")
        print("   - éªŒè¯é›†è¯„ä¼°")
        print("   - æµ‹è¯•é›†è¯„ä¼°")
        
        # 7. æ¨¡å‹ä¼˜åŒ–
        print("\n7. æ¨¡å‹ä¼˜åŒ–")
        print("   - è¶…å‚æ•°è°ƒä¼˜")
        print("   - ç‰¹å¾ä¼˜åŒ–")
        print("   - é›†æˆæ–¹æ³•")
        
        # 8. æ¨¡å‹éƒ¨ç½²
        print("\n8. æ¨¡å‹éƒ¨ç½²")
        print("   - æ¨¡å‹åºåˆ—åŒ–")
        print("   - APIå¼€å‘")
        print("   - ç›‘æ§ç»´æŠ¤")
    
    def create_sample_dataset(self, n_samples=1000, n_features=4, 
                            task='classification'):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
        np.random.seed(42)
        
        if task == 'classification':
            from sklearn.datasets import make_classification
            X, y = make_classification(
                n_samples=n_samples,
                n_features=n_features,
                n_informative=3,
                n_redundant=1,
                n_clusters_per_class=2,
                random_state=42
            )
        else:
            from sklearn.datasets import make_regression
            X, y = make_regression(
                n_samples=n_samples,
                n_features=n_features,
                n_informative=3,
                noise=10,
                random_state=42
            )
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
        df['target'] = y
        
        return df
```

## 2. æ•°æ®é¢„å¤„ç† ğŸ”§

```python
class DataPreprocessing:
    """æ•°æ®é¢„å¤„ç†æŠ€æœ¯"""
    
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
    
    def handle_missing_data(self, df, strategy='mean'):
        """å¤„ç†ç¼ºå¤±æ•°æ®"""
        df_processed = df.copy()
        
        # æ•°å€¼å‹ç‰¹å¾
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        if strategy == 'mean':
            for col in numeric_columns:
                df_processed[col].fillna(df[col].mean(), inplace=True)
        elif strategy == 'median':
            for col in numeric_columns:
                df_processed[col].fillna(df[col].median(), inplace=True)
        elif strategy == 'mode':
            for col in numeric_columns:
                df_processed[col].fillna(df[col].mode()[0], inplace=True)
        elif strategy == 'forward_fill':
            df_processed.fillna(method='ffill', inplace=True)
        elif strategy == 'backward_fill':
            df_processed.fillna(method='bfill', inplace=True)
        elif strategy == 'interpolate':
            df_processed[numeric_columns] = df[numeric_columns].interpolate()
        
        # ç±»åˆ«å‹ç‰¹å¾
        categorical_columns = df.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            df_processed[col].fillna('Unknown', inplace=True)
        
        return df_processed
    
    def scale_features(self, X, method='standard'):
        """ç‰¹å¾ç¼©æ”¾"""
        from sklearn.preprocessing import (
            StandardScaler, MinMaxScaler, RobustScaler, Normalizer
        )
        
        if method == 'standard':
            scaler = StandardScaler()
        elif method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'robust':
            scaler = RobustScaler()
        elif method == 'normalize':
            scaler = Normalizer()
        else:
            return X
        
        X_scaled = scaler.fit_transform(X)
        self.scalers[method] = scaler
        
        return X_scaled
    
    def encode_categorical(self, df, columns, method='onehot'):
        """ç¼–ç ç±»åˆ«å˜é‡"""
        from sklearn.preprocessing import LabelEncoder, OneHotEncoder
        
        df_encoded = df.copy()
        
        if method == 'label':
            for col in columns:
                le = LabelEncoder()
                df_encoded[col] = le.fit_transform(df[col])
                self.encoders[col] = le
        
        elif method == 'onehot':
            df_encoded = pd.get_dummies(df, columns=columns)
        
        elif method == 'target':
            # ç›®æ ‡ç¼–ç 
            for col in columns:
                mean_target = df.groupby(col)['target'].mean()
                df_encoded[col] = df[col].map(mean_target)
        
        return df_encoded
    
    def remove_outliers(self, df, columns, method='iqr', threshold=1.5):
        """ç§»é™¤å¼‚å¸¸å€¼"""
        df_clean = df.copy()
        
        if method == 'iqr':
            for col in columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR
                
                df_clean = df_clean[(df_clean[col] >= lower_bound) & 
                                   (df_clean[col] <= upper_bound)]
        
        elif method == 'zscore':
            from scipy import stats
            for col in columns:
                z_scores = np.abs(stats.zscore(df[col]))
                df_clean = df_clean[z_scores < threshold]
        
        return df_clean
    
    def split_data(self, X, y, test_size=0.2, val_size=0.2, random_state=42):
        """åˆ†å‰²æ•°æ®é›†"""
        # é¦–å…ˆåˆ†å‰²å‡ºæµ‹è¯•é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
        val_size_adjusted = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted, 
            random_state=random_state
        )
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
        print(f"éªŒè¯é›†å¤§å°: {len(X_val)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test

# æ•°æ®å¯è§†åŒ–
class DataVisualization:
    """æ•°æ®å¯è§†åŒ–å·¥å…·"""
    
    @staticmethod
    def plot_data_distribution(df, columns=None):
        """ç»˜åˆ¶æ•°æ®åˆ†å¸ƒ"""
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns
        
        n_cols = len(columns)
        n_rows = (n_cols + 2) // 3
        
        fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))
        axes = axes.flatten()
        
        for i, col in enumerate(columns):
            axes[i].hist(df[col], bins=30, edgecolor='black')
            axes[i].set_title(f'Distribution of {col}')
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('Frequency')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_cols, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def plot_correlation_matrix(df):
        """ç»˜åˆ¶ç›¸å…³æ€§çŸ©é˜µ"""
        numeric_df = df.select_dtypes(include=[np.number])
        correlation = numeric_df.corr()
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(correlation, annot=True, cmap='coolwarm', 
                   center=0, vmin=-1, vmax=1)
        plt.title('Feature Correlation Matrix')
        plt.show()
        
        return correlation
    
    @staticmethod
    def plot_feature_importance(feature_names, importance_scores):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        indices = np.argsort(importance_scores)[::-1]
        
        plt.figure(figsize=(10, 6))
        plt.bar(range(len(importance_scores)), importance_scores[indices])
        plt.xticks(range(len(importance_scores)), 
                  [feature_names[i] for i in indices], rotation=45)
        plt.xlabel('Features')
        plt.ylabel('Importance Score')
        plt.title('Feature Importance')
        plt.tight_layout()
        plt.show()
```

## 3. ç›‘ç£å­¦ä¹ åŸºç¡€ ğŸ“š

```python
class SupervisedLearning:
    """ç›‘ç£å­¦ä¹ åŸºç¡€"""
    
    def __init__(self):
        self.models = {}
        self.results = {}
    
    def train_linear_regression(self, X_train, y_train, X_test, y_test):
        """çº¿æ€§å›å½’ç¤ºä¾‹"""
        from sklearn.linear_model import LinearRegression
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è¯„ä¼°
        train_mse = mean_squared_error(y_train, y_pred_train)
        test_mse = mean_squared_error(y_test, y_pred_test)
        
        print("çº¿æ€§å›å½’ç»“æœ:")
        print(f"è®­ç»ƒMSE: {train_mse:.4f}")
        print(f"æµ‹è¯•MSE: {test_mse:.4f}")
        print(f"ç³»æ•°: {model.coef_}")
        print(f"æˆªè·: {model.intercept_:.4f}")
        
        self.models['linear_regression'] = model
        return model
    
    def train_logistic_regression(self, X_train, y_train, X_test, y_test):
        """é€»è¾‘å›å½’ç¤ºä¾‹"""
        from sklearn.linear_model import LogisticRegression
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = LogisticRegression(random_state=42)
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è¯„ä¼°
        train_acc = accuracy_score(y_train, y_pred_train)
        test_acc = accuracy_score(y_test, y_pred_test)
        
        print("é€»è¾‘å›å½’ç»“æœ:")
        print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        
        self.models['logistic_regression'] = model
        return model
    
    def train_decision_tree(self, X_train, y_train, X_test, y_test, 
                          task='classification'):
        """å†³ç­–æ ‘ç¤ºä¾‹"""
        if task == 'classification':
            from sklearn.tree import DecisionTreeClassifier
            model = DecisionTreeClassifier(max_depth=5, random_state=42)
            metric = accuracy_score
            metric_name = "å‡†ç¡®ç‡"
        else:
            from sklearn.tree import DecisionTreeRegressor
            model = DecisionTreeRegressor(max_depth=5, random_state=42)
            metric = mean_squared_error
            metric_name = "MSE"
        
        # è®­ç»ƒ
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è¯„ä¼°
        train_score = metric(y_train, y_pred_train)
        test_score = metric(y_test, y_pred_test)
        
        print(f"å†³ç­–æ ‘ç»“æœ:")
        print(f"è®­ç»ƒ{metric_name}: {train_score:.4f}")
        print(f"æµ‹è¯•{metric_name}: {test_score:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§
        if hasattr(X_train, 'columns'):
            feature_importance = pd.DataFrame({
                'feature': X_train.columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            print("\nç‰¹å¾é‡è¦æ€§:")
            print(feature_importance.head())
        
        self.models['decision_tree'] = model
        return model
    
    def train_svm(self, X_train, y_train, X_test, y_test):
        """æ”¯æŒå‘é‡æœºç¤ºä¾‹"""
        from sklearn.svm import SVC
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = SVC(kernel='rbf', random_state=42)
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è¯„ä¼°
        train_acc = accuracy_score(y_train, y_pred_train)
        test_acc = accuracy_score(y_test, y_pred_test)
        
        print("SVMç»“æœ:")
        print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"æ”¯æŒå‘é‡æ•°: {len(model.support_)}")
        
        self.models['svm'] = model
        return model
    
    def train_random_forest(self, X_train, y_train, X_test, y_test):
        """éšæœºæ£®æ—ç¤ºä¾‹"""
        from sklearn.ensemble import RandomForestClassifier
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è¯„ä¼°
        train_acc = accuracy_score(y_train, y_pred_train)
        test_acc = accuracy_score(y_test, y_pred_test)
        
        print("éšæœºæ£®æ—ç»“æœ:")
        print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        
        self.models['random_forest'] = model
        return model
```

## 4. æ— ç›‘ç£å­¦ä¹ åŸºç¡€ ğŸ”

```python
class UnsupervisedLearning:
    """æ— ç›‘ç£å­¦ä¹ åŸºç¡€"""
    
    def __init__(self):
        self.models = {}
    
    def kmeans_clustering(self, X, n_clusters=3):
        """Kå‡å€¼èšç±»"""
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = KMeans(n_clusters=n_clusters, random_state=42)
        labels = model.fit_predict(X)
        
        # è¯„ä¼°
        silhouette = silhouette_score(X, labels)
        inertia = model.inertia_
        
        print(f"K-Meansèšç±»ç»“æœ (k={n_clusters}):")
        print(f"Silhouette Score: {silhouette:.4f}")
        print(f"Inertia: {inertia:.4f}")
        
        self.models['kmeans'] = model
        
        # å¯è§†åŒ–ï¼ˆå¦‚æœæ˜¯2Dï¼‰
        if X.shape[1] == 2:
            plt.figure(figsize=(10, 6))
            plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
            plt.scatter(model.cluster_centers_[:, 0], 
                       model.cluster_centers_[:, 1],
                       marker='x', s=200, linewidths=3, 
                       color='red', label='Centroids')
            plt.title('K-Means Clustering')
            plt.legend()
            plt.show()
        
        return labels
    
    def hierarchical_clustering(self, X, n_clusters=3):
        """å±‚æ¬¡èšç±»"""
        from sklearn.cluster import AgglomerativeClustering
        from scipy.cluster.hierarchy import dendrogram, linkage
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = AgglomerativeClustering(n_clusters=n_clusters)
        labels = model.fit_predict(X)
        
        print(f"å±‚æ¬¡èšç±»ç»“æœ (n_clusters={n_clusters}):")
        print(f"èšç±»æ ‡ç­¾: {np.unique(labels)}")
        
        # ç»˜åˆ¶æ ‘çŠ¶å›¾
        if len(X) < 100:  # åªå¯¹å°æ•°æ®é›†ç»˜åˆ¶
            plt.figure(figsize=(12, 6))
            linkage_matrix = linkage(X, method='ward')
            dendrogram(linkage_matrix)
            plt.title('Hierarchical Clustering Dendrogram')
            plt.xlabel('Sample Index')
            plt.ylabel('Distance')
            plt.show()
        
        self.models['hierarchical'] = model
        return labels
    
    def dbscan_clustering(self, X, eps=0.5, min_samples=5):
        """DBSCANèšç±»"""
        from sklearn.cluster import DBSCAN
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = DBSCAN(eps=eps, min_samples=min_samples)
        labels = model.fit_predict(X)
        
        # ç»Ÿè®¡
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        print(f"DBSCANèšç±»ç»“æœ:")
        print(f"èšç±»æ•°: {n_clusters}")
        print(f"å™ªå£°ç‚¹æ•°: {n_noise}")
        
        self.models['dbscan'] = model
        return labels
    
    def pca_reduction(self, X, n_components=2):
        """ä¸»æˆåˆ†åˆ†æ"""
        from sklearn.decomposition import PCA
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        pca = PCA(n_components=n_components)
        X_reduced = pca.fit_transform(X)
        
        # è§£é‡Šæ–¹å·®
        explained_variance_ratio = pca.explained_variance_ratio_
        cumulative_variance = np.cumsum(explained_variance_ratio)
        
        print(f"PCAé™ç»´ç»“æœ:")
        print(f"è§£é‡Šæ–¹å·®æ¯”: {explained_variance_ratio}")
        print(f"ç´¯ç§¯è§£é‡Šæ–¹å·®: {cumulative_variance}")
        
        # å¯è§†åŒ–
        if n_components >= 2:
            plt.figure(figsize=(10, 6))
            plt.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.5)
            plt.xlabel(f'PC1 ({explained_variance_ratio[0]:.2%} variance)')
            plt.ylabel(f'PC2 ({explained_variance_ratio[1]:.2%} variance)')
            plt.title('PCA Visualization')
            plt.show()
        
        self.models['pca'] = pca
        return X_reduced
    
    def anomaly_detection(self, X, contamination=0.1):
        """å¼‚å¸¸æ£€æµ‹"""
        from sklearn.ensemble import IsolationForest
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = IsolationForest(contamination=contamination, random_state=42)
        predictions = model.fit_predict(X)
        
        # ç»Ÿè®¡
        n_outliers = list(predictions).count(-1)
        n_inliers = list(predictions).count(1)
        
        print(f"å¼‚å¸¸æ£€æµ‹ç»“æœ:")
        print(f"æ­£å¸¸ç‚¹: {n_inliers}")
        print(f"å¼‚å¸¸ç‚¹: {n_outliers}")
        print(f"å¼‚å¸¸æ¯”ä¾‹: {n_outliers/len(X):.2%}")
        
        self.models['isolation_forest'] = model
        return predictions
```

## 5. æ¨¡å‹é€‰æ‹©ä¸éªŒè¯ âœ…

```python
class ModelSelection:
    """æ¨¡å‹é€‰æ‹©ä¸éªŒè¯"""
    
    def __init__(self):
        self.best_model = None
        self.best_params = None
    
    def compare_models(self, X, y, models_dict, cv=5):
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹"""
        from sklearn.model_selection import cross_val_score
        
        results = []
        
        for name, model in models_dict.items():
            scores = cross_val_score(model, X, y, cv=cv, 
                                    scoring='accuracy')
            results.append({
                'Model': name,
                'Mean Score': scores.mean(),
                'Std Score': scores.std(),
                'Min Score': scores.min(),
                'Max Score': scores.max()
            })
            print(f"{name}: {scores.mean():.4f} (+/- {scores.std():.4f})")
        
        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('Mean Score', ascending=False)
        
        # å¯è§†åŒ–
        plt.figure(figsize=(10, 6))
        plt.errorbar(results_df['Model'], results_df['Mean Score'],
                    yerr=results_df['Std Score'], fmt='o', capsize=5)
        plt.xlabel('Model')
        plt.ylabel('Accuracy')
        plt.title('Model Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        return results_df
    
    def grid_search(self, model, param_grid, X, y, cv=5):
        """ç½‘æ ¼æœç´¢"""
        from sklearn.model_selection import GridSearchCV
        
        grid_search = GridSearchCV(
            model, param_grid, cv=cv, 
            scoring='accuracy', n_jobs=-1
        )
        grid_search.fit(X, y)
        
        print("ç½‘æ ¼æœç´¢ç»“æœ:")
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.4f}")
        
        self.best_model = grid_search.best_estimator_
        self.best_params = grid_search.best_params_
        
        return grid_search
    
    def random_search(self, model, param_distributions, X, y, 
                     n_iter=100, cv=5):
        """éšæœºæœç´¢"""
        from sklearn.model_selection import RandomizedSearchCV
        
        random_search = RandomizedSearchCV(
            model, param_distributions, 
            n_iter=n_iter, cv=cv,
            scoring='accuracy', n_jobs=-1,
            random_state=42
        )
        random_search.fit(X, y)
        
        print("éšæœºæœç´¢ç»“æœ:")
        print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
        print(f"æœ€ä½³åˆ†æ•°: {random_search.best_score_:.4f}")
        
        self.best_model = random_search.best_estimator_
        self.best_params = random_search.best_params_
        
        return random_search
    
    def learning_curve_analysis(self, model, X, y, cv=5):
        """å­¦ä¹ æ›²çº¿åˆ†æ"""
        from sklearn.model_selection import learning_curve
        
        train_sizes, train_scores, val_scores = learning_curve(
            model, X, y, cv=cv,
            train_sizes=np.linspace(0.1, 1.0, 10),
            n_jobs=-1
        )
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_mean, 'o-', label='Training Score')
        plt.plot(train_sizes, val_mean, 'o-', label='Validation Score')
        plt.fill_between(train_sizes, train_mean - train_std,
                        train_mean + train_std, alpha=0.1)
        plt.fill_between(train_sizes, val_mean - val_std,
                        val_mean + val_std, alpha=0.1)
        plt.xlabel('Training Set Size')
        plt.ylabel('Score')
        plt.title('Learning Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        # è¯Šæ–­
        final_train = train_mean[-1]
        final_val = val_mean[-1]
        gap = final_train - final_val
        
        if gap > 0.1:
            print("è¯Šæ–­: å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
            print("å»ºè®®: å¢åŠ æ­£åˆ™åŒ–ã€å‡å°‘æ¨¡å‹å¤æ‚åº¦æˆ–å¢åŠ æ•°æ®")
        elif final_val < 0.7:
            print("è¯Šæ–­: å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆ")
            print("å»ºè®®: å¢åŠ æ¨¡å‹å¤æ‚åº¦æˆ–æ”¹è¿›ç‰¹å¾")
        else:
            print("è¯Šæ–­: æ¨¡å‹è¡¨ç°è‰¯å¥½")
```

## 6. å®æˆ˜é¡¹ç›®ç¤ºä¾‹ ğŸ’¼

```python
class MLProject:
    """å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®"""
    
    def __init__(self):
        self.preprocessor = DataPreprocessing()
        self.visualizer = DataVisualization()
        self.supervised = SupervisedLearning()
        self.selector = ModelSelection()
    
    def run_classification_project(self):
        """è¿è¡Œåˆ†ç±»é¡¹ç›®"""
        print("=" * 50)
        print("æœºå™¨å­¦ä¹ åˆ†ç±»é¡¹ç›®ç¤ºä¾‹")
        print("=" * 50)
        
        # 1. åˆ›å»ºæ•°æ®
        print("\n1. åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
        basics = MachineLearningBasics()
        df = basics.create_sample_dataset(n_samples=1000, task='classification')
        
        # 2. æ•°æ®æ¢ç´¢
        print("\n2. æ•°æ®æ¢ç´¢...")
        print(df.head())
        print(f"\næ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ç±»åˆ«åˆ†å¸ƒ:\n{df['target'].value_counts()}")
        
        # 3. æ•°æ®å¯è§†åŒ–
        print("\n3. æ•°æ®å¯è§†åŒ–...")
        self.visualizer.plot_correlation_matrix(df)
        
        # 4. æ•°æ®é¢„å¤„ç†
        print("\n4. æ•°æ®é¢„å¤„ç†...")
        X = df.drop('target', axis=1)
        y = df['target']
        
        # åˆ†å‰²æ•°æ®
        X_train, X_val, X_test, y_train, y_val, y_test = \
            self.preprocessor.split_data(X, y)
        
        # ç‰¹å¾ç¼©æ”¾
        X_train_scaled = self.preprocessor.scale_features(X_train)
        X_val_scaled = self.preprocessor.scale_features(X_val)
        X_test_scaled = self.preprocessor.scale_features(X_test)
        
        # 5. æ¨¡å‹è®­ç»ƒ
        print("\n5. è®­ç»ƒå¤šä¸ªæ¨¡å‹...")
        models = {
            'Logistic Regression': LogisticRegression(),
            'Decision Tree': DecisionTreeClassifier(),
            'Random Forest': RandomForestClassifier(),
            'SVM': SVC()
        }
        
        from sklearn.linear_model import LogisticRegression
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.svm import SVC
        
        # 6. æ¨¡å‹æ¯”è¾ƒ
        print("\n6. æ¨¡å‹æ¯”è¾ƒ...")
        results = self.selector.compare_models(
            X_train_scaled, y_train, models, cv=5
        )
        
        # 7. è¶…å‚æ•°è°ƒä¼˜
        print("\n7. å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
        best_model_name = results.iloc[0]['Model']
        print(f"æœ€ä½³æ¨¡å‹: {best_model_name}")
        
        if best_model_name == 'Random Forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20],
                'min_samples_split': [2, 5, 10]
            }
            grid_search = self.selector.grid_search(
                RandomForestClassifier(), param_grid,
                X_train_scaled, y_train
            )
        
        # 8. æœ€ç»ˆè¯„ä¼°
        print("\n8. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        if hasattr(self.selector, 'best_model'):
            final_model = self.selector.best_model
            test_score = final_model.score(X_test_scaled, y_test)
            print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")
        
        print("\né¡¹ç›®å®Œæˆï¼")
        
        return results
```

## 7. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ â“

```python
class MLTroubleshooting:
    """æœºå™¨å­¦ä¹ å¸¸è§é—®é¢˜è§£å†³"""
    
    @staticmethod
    def diagnose_overfitting(train_score, val_score, threshold=0.1):
        """è¯Šæ–­è¿‡æ‹Ÿåˆ"""
        gap = train_score - val_score
        
        if gap > threshold:
            print("æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆ!")
            print(f"è®­ç»ƒåˆ†æ•°: {train_score:.4f}")
            print(f"éªŒè¯åˆ†æ•°: {val_score:.4f}")
            print(f"å·®è·: {gap:.4f}")
            print("\nè§£å†³æ–¹æ¡ˆ:")
            print("1. å¢åŠ è®­ç»ƒæ•°æ®")
            print("2. å‡å°‘æ¨¡å‹å¤æ‚åº¦")
            print("3. å¢åŠ æ­£åˆ™åŒ–")
            print("4. ä½¿ç”¨Dropoutï¼ˆæ·±åº¦å­¦ä¹ ï¼‰")
            print("5. æ—©åœï¼ˆEarly Stoppingï¼‰")
            print("6. æ•°æ®å¢å¼º")
            return True
        return False
    
    @staticmethod
    def diagnose_underfitting(train_score, val_score, target_score=0.8):
        """è¯Šæ–­æ¬ æ‹Ÿåˆ"""
        if train_score < target_score and val_score < target_score:
            print("æ£€æµ‹åˆ°æ¬ æ‹Ÿåˆ!")
            print(f"è®­ç»ƒåˆ†æ•°: {train_score:.4f}")
            print(f"éªŒè¯åˆ†æ•°: {val_score:.4f}")
            print("\nè§£å†³æ–¹æ¡ˆ:")
            print("1. å¢åŠ æ¨¡å‹å¤æ‚åº¦")
            print("2. å¢åŠ ç‰¹å¾")
            print("3. å‡å°‘æ­£åˆ™åŒ–")
            print("4. æ›´æ¢æ›´å¼ºå¤§çš„æ¨¡å‹")
            print("5. ç‰¹å¾å·¥ç¨‹")
            return True
        return False
    
    @staticmethod
    def handle_imbalanced_data():
        """å¤„ç†ä¸å¹³è¡¡æ•°æ®"""
        print("å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„ç­–ç•¥:")
        print("\n1. é‡é‡‡æ ·æ–¹æ³•:")
        print("   - è¿‡é‡‡æ ·å°‘æ•°ç±»ï¼ˆSMOTEï¼‰")
        print("   - æ¬ é‡‡æ ·å¤šæ•°ç±»")
        print("   - ç»„åˆé‡‡æ ·")
        
        print("\n2. ç®—æ³•å±‚é¢:")
        print("   - ä½¿ç”¨class_weightå‚æ•°")
        print("   - ä½¿ç”¨ä¸“é—¨çš„ç®—æ³•ï¼ˆå¦‚BalancedRandomForestï¼‰")
        
        print("\n3. è¯„ä¼°æŒ‡æ ‡:")
        print("   - ä¸è¦åªçœ‹å‡†ç¡®ç‡")
        print("   - ä½¿ç”¨F1ã€AUCã€ç²¾ç¡®ç‡ã€å¬å›ç‡")
        
        print("\n4. å…¶ä»–æ–¹æ³•:")
        print("   - é›†æˆæ–¹æ³•")
        print("   - å¼‚å¸¸æ£€æµ‹æ–¹æ³•")
        print("   - æˆæœ¬æ•æ„Ÿå­¦ä¹ ")
    
    @staticmethod
    def feature_selection_guide():
        """ç‰¹å¾é€‰æ‹©æŒ‡å—"""
        print("ç‰¹å¾é€‰æ‹©æ–¹æ³•:")
        print("\n1. è¿‡æ»¤æ³•ï¼ˆFilterï¼‰:")
        print("   - æ–¹å·®é˜ˆå€¼")
        print("   - ç›¸å…³ç³»æ•°")
        print("   - äº’ä¿¡æ¯")
        print("   - å¡æ–¹æ£€éªŒ")
        
        print("\n2. åŒ…è£…æ³•ï¼ˆWrapperï¼‰:")
        print("   - é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰")
        print("   - å‰å‘é€‰æ‹©")
        print("   - åå‘æ¶ˆé™¤")
        
        print("\n3. åµŒå…¥æ³•ï¼ˆEmbeddedï¼‰:")
        print("   - L1æ­£åˆ™åŒ–")
        print("   - æ ‘æ¨¡å‹ç‰¹å¾é‡è¦æ€§")
        print("   - éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§")
        
        print("\né€‰æ‹©å»ºè®®:")
        print("- ç‰¹å¾å¾ˆå¤šï¼ˆ>100ï¼‰ï¼šå…ˆç”¨è¿‡æ»¤æ³•")
        print("- ç‰¹å¾ä¸­ç­‰ï¼ˆ10-100ï¼‰ï¼šåŒ…è£…æ³•æˆ–åµŒå…¥æ³•")
        print("- ç‰¹å¾å¾ˆå°‘ï¼ˆ<10ï¼‰ï¼šè°¨æ…é€‰æ‹©ï¼Œå¯èƒ½éƒ½éœ€è¦")
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“‹

```python
def ml_best_practices():
    """æœºå™¨å­¦ä¹ æœ€ä½³å®è·µ"""
    
    practices = {
        "æ•°æ®å‡†å¤‡": [
            "å§‹ç»ˆæ£€æŸ¥æ•°æ®è´¨é‡",
            "å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼",
            "æ­£ç¡®ç¼–ç ç±»åˆ«å˜é‡",
            "ç‰¹å¾ç¼©æ”¾å¾ˆé‡è¦",
            "ä¿æŒè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ç‹¬ç«‹"
        ],
        
        "æ¨¡å‹å¼€å‘": [
            "ä»ç®€å•æ¨¡å‹å¼€å§‹",
            "å»ºç«‹åŸºå‡†æ¨¡å‹",
            "ä½¿ç”¨äº¤å‰éªŒè¯",
            "é¿å…æ•°æ®æ³„éœ²",
            "è®°å½•æ‰€æœ‰å®éªŒ"
        ],
        
        "ç‰¹å¾å·¥ç¨‹": [
            "ç†è§£ä¸šåŠ¡èƒŒæ™¯",
            "åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾",
            "è€ƒè™‘ç‰¹å¾äº¤äº’",
            "å®šæœŸè¯„ä¼°ç‰¹å¾é‡è¦æ€§",
            "é¿å…è¿‡å¤šç‰¹å¾"
        ],
        
        "æ¨¡å‹è¯„ä¼°": [
            "é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡",
            "ä¸è¦åªçœ‹å•ä¸€æŒ‡æ ‡",
            "åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°",
            "è€ƒè™‘ä¸šåŠ¡æŒ‡æ ‡",
            "è¿›è¡Œé”™è¯¯åˆ†æ"
        ],
        
        "éƒ¨ç½²è€ƒè™‘": [
            "æ¨¡å‹å¤§å°å’Œæ¨ç†é€Ÿåº¦",
            "æ¨¡å‹å¯è§£é‡Šæ€§",
            "ç›‘æ§æ¨¡å‹æ€§èƒ½",
            "å‡†å¤‡æ¨¡å‹æ›´æ–°ç­–ç•¥",
            "è€ƒè™‘è¾¹ç¼˜æƒ…å†µ"
        ]
    }
    
    return practices

# å­¦ä¹ è·¯å¾„
learning_path = """
æœºå™¨å­¦ä¹ å­¦ä¹ è·¯å¾„ï¼š

1. åŸºç¡€é˜¶æ®µï¼ˆ1-2ä¸ªæœˆï¼‰
   - Pythonç¼–ç¨‹åŸºç¡€
   - NumPyã€Pandasã€Matplotlib
   - ç»Ÿè®¡å­¦åŸºç¡€
   - çº¿æ€§ä»£æ•°åŸºç¡€

2. æ ¸å¿ƒç®—æ³•ï¼ˆ2-3ä¸ªæœˆï¼‰
   - ç›‘ç£å­¦ä¹ ç®—æ³•
   - æ— ç›‘ç£å­¦ä¹ ç®—æ³•
   - æ¨¡å‹è¯„ä¼°æ–¹æ³•
   - ç‰¹å¾å·¥ç¨‹

3. è¿›é˜¶æŠ€æœ¯ï¼ˆ2-3ä¸ªæœˆï¼‰
   - é›†æˆå­¦ä¹ 
   - æ·±åº¦å­¦ä¹ åŸºç¡€
   - è‡ªç„¶è¯­è¨€å¤„ç†
   - è®¡ç®—æœºè§†è§‰

4. å®æˆ˜é¡¹ç›®ï¼ˆæŒç»­ï¼‰
   - Kaggleç«èµ›
   - å¼€æºé¡¹ç›®è´¡çŒ®
   - ä¸ªäººé¡¹ç›®
   - è®ºæ–‡å¤ç°

5. ä¸“ä¸šåŒ–ï¼ˆæ ¹æ®å…´è¶£ï¼‰
   - æ·±åº¦å­¦ä¹ 
   - å¼ºåŒ–å­¦ä¹ 
   - æ¨èç³»ç»Ÿ
   - æ—¶é—´åºåˆ—
"""

print("æœºå™¨å­¦ä¹ åŸºç¡€æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [å›å½’ç®—æ³•](regression.md) - æ·±å…¥å­¦ä¹ å›å½’æŠ€æœ¯
- [åˆ†ç±»ç®—æ³•](classification.md) - æŒæ¡åˆ†ç±»æ–¹æ³•
- [ç‰¹å¾å·¥ç¨‹](feature_engineering.md) - æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®