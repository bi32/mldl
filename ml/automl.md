# è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹  (AutoML) ğŸ¤–

å…¨é¢æŒæ¡AutoMLçš„åŸç†ã€æ–¹æ³•å’Œå®è·µï¼Œå®ç°æœºå™¨å­¦ä¹ çš„è‡ªåŠ¨åŒ–ã€‚

## 1. AutoMLæ¦‚è¿° ğŸŒŸ

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_boston, load_iris, make_classification
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class AutoMLOverview:
    """AutoMLæ¦‚è¿°"""
    
    def __init__(self):
        self.automl_components = {
            "æ•°æ®é¢„å¤„ç†": ["ç‰¹å¾é€‰æ‹©", "æ•°æ®æ¸…æ´—", "ç‰¹å¾å·¥ç¨‹", "æ•°æ®å¢å¼º"],
            "æ¨¡å‹é€‰æ‹©": ["ç®—æ³•é€‰æ‹©", "è¶…å‚æ•°ä¼˜åŒ–", "æ¶æ„æœç´¢", "é›†æˆæ–¹æ³•"],
            "è¯„ä¼°ä¼˜åŒ–": ["äº¤å‰éªŒè¯", "æ—©åœç­–ç•¥", "æ¨¡å‹å‹ç¼©", "æ€§èƒ½è¯„ä¼°"],
            "éƒ¨ç½²ç›‘æ§": ["æ¨¡å‹éƒ¨ç½²", "æ€§èƒ½ç›‘æ§", "æ¨¡å‹æ›´æ–°", "A/Bæµ‹è¯•"]
        }
    
    def automl_motivation(self):
        """AutoMLçš„åŠ¨æœºå’Œä»·å€¼"""
        print("=== AutoMLçš„åŠ¨æœº ===")
        
        challenges = {
            "äººå·¥æˆæœ¬": {
                "é—®é¢˜": "éœ€è¦å¤§é‡ä¸“ä¸šçŸ¥è¯†å’Œç»éªŒ",
                "è§£å†³": "è‡ªåŠ¨åŒ–æµç¨‹ï¼Œé™ä½æŠ€æœ¯é—¨æ§›",
                "ä»·å€¼": "è®©éä¸“å®¶ä¹Ÿèƒ½ä½¿ç”¨ML"
            },
            "æ—¶é—´æˆæœ¬": {
                "é—®é¢˜": "æ¨¡å‹å¼€å‘å‘¨æœŸé•¿",
                "è§£å†³": "å¹¶è¡Œæœç´¢å’Œå¿«é€Ÿè¿­ä»£",
                "ä»·å€¼": "åŠ é€Ÿæ¨¡å‹éƒ¨ç½²ä¸Šçº¿"
            },
            "æœç´¢ç©ºé—´": {
                "é—®é¢˜": "è¶…å‚æ•°ç©ºé—´å·¨å¤§",
                "è§£å†³": "æ™ºèƒ½æœç´¢ç­–ç•¥",
                "ä»·å€¼": "æ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹é…ç½®"
            },
            "ä¸€è‡´æ€§": {
                "é—®é¢˜": "ä¸åŒäººå‘˜ç»“æœå·®å¼‚å¤§",
                "è§£å†³": "æ ‡å‡†åŒ–æµç¨‹å’Œè¯„ä¼°",
                "ä»·å€¼": "ä¿è¯ç»“æœçš„å¯é‡å¤æ€§"
            }
        }
        
        for challenge, details in challenges.items():
            print(f"\n{challenge}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        return challenges
    
    def automl_taxonomy(self):
        """AutoMLåˆ†ç±»ä½“ç³»"""
        print("=== AutoMLåˆ†ç±»ä½“ç³» ===")
        
        # æŒ‰è‡ªåŠ¨åŒ–ç¨‹åº¦åˆ†ç±»
        automation_levels = {
            "éƒ¨åˆ†è‡ªåŠ¨åŒ–": {
                "ç‰¹ç‚¹": "è‡ªåŠ¨åŒ–ç‰¹å®šç¯èŠ‚",
                "ç¤ºä¾‹": "è¶…å‚æ•°è°ƒä¼˜ã€ç‰¹å¾é€‰æ‹©",
                "é€‚ç”¨": "æœ‰ä¸€å®šMLç»éªŒçš„ç”¨æˆ·",
                "å·¥å…·": "Optuna, Hyperopt, scikit-optimize"
            },
            "å…¨è‡ªåŠ¨åŒ–": {
                "ç‰¹ç‚¹": "ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–",
                "ç¤ºä¾‹": "ä»æ•°æ®åˆ°æ¨¡å‹çš„å®Œæ•´æµç¨‹",
                "é€‚ç”¨": "MLæ–°æ‰‹æˆ–å¿«é€ŸåŸå‹",
                "å·¥å…·": "AutoML platforms"
            },
            "äº¤äº’å¼è‡ªåŠ¨åŒ–": {
                "ç‰¹ç‚¹": "äººæœºåä½œ",
                "ç¤ºä¾‹": "ç”¨æˆ·æä¾›çº¦æŸå’Œåå¥½",
                "é€‚ç”¨": "éœ€è¦é¢†åŸŸçŸ¥è¯†çš„åœºæ™¯",
                "å·¥å…·": "Interactive ML systems"
            }
        }
        
        for level, details in automation_levels.items():
            print(f"\n{level}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
        
        # æŒ‰åº”ç”¨é¢†åŸŸåˆ†ç±»
        domain_types = {
            "è¡¨æ ¼æ•°æ®AutoML": ["H2O.ai", "AutoGluon", "TPOT"],
            "è®¡ç®—æœºè§†è§‰AutoML": ["AutoKeras", "NAS", "EfficientNet"],
            "è‡ªç„¶è¯­è¨€å¤„ç†AutoML": ["AutoNLP", "Neural Architecture Search"],
            "æ—¶é—´åºåˆ—AutoML": ["Prophet", "Auto-ARIMA", "NeuralProphet"],
            "æ¨èç³»ç»ŸAutoML": ["AutoRec", "AutoCTR", "AutoFM"]
        }
        
        print("\n=== æŒ‰åº”ç”¨é¢†åŸŸåˆ†ç±» ===")
        for domain, tools in domain_types.items():
            print(f"{domain}: {', '.join(tools)}")
        
        return automation_levels, domain_types

class HyperparameterOptimization:
    """è¶…å‚æ•°ä¼˜åŒ–"""
    
    def __init__(self):
        self.optimization_methods = {}
    
    def grid_search_implementation(self):
        """ç½‘æ ¼æœç´¢å®ç°"""
        print("=== ç½‘æ ¼æœç´¢ (Grid Search) ===")
        
        print("åŸç†:")
        print("- ç©·ä¸¾æœç´¢é¢„å®šä¹‰çš„å‚æ•°ç»„åˆ")
        print("- ä¿è¯æ‰¾åˆ°æœ€ä¼˜è§£(åœ¨æœç´¢ç©ºé—´å†…)")
        print("- è®¡ç®—å¤æ‚åº¦: O(n^d), dä¸ºå‚æ•°ç»´åº¦")
        print()
        
        # å®ç°ç®€å•çš„ç½‘æ ¼æœç´¢
        from sklearn.model_selection import GridSearchCV
        from sklearn.datasets import make_classification
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        X, y = make_classification(n_samples=1000, n_features=10, 
                                 n_informative=5, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        # å®šä¹‰å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7, None],
            'min_samples_split': [2, 5, 10]
        }
        
        # ç½‘æ ¼æœç´¢
        rf = RandomForestClassifier(random_state=42)
        grid_search = GridSearchCV(
            rf, param_grid, cv=5, 
            scoring='accuracy', n_jobs=-1, verbose=1
        )
        
        print("æ‰§è¡Œç½‘æ ¼æœç´¢...")
        grid_search.fit(X_train, y_train)
        
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
        
        # å¯è§†åŒ–æœç´¢ç»“æœ
        self.visualize_grid_search_results(grid_search)
        
        return grid_search
    
    def visualize_grid_search_results(self, grid_search):
        """å¯è§†åŒ–ç½‘æ ¼æœç´¢ç»“æœ"""
        results_df = pd.DataFrame(grid_search.cv_results_)
        
        # é€‰æ‹©ä¸»è¦å‚æ•°è¿›è¡Œå¯è§†åŒ–
        pivot_table = results_df.pivot_table(
            values='mean_test_score',
            index='param_n_estimators',
            columns='param_max_depth',
            aggfunc='mean'
        )
        
        plt.figure(figsize=(10, 6))
        sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='viridis')
        plt.title('Grid Search Results: Accuracy by Parameters')
        plt.xlabel('max_depth')
        plt.ylabel('n_estimators')
        plt.tight_layout()
        plt.show()
    
    def random_search_implementation(self):
        """éšæœºæœç´¢å®ç°"""
        print("=== éšæœºæœç´¢ (Random Search) ===")
        
        print("åŸç†:")
        print("- ä»å‚æ•°åˆ†å¸ƒä¸­éšæœºé‡‡æ ·")
        print("- åœ¨å›ºå®šé¢„ç®—ä¸‹é€šå¸¸ä¼˜äºç½‘æ ¼æœç´¢")
        print("- é€‚åˆé«˜ç»´å‚æ•°ç©ºé—´")
        print()
        
        from sklearn.model_selection import RandomizedSearchCV
        from scipy.stats import randint, uniform
        
        # ç”Ÿæˆæ•°æ®
        X, y = make_classification(n_samples=1000, n_features=10, 
                                 n_informative=5, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        # å®šä¹‰å‚æ•°åˆ†å¸ƒ
        param_distributions = {
            'n_estimators': randint(50, 300),
            'max_depth': randint(3, 20),
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10)
        }
        
        # éšæœºæœç´¢
        rf = RandomForestClassifier(random_state=42)
        random_search = RandomizedSearchCV(
            rf, param_distributions, n_iter=50, cv=5,
            scoring='accuracy', random_state=42, n_jobs=-1
        )
        
        print("æ‰§è¡Œéšæœºæœç´¢...")
        random_search.fit(X_train, y_train)
        
        print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
        print(f"æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}")
        
        # æ¯”è¾ƒæœç´¢æ•ˆç‡
        self.compare_search_methods(X_train, y_train)
        
        return random_search
    
    def compare_search_methods(self, X_train, y_train):
        """æ¯”è¾ƒä¸åŒæœç´¢æ–¹æ³•çš„æ•ˆç‡"""
        print("\n=== æœç´¢æ–¹æ³•æ•ˆç‡æ¯”è¾ƒ ===")
        
        import time
        
        # ç½‘æ ¼æœç´¢ï¼ˆå°è§„æ¨¡ï¼‰
        param_grid_small = {
            'n_estimators': [50, 100],
            'max_depth': [3, 5, 7]
        }
        
        rf = RandomForestClassifier(random_state=42)
        
        # ç½‘æ ¼æœç´¢æ—¶é—´æµ‹è¯•
        start_time = time.time()
        grid_search = GridSearchCV(rf, param_grid_small, cv=3)
        grid_search.fit(X_train, y_train)
        grid_time = time.time() - start_time
        
        # éšæœºæœç´¢æ—¶é—´æµ‹è¯•
        from scipy.stats import randint
        param_dist = {
            'n_estimators': randint(50, 200),
            'max_depth': randint(3, 10)
        }
        
        start_time = time.time()
        random_search = RandomizedSearchCV(
            rf, param_dist, n_iter=6, cv=3, random_state=42)
        random_search.fit(X_train, y_train)
        random_time = time.time() - start_time
        
        print(f"ç½‘æ ¼æœç´¢æ—¶é—´: {grid_time:.2f}ç§’")
        print(f"éšæœºæœç´¢æ—¶é—´: {random_time:.2f}ç§’")
        print(f"ç½‘æ ¼æœç´¢æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
        print(f"éšæœºæœç´¢æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}")
    
    def bayesian_optimization(self):
        """è´å¶æ–¯ä¼˜åŒ–"""
        print("=== è´å¶æ–¯ä¼˜åŒ– ===")
        
        print("åŸç†:")
        print("- ä½¿ç”¨æ¦‚ç‡æ¨¡å‹(å¦‚é«˜æ–¯è¿‡ç¨‹)å»ºæ¨¡ç›®æ ‡å‡½æ•°")
        print("- é€šè¿‡è·å–å‡½æ•°(acquisition function)æŒ‡å¯¼æœç´¢")
        print("- åœ¨æ¢ç´¢(exploration)å’Œåˆ©ç”¨(exploitation)é—´å¹³è¡¡")
        print()
        
        # ç®€åŒ–çš„è´å¶æ–¯ä¼˜åŒ–ç¤ºä¾‹
        try:
            from skopt import gp_minimize
            from skopt.space import Real, Integer
            from skopt.utils import use_named_args
            
            # å®šä¹‰æœç´¢ç©ºé—´
            dimensions = [
                Integer(low=10, high=300, name='n_estimators'),
                Integer(low=1, high=20, name='max_depth'),
                Real(low=0.01, high=0.5, name='min_samples_split', prior='log-uniform'),
            ]
            
            # ç”Ÿæˆæ•°æ®
            X, y = make_classification(n_samples=1000, n_features=10, random_state=42)
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # å®šä¹‰ç›®æ ‡å‡½æ•°
            @use_named_args(dimensions)
            def objective(**params):
                rf = RandomForestClassifier(random_state=42, **params)
                score = cross_val_score(rf, X_train, y_train, cv=3, scoring='accuracy').mean()
                return -score  # minimizeè´Ÿæ•° = maximizeæ­£æ•°
            
            print("æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–...")
            result = gp_minimize(func=objective, dimensions=dimensions, 
                               n_calls=20, random_state=42, verbose=False)
            
            print(f"æœ€ä½³å‚æ•°: {dict(zip([d.name for d in dimensions], result.x))}")
            print(f"æœ€ä½³å¾—åˆ†: {-result.fun:.4f}")
            
            # å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
            self.plot_bayesian_optimization(result)
            
        except ImportError:
            print("éœ€è¦å®‰è£…scikit-optimize: pip install scikit-optimize")
            self.manual_bayesian_example()
        
    def manual_bayesian_example(self):
        """æ‰‹åŠ¨è´å¶æ–¯ä¼˜åŒ–ç¤ºä¾‹"""
        print("\næ‰‹åŠ¨è´å¶æ–¯ä¼˜åŒ–æ¦‚å¿µæ¼”ç¤º:")
        
        # æ¨¡æ‹Ÿç›®æ ‡å‡½æ•°
        def objective_function(x):
            return -(x - 0.5)**2 + 0.8 + 0.1 * np.sin(10*x)
        
        x = np.linspace(0, 1, 100)
        y = [objective_function(xi) for xi in x]
        
        plt.figure(figsize=(10, 6))
        plt.plot(x, y, 'b-', label='çœŸå®ç›®æ ‡å‡½æ•°')
        
        # æ¨¡æ‹Ÿå·²è¯„ä¼°çš„ç‚¹
        evaluated_x = [0.2, 0.7, 0.9]
        evaluated_y = [objective_function(xi) for xi in evaluated_x]
        plt.scatter(evaluated_x, evaluated_y, c='red', s=100, label='å·²è¯„ä¼°ç‚¹')
        
        plt.xlabel('å‚æ•°å€¼')
        plt.ylabel('ç›®æ ‡å‡½æ•°å€¼')
        plt.title('è´å¶æ–¯ä¼˜åŒ–æ¦‚å¿µå›¾')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def plot_bayesian_optimization(self, result):
        """ç»˜åˆ¶è´å¶æ–¯ä¼˜åŒ–æ”¶æ•›æ›²çº¿"""
        plt.figure(figsize=(10, 6))
        
        # æ”¶æ•›æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot([-y for y in result.func_vals], 'b-o')
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('æœ€ä½³å¾—åˆ†')
        plt.title('è´å¶æ–¯ä¼˜åŒ–æ”¶æ•›æ›²çº¿')
        plt.grid(True, alpha=0.3)
        
        # è¯„ä¼°å†å²
        plt.subplot(1, 2, 2)
        cumulative_best = np.maximum.accumulate([-y for y in result.func_vals])
        plt.plot(cumulative_best, 'g-', linewidth=2)
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('ç´¯ç§¯æœ€ä½³å¾—åˆ†')
        plt.title('ç´¯ç§¯æœ€ä½³æ€§èƒ½')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

class AutoMLPipelines:
    """AutoMLç®¡é“"""
    
    def __init__(self):
        self.pipeline_components = {}
    
    def automated_feature_engineering(self):
        """è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹"""
        print("=== è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹ ===")
        
        feature_engineering_techniques = {
            "ç‰¹å¾é€‰æ‹©": {
                "æ–¹æ³•": ["ç›¸å…³æ€§åˆ†æ", "äº’ä¿¡æ¯", "é€’å½’ç‰¹å¾æ¶ˆé™¤"],
                "ç›®æ ‡": "å»é™¤å†—ä½™å’Œæ— å…³ç‰¹å¾",
                "å®ç°": "sklearn.feature_selection"
            },
            "ç‰¹å¾è½¬æ¢": {
                "æ–¹æ³•": ["æ ‡å‡†åŒ–", "å½’ä¸€åŒ–", "å¤šé¡¹å¼ç‰¹å¾"],
                "ç›®æ ‡": "æ”¹å–„æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹æ€§èƒ½",
                "å®ç°": "sklearn.preprocessing"
            },
            "ç‰¹å¾æ„é€ ": {
                "æ–¹æ³•": ["äº¤äº’ç‰¹å¾", "æ—¶é—´ç‰¹å¾", "èšåˆç‰¹å¾"],
                "ç›®æ ‡": "åˆ›é€ æ–°çš„æœ‰ç”¨ç‰¹å¾",
                "å®ç°": "featuretools, tsfresh"
            }
        }
        
        for technique, details in feature_engineering_techniques.items():
            print(f"\n{technique}:")
            for key, value in details.items():
                if isinstance(value, list):
                    print(f"  {key}: {', '.join(value)}")
                else:
                    print(f"  {key}: {value}")
        
        # å®ç°è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
        self.implement_auto_feature_selection()
        
        return feature_engineering_techniques
    
    def implement_auto_feature_selection(self):
        """å®ç°è‡ªåŠ¨ç‰¹å¾é€‰æ‹©"""
        print("\n=== è‡ªåŠ¨ç‰¹å¾é€‰æ‹©ç¤ºä¾‹ ===")
        
        from sklearn.feature_selection import SelectKBest, f_classif, RFE
        from sklearn.ensemble import RandomForestClassifier
        
        # ç”Ÿæˆé«˜ç»´æ•°æ®
        X, y = make_classification(n_samples=1000, n_features=50, 
                                 n_informative=10, n_redundant=10,
                                 random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        print(f"åŸå§‹ç‰¹å¾æ•°: {X_train.shape[1]}")
        
        # æ–¹æ³•1: å•å˜é‡ç‰¹å¾é€‰æ‹©
        selector_univariate = SelectKBest(score_func=f_classif, k=20)
        X_train_uni = selector_univariate.fit_transform(X_train, y_train)
        X_test_uni = selector_univariate.transform(X_test)
        
        print(f"å•å˜é‡é€‰æ‹©åç‰¹å¾æ•°: {X_train_uni.shape[1]}")
        
        # æ–¹æ³•2: é€’å½’ç‰¹å¾æ¶ˆé™¤
        rf = RandomForestClassifier(n_estimators=50, random_state=42)
        selector_rfe = RFE(estimator=rf, n_features_to_select=20)
        X_train_rfe = selector_rfe.fit_transform(X_train, y_train)
        X_test_rfe = selector_rfe.transform(X_test)
        
        print(f"RFEé€‰æ‹©åç‰¹å¾æ•°: {X_train_rfe.shape[1]}")
        
        # æ¯”è¾ƒä¸åŒç‰¹å¾é€‰æ‹©æ–¹æ³•çš„æ•ˆæœ
        methods = {
            'Original': (X_train, X_test),
            'Univariate': (X_train_uni, X_test_uni),
            'RFE': (X_train_rfe, X_test_rfe)
        }
        
        results = {}
        for method, (X_tr, X_te) in methods.items():
            rf_eval = RandomForestClassifier(n_estimators=100, random_state=42)
            rf_eval.fit(X_tr, y_train)
            score = rf_eval.score(X_te, y_test)
            results[method] = score
        
        print("\nç‰¹å¾é€‰æ‹©æ•ˆæœæ¯”è¾ƒ:")
        for method, score in results.items():
            print(f"{method}: {score:.4f}")
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        self.visualize_feature_importance(rf, X_train.shape[1], selector_rfe)
        
        return results
    
    def visualize_feature_importance(self, model, n_features, selector_rfe):
        """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = model.feature_importances_
        selected_features = selector_rfe.support_
        
        plt.figure(figsize=(12, 6))
        
        # åŸå§‹ç‰¹å¾é‡è¦æ€§
        plt.subplot(1, 2, 1)
        plt.bar(range(n_features), feature_importance)
        plt.xlabel('ç‰¹å¾ç´¢å¼•')
        plt.ylabel('é‡è¦æ€§')
        plt.title('æ‰€æœ‰ç‰¹å¾çš„é‡è¦æ€§')
        
        # é€‰ä¸­ç‰¹å¾çš„é‡è¦æ€§
        plt.subplot(1, 2, 2)
        selected_importance = feature_importance[selected_features]
        selected_indices = np.where(selected_features)[0]
        plt.bar(range(len(selected_importance)), selected_importance)
        plt.xlabel('é€‰ä¸­ç‰¹å¾ç´¢å¼•')
        plt.ylabel('é‡è¦æ€§')
        plt.title('RFEé€‰ä¸­ç‰¹å¾çš„é‡è¦æ€§')
        
        plt.tight_layout()
        plt.show()
    
    def automated_model_selection(self):
        """è‡ªåŠ¨æ¨¡å‹é€‰æ‹©"""
        print("=== è‡ªåŠ¨æ¨¡å‹é€‰æ‹© ===")
        
        # å®šä¹‰å€™é€‰æ¨¡å‹
        models = {
            'RandomForest': RandomForestClassifier(random_state=42),
            'GradientBoosting': GradientBoostingClassifier(random_state=42),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
            'SVM': SVC(random_state=42, probability=True)
        }
        
        # ç”Ÿæˆæ•°æ®
        X, y = make_classification(n_samples=1000, n_features=20, 
                                 n_informative=10, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        # æ•°æ®é¢„å¤„ç†
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        results = {}
        cv_results = {}
        
        print("è¯„ä¼°å„ä¸ªæ¨¡å‹:")
        for name, model in models.items():
            # é€‰æ‹©æ˜¯å¦éœ€è¦ç¼©æ”¾çš„æ•°æ®
            if name in ['LogisticRegression', 'SVM']:
                X_tr, X_te = X_train_scaled, X_test_scaled
            else:
                X_tr, X_te = X_train, X_test
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_tr, y_train, cv=5)
            cv_results[name] = cv_scores
            
            # è®­ç»ƒå’Œæµ‹è¯•
            model.fit(X_tr, y_train)
            test_score = model.score(X_te, y_test)
            results[name] = test_score
            
            print(f"{name}: CV={cv_scores.mean():.4f}(Â±{cv_scores.std():.4f}), "
                  f"Test={test_score:.4f}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model = max(results, key=results.get)
        print(f"\næœ€ä½³æ¨¡å‹: {best_model} (Test Score: {results[best_model]:.4f})")
        
        # å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ
        self.visualize_model_comparison(cv_results, results)
        
        return results, best_model
    
    def visualize_model_comparison(self, cv_results, test_results):
        """å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ"""
        plt.figure(figsize=(15, 5))
        
        # äº¤å‰éªŒè¯ç»“æœç®±çº¿å›¾
        plt.subplot(1, 3, 1)
        cv_data = [scores for scores in cv_results.values()]
        plt.boxplot(cv_data, labels=cv_results.keys())
        plt.ylabel('äº¤å‰éªŒè¯å¾—åˆ†')
        plt.title('æ¨¡å‹äº¤å‰éªŒè¯æ¯”è¾ƒ')
        plt.xticks(rotation=45)
        
        # æµ‹è¯•å¾—åˆ†æŸ±çŠ¶å›¾
        plt.subplot(1, 3, 2)
        models = list(test_results.keys())
        scores = list(test_results.values())
        plt.bar(models, scores)
        plt.ylabel('æµ‹è¯•å¾—åˆ†')
        plt.title('æ¨¡å‹æµ‹è¯•æ€§èƒ½æ¯”è¾ƒ')
        plt.xticks(rotation=45)
        
        # CVå‡å€¼vsæµ‹è¯•å¾—åˆ†æ•£ç‚¹å›¾
        plt.subplot(1, 3, 3)
        cv_means = [np.mean(scores) for scores in cv_results.values()]
        test_scores = list(test_results.values())
        plt.scatter(cv_means, test_scores)
        
        for i, model in enumerate(models):
            plt.annotate(model, (cv_means[i], test_scores[i]))
        
        plt.xlabel('äº¤å‰éªŒè¯å‡å€¼')
        plt.ylabel('æµ‹è¯•å¾—åˆ†')
        plt.title('CV vs æµ‹è¯•æ€§èƒ½')
        plt.plot([min(cv_means), max(cv_means)], 
                [min(cv_means), max(cv_means)], 'r--', alpha=0.5)
        
        plt.tight_layout()
        plt.show()

class AutoMLFrameworks:
    """AutoMLæ¡†æ¶"""
    
    def __init__(self):
        self.frameworks = {}
    
    def popular_automl_tools(self):
        """æµè¡Œçš„AutoMLå·¥å…·"""
        print("=== æµè¡Œçš„AutoMLå·¥å…· ===")
        
        tools = {
            "å•†ä¸šå·¥å…·": {
                "H2O.ai": {
                    "ç‰¹ç‚¹": "å¼€æº+ä¼ä¸šç‰ˆï¼Œæ”¯æŒå¤šç§ç®—æ³•",
                    "ä¼˜åŠ¿": "æ˜“ç”¨æ€§å¼ºï¼Œå¯è§£é‡Šæ€§å¥½",
                    "é€‚ç”¨": "ä¼ä¸šçº§åº”ç”¨",
                    "è¯­è¨€": "Python, R, Java, Scala"
                },
                "DataRobot": {
                    "ç‰¹ç‚¹": "å…¨è‡ªåŠ¨åŒ–å¹³å°",
                    "ä¼˜åŠ¿": "æ— éœ€ç¼–ç¨‹ï¼ŒMLOpsé›†æˆ",
                    "é€‚ç”¨": "å•†ä¸šç”¨æˆ·",
                    "è¯­è¨€": "Webç•Œé¢"
                },
                "Google AutoML": {
                    "ç‰¹ç‚¹": "äº‘åŸç”Ÿï¼Œæ”¯æŒå¤šæ¨¡æ€",
                    "ä¼˜åŠ¿": "GoogleåŸºç¡€è®¾æ–½",
                    "é€‚ç”¨": "äº‘ç”¨æˆ·",
                    "è¯­è¨€": "APIè°ƒç”¨"
                }
            },
            
            "å¼€æºå·¥å…·": {
                "Auto-sklearn": {
                    "ç‰¹ç‚¹": "åŸºäºscikit-learn",
                    "ä¼˜åŠ¿": "å…ƒå­¦ä¹ ï¼Œé›†æˆæ–¹æ³•",
                    "é€‚ç”¨": "Pythonç”¨æˆ·",
                    "è¯­è¨€": "Python"
                },
                "TPOT": {
                    "ç‰¹ç‚¹": "é—ä¼ ç¼–ç¨‹ä¼˜åŒ–",
                    "ä¼˜åŠ¿": "æœç´¢ç®¡é“ç©ºé—´",
                    "é€‚ç”¨": "ç ”ç©¶å’Œå®éªŒ",
                    "è¯­è¨€": "Python"
                },
                "AutoGluon": {
                    "ç‰¹ç‚¹": "Amazonå¼€å‘ï¼Œå¤šæ¨¡æ€",
                    "ä¼˜åŠ¿": "æ˜“ç”¨ï¼Œæ€§èƒ½å¥½",
                    "é€‚ç”¨": "å¿«é€ŸåŸå‹",
                    "è¯­è¨€": "Python"
                }
            }
        }
        
        for category, tools_dict in tools.items():
            print(f"\n{category}:")
            for tool, details in tools_dict.items():
                print(f"  {tool}:")
                for key, value in details.items():
                    print(f"    {key}: {value}")
        
        return tools
    
    def implement_simple_automl(self):
        """å®ç°ç®€å•çš„AutoMLç³»ç»Ÿ"""
        print("=== ç®€å•AutoMLç³»ç»Ÿå®ç° ===")
        
        class SimpleAutoML:
            def __init__(self, time_budget=60):
                self.time_budget = time_budget
                self.best_model = None
                self.best_score = -float('inf')
                self.best_params = None
                self.preprocessing_steps = []
                
                # å®šä¹‰å€™é€‰æ¨¡å‹å’Œå‚æ•°ç©ºé—´
                self.models_space = {
                    'rf': {
                        'model': RandomForestClassifier,
                        'params': {
                            'n_estimators': [50, 100, 200],
                            'max_depth': [None, 10, 20],
                            'min_samples_split': [2, 5, 10]
                        }
                    },
                    'gb': {
                        'model': GradientBoostingClassifier,
                        'params': {
                            'n_estimators': [50, 100],
                            'learning_rate': [0.01, 0.1, 0.2],
                            'max_depth': [3, 6, 9]
                        }
                    },
                    'lr': {
                        'model': LogisticRegression,
                        'params': {
                            'C': [0.1, 1, 10],
                            'solver': ['liblinear', 'lbfgs']
                        }
                    }
                }
            
            def fit(self, X, y):
                import time
                import itertools
                from sklearn.model_selection import cross_val_score
                
                start_time = time.time()
                
                print(f"å¼€å§‹AutoMLè®­ç»ƒï¼Œæ—¶é—´é¢„ç®—: {self.time_budget}ç§’")
                print(f"æ•°æ®ç»´åº¦: {X.shape}")
                
                # æ•°æ®é¢„å¤„ç†
                X_processed = self._preprocess_data(X, y)
                
                # æ¨¡å‹æœç´¢
                for model_name, model_config in self.models_space.items():
                    if time.time() - start_time > self.time_budget:
                        print(f"æ—¶é—´é¢„ç®—ç”¨å®Œï¼Œåœæ­¢æœç´¢")
                        break
                    
                    model_class = model_config['model']
                    param_space = model_config['params']
                    
                    # ç”Ÿæˆå‚æ•°ç»„åˆ
                    param_names = list(param_space.keys())
                    param_values = list(param_space.values())
                    
                    for param_combination in itertools.product(*param_values):
                        if time.time() - start_time > self.time_budget:
                            break
                        
                        params = dict(zip(param_names, param_combination))
                        
                        try:
                            # åˆ›å»ºå’Œè¯„ä¼°æ¨¡å‹
                            if model_name == 'lr':
                                model = model_class(random_state=42, max_iter=1000, **params)
                            else:
                                model = model_class(random_state=42, **params)
                            
                            # äº¤å‰éªŒè¯
                            scores = cross_val_score(model, X_processed, y, cv=3, 
                                                   scoring='accuracy')
                            avg_score = scores.mean()
                            
                            if avg_score > self.best_score:
                                self.best_score = avg_score
                                self.best_model = model
                                self.best_params = params
                                
                                print(f"æ–°çš„æœ€ä½³æ¨¡å‹: {model_name} "
                                      f"(score: {avg_score:.4f}, params: {params})")
                        
                        except Exception as e:
                            continue
                
                # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
                if self.best_model is not None:
                    self.best_model.fit(X_processed, y)
                    
                elapsed_time = time.time() - start_time
                print(f"AutoMLå®Œæˆï¼Œç”¨æ—¶: {elapsed_time:.2f}ç§’")
                print(f"æœ€ä½³æ¨¡å‹å¾—åˆ†: {self.best_score:.4f}")
                
                return self
            
            def _preprocess_data(self, X, y):
                """æ•°æ®é¢„å¤„ç†"""
                from sklearn.preprocessing import StandardScaler
                from sklearn.impute import SimpleImputer
                
                X_processed = X.copy()
                
                # å¤„ç†ç¼ºå¤±å€¼
                if np.isnan(X_processed).any():
                    imputer = SimpleImputer(strategy='mean')
                    X_processed = imputer.fit_transform(X_processed)
                    self.preprocessing_steps.append(('imputer', imputer))
                
                # æ ‡å‡†åŒ–
                scaler = StandardScaler()
                X_processed = scaler.fit_transform(X_processed)
                self.preprocessing_steps.append(('scaler', scaler))
                
                return X_processed
            
            def predict(self, X):
                """é¢„æµ‹"""
                if self.best_model is None:
                    raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
                
                X_processed = X.copy()
                
                # åº”ç”¨é¢„å¤„ç†æ­¥éª¤
                for step_name, step_obj in self.preprocessing_steps:
                    X_processed = step_obj.transform(X_processed)
                
                return self.best_model.predict(X_processed)
            
            def predict_proba(self, X):
                """é¢„æµ‹æ¦‚ç‡"""
                if self.best_model is None:
                    raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
                
                X_processed = X.copy()
                
                for step_name, step_obj in self.preprocessing_steps:
                    X_processed = step_obj.transform(X_processed)
                
                return self.best_model.predict_proba(X_processed)
        
        # æµ‹è¯•SimpleAutoML
        print("\næµ‹è¯•SimpleAutoML:")
        X, y = make_classification(n_samples=1000, n_features=10, 
                                 n_informative=5, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        # è®­ç»ƒAutoML
        automl = SimpleAutoML(time_budget=30)
        automl.fit(X_train, y_train)
        
        # è¯„ä¼°
        y_pred = automl.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
        
        return SimpleAutoML
    
    def neural_architecture_search(self):
        """ç¥ç»æ¶æ„æœç´¢ (NAS)"""
        print("=== ç¥ç»æ¶æ„æœç´¢ (NAS) ===")
        
        print("NASæ¦‚å¿µ:")
        print("- è‡ªåŠ¨è®¾è®¡ç¥ç»ç½‘ç»œæ¶æ„")
        print("- æœç´¢ç©ºé—´åŒ…æ‹¬å±‚ç±»å‹ã€è¿æ¥æ–¹å¼ã€è¶…å‚æ•°")
        print("- ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜çš„ç½‘ç»œç»“æ„")
        print()
        
        nas_methods = {
            "å¼ºåŒ–å­¦ä¹ NAS": {
                "ä»£è¡¨": "NASNet, ENAS",
                "åŸç†": "ç”¨RL agentç”Ÿæˆæ¶æ„",
                "ä¼˜ç‚¹": "å¯ä»¥å‘ç°æ–°é¢–æ¶æ„",
                "ç¼ºç‚¹": "è®¡ç®—æˆæœ¬æé«˜"
            },
            "å¯å¾®åˆ†NAS": {
                "ä»£è¡¨": "DARTS, PC-DARTS",
                "åŸç†": "å°†æ¶æ„æœç´¢è½¬ä¸ºè¿ç»­ä¼˜åŒ–",
                "ä¼˜ç‚¹": "æ•ˆç‡é«˜ï¼Œæ¢¯åº¦ä¼˜åŒ–",
                "ç¼ºç‚¹": "æœç´¢ç©ºé—´å—é™"
            },
            "è¿›åŒ–ç®—æ³•NAS": {
                "ä»£è¡¨": "AmoebaNet, AmobaNet",
                "åŸç†": "è¿›åŒ–ç®—æ³•æœç´¢æ¶æ„",
                "ä¼˜ç‚¹": "æ— æ¢¯åº¦è¦æ±‚",
                "ç¼ºç‚¹": "éœ€è¦å¤§é‡è®¡ç®—èµ„æº"
            },
            "æƒé‡å…±äº«NAS": {
                "ä»£è¡¨": "Once-for-All, BigNAS",
                "åŸç†": "é¢„è®­ç»ƒè¶…ç½‘ç»œï¼Œå­ç½‘ç»§æ‰¿æƒé‡",
                "ä¼˜ç‚¹": "å¤§å¹…é™ä½æœç´¢æˆæœ¬",
                "ç¼ºç‚¹": "æƒé‡ç»§æ‰¿å¯èƒ½ä¸optimal"
            }
        }
        
        for method, details in nas_methods.items():
            print(f"{method}:")
            for key, value in details.items():
                print(f"  {key}: {value}")
            print()
        
        # ç®€åŒ–çš„æ¶æ„æœç´¢æ¼”ç¤º
        self.demo_simple_nas()
        
        return nas_methods
    
    def demo_simple_nas(self):
        """æ¼”ç¤ºç®€å•çš„æ¶æ„æœç´¢"""
        print("=== ç®€åŒ–æ¶æ„æœç´¢æ¼”ç¤º ===")
        
        # å®šä¹‰æœç´¢ç©ºé—´ï¼ˆä¸åŒçš„ç½‘ç»œå®½åº¦å’Œæ·±åº¦ï¼‰
        architecture_space = {
            'n_layers': [1, 2, 3],
            'layer_sizes': [[32], [64], [128], [32, 16], [64, 32], [128, 64, 32]]
        }
        
        from sklearn.neural_network import MLPClassifier
        
        # ç”Ÿæˆæ•°æ®
        X, y = make_classification(n_samples=1000, n_features=20, 
                                 n_informative=10, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        best_architecture = None
        best_score = -float('inf')
        results = []
        
        print("æœç´¢æœ€ä½³ç½‘ç»œæ¶æ„:")
        
        for layer_sizes in architecture_space['layer_sizes']:
            try:
                # åˆ›å»ºMLP
                mlp = MLPClassifier(
                    hidden_layer_sizes=tuple(layer_sizes),
                    max_iter=500,
                    random_state=42
                )
                
                # è®­ç»ƒå’Œè¯„ä¼°
                cv_scores = cross_val_score(mlp, X_train_scaled, y_train, 
                                          cv=3, scoring='accuracy')
                avg_score = cv_scores.mean()
                
                results.append({
                    'architecture': layer_sizes,
                    'score': avg_score,
                    'std': cv_scores.std()
                })
                
                if avg_score > best_score:
                    best_score = avg_score
                    best_architecture = layer_sizes
                
                print(f"æ¶æ„ {layer_sizes}: {avg_score:.4f} (Â±{cv_scores.std():.4f})")
                
            except Exception as e:
                print(f"æ¶æ„ {layer_sizes} å¤±è´¥: {e}")
                continue
        
        print(f"\næœ€ä½³æ¶æ„: {best_architecture}")
        print(f"æœ€ä½³å¾—åˆ†: {best_score:.4f}")
        
        # å¯è§†åŒ–æ¶æ„æœç´¢ç»“æœ
        self.visualize_architecture_search(results)
        
        return best_architecture, results
    
    def visualize_architecture_search(self, results):
        """å¯è§†åŒ–æ¶æ„æœç´¢ç»“æœ"""
        if not results:
            return
        
        # æå–æ•°æ®
        architectures = [str(r['architecture']) for r in results]
        scores = [r['score'] for r in results]
        stds = [r['std'] for r in results]
        
        # ç»˜åˆ¶ç»“æœ
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        x = range(len(architectures))
        plt.errorbar(x, scores, yerr=stds, fmt='o-', capsize=5)
        plt.xlabel('æ¶æ„é…ç½®')
        plt.ylabel('äº¤å‰éªŒè¯å¾—åˆ†')
        plt.title('ä¸åŒæ¶æ„çš„æ€§èƒ½æ¯”è¾ƒ')
        plt.xticks(x, architectures, rotation=45)
        plt.grid(True, alpha=0.3)
        
        # æ¶æ„å¤æ‚åº¦vsæ€§èƒ½
        plt.subplot(1, 2, 2)
        complexities = [sum(r['architecture']) for r in results]  # æ€»ç¥ç»å…ƒæ•°ä½œä¸ºå¤æ‚åº¦
        plt.scatter(complexities, scores, s=100, alpha=0.7)
        
        for i, arch in enumerate(architectures):
            plt.annotate(arch, (complexities[i], scores[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.xlabel('æ¶æ„å¤æ‚åº¦ (æ€»ç¥ç»å…ƒæ•°)')
        plt.ylabel('æ€§èƒ½å¾—åˆ†')
        plt.title('æ¶æ„å¤æ‚åº¦ vs æ€§èƒ½')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def comprehensive_automl_summary():
    """AutoMLç»¼åˆæ€»ç»“"""
    print("=== AutoMLç»¼åˆæ€»ç»“ ===")
    
    summary = {
        "æ ¸å¿ƒç»„ä»¶": {
            "æ•°æ®é¢„å¤„ç†": "è‡ªåŠ¨æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®å¢å¼º",
            "æ¨¡å‹é€‰æ‹©": "ç®—æ³•é€‰æ‹©ã€æ¶æ„è®¾è®¡ã€è¶…å‚æ•°ä¼˜åŒ–", 
            "æ¨¡å‹è¯„ä¼°": "äº¤å‰éªŒè¯ã€æ€§èƒ½è¯„ä¼°ã€æ¨¡å‹è§£é‡Š",
            "éƒ¨ç½²ä¼˜åŒ–": "æ¨¡å‹å‹ç¼©ã€æ¨ç†ä¼˜åŒ–ã€ç›‘æ§æ›´æ–°"
        },
        
        "å…³é”®æŠ€æœ¯": {
            "æœç´¢ç­–ç•¥": "ç½‘æ ¼æœç´¢ã€éšæœºæœç´¢ã€è´å¶æ–¯ä¼˜åŒ–ã€è¿›åŒ–ç®—æ³•",
            "å…ƒå­¦ä¹ ": "å­¦ä¹ å­¦ä¹ ã€è¿ç§»å­¦ä¹ ã€few-shotå­¦ä¹ ",
            "å¤šç›®æ ‡ä¼˜åŒ–": "å‡†ç¡®ç‡-æ•ˆç‡æƒè¡¡ã€å¸•ç´¯æ‰˜å‰æ²¿",
            "æ—©åœç­–ç•¥": "èµ„æºé¢„ç®—ã€æ”¶æ•›æ£€æµ‹ã€æ€§èƒ½é˜ˆå€¼"
        },
        
        "åº”ç”¨åœºæ™¯": {
            "ä¼ä¸šåº”ç”¨": "é™ä½AIé—¨æ§›ã€åŠ é€Ÿéƒ¨ç½²ã€æ ‡å‡†åŒ–æµç¨‹",
            "ç§‘ç ”æ¢ç´¢": "å¿«é€ŸåŸå‹ã€åŸºå‡†æ¯”è¾ƒã€æ–°æ–¹æ³•éªŒè¯",
            "æ•™è‚²åŸ¹è®­": "å­¦ä¹ å·¥å…·ã€æ¦‚å¿µç†è§£ã€æœ€ä½³å®è·µ",
            "ä¸ªäººé¡¹ç›®": "å¿«é€Ÿå»ºæ¨¡ã€å‚æ•°è°ƒä¼˜ã€æ€§èƒ½æå‡"
        },
        
        "å‘å±•è¶‹åŠ¿": {
            "å¤§æ¨¡å‹AutoML": "é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©ã€æç¤ºå·¥ç¨‹è‡ªåŠ¨åŒ–",
            "å¤šæ¨¡æ€AutoML": "è§†è§‰-è¯­è¨€-è¯­éŸ³è”åˆä¼˜åŒ–",
            "è”é‚¦AutoML": "åˆ†å¸ƒå¼æ•°æ®ä¸Šçš„è‡ªåŠ¨å­¦ä¹ ",
            "å¯è§£é‡ŠAutoML": "è‡ªåŠ¨ç”Ÿæˆæ¨¡å‹è§£é‡Šã€å†³ç­–é€æ˜åŒ–",
            "ç»¿è‰²AutoML": "èƒ½è€—ä¼˜åŒ–ã€ç¢³è¶³è¿¹è€ƒè™‘"
        },
        
        "æŒ‘æˆ˜ä¸é™åˆ¶": {
            "è®¡ç®—æˆæœ¬": "æœç´¢ç©ºé—´å¤§ã€è¯„ä¼°è€—æ—¶",
            "æ•°æ®è´¨é‡": "åƒåœ¾è¿›åƒåœ¾å‡ºã€æ•°æ®åå·®",
            "é¢†åŸŸçŸ¥è¯†": "é€šç”¨æ–¹æ³•vsä¸“ä¸šçŸ¥è¯†",
            "å¯è§£é‡Šæ€§": "é»‘ç›’ä¼˜åŒ–ã€ç»“æœå¯ä¿¡åº¦"
        }
    }
    
    for category, items in summary.items():
        print(f"\n{category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")
    
    return summary

print("è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹  (AutoML) æŒ‡å—åŠ è½½å®Œæˆï¼")
```

## æœ€ä½³å®è·µæŒ‡å— ğŸ“‹

```python
def automl_best_practices():
    """AutoMLæœ€ä½³å®è·µ"""
    
    practices = {
        "æ•°æ®å‡†å¤‡": [
            "ç¡®ä¿æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§",
            "ç†è§£ä¸šåŠ¡é—®é¢˜å’Œè¯„ä¼°æŒ‡æ ‡", 
            "é€‚å½“çš„æ•°æ®åˆ†å‰²å’ŒéªŒè¯ç­–ç•¥",
            "è€ƒè™‘æ•°æ®ä¸å¹³è¡¡é—®é¢˜"
        ],
        
        "æœç´¢ç­–ç•¥": [
            "åˆç†è®¾ç½®æ—¶é—´å’Œè®¡ç®—é¢„ç®—",
            "é€‰æ‹©é€‚åˆçš„æœç´¢ç®—æ³•",
            "å®šä¹‰åˆé€‚çš„æœç´¢ç©ºé—´",
            "ä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡"
        ],
        
        "æ¨¡å‹é€‰æ‹©": [
            "ä¸è¦å¿½è§†ç®€å•æ¨¡å‹",
            "è€ƒè™‘æ¨¡å‹çš„å¯è§£é‡Šæ€§",
            "è¯„ä¼°æ¨¡å‹çš„ç¨³å®šæ€§",
            "æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›"
        ],
        
        "éƒ¨ç½²è€ƒè™‘": [
            "è¯„ä¼°æ¨ç†å»¶è¿Ÿå’Œèµ„æºéœ€æ±‚",
            "è€ƒè™‘æ¨¡å‹æ›´æ–°å’Œç»´æŠ¤",
            "è®¾ç½®ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶",
            "å‡†å¤‡æ¨¡å‹å›æ»šç­–ç•¥"
        ]
    }
    
    return practices

# å¸¸è§é™·é˜±
common_pitfalls = """
AutoMLå¸¸è§é™·é˜±ï¼š

1. è¿‡åº¦ä¾èµ–è‡ªåŠ¨åŒ–
   - å¿½è§†é¢†åŸŸçŸ¥è¯†
   - ä¸ç†è§£æ¨¡å‹åŸç†
   - ç¼ºä¹ç»“æœéªŒè¯

2. æ•°æ®æ³„éœ²é—®é¢˜
   - æ—¶é—´åºåˆ—æ•°æ®çš„æœªæ¥æ³„éœ²
   - æµ‹è¯•é›†ä¿¡æ¯æ³„éœ²åˆ°è®­ç»ƒä¸­
   - ç‰¹å¾å·¥ç¨‹ä¸­ä½¿ç”¨å…¨å±€ç»Ÿè®¡

3. è¯„ä¼°åå·®
   - è¿‡æ‹ŸåˆéªŒè¯é›†
   - è¯„ä¼°æŒ‡æ ‡ä¸åŒ¹é…ä¸šåŠ¡ç›®æ ‡
   - å¿½è§†æ ·æœ¬ä¸å¹³è¡¡

4. èµ„æºæµªè´¹
   - æœç´¢ç©ºé—´è®¾ç½®ä¸å½“
   - æ²¡æœ‰ä½¿ç”¨æ—©åœç­–ç•¥
   - é‡å¤è®¡ç®—ç›¸åŒé…ç½®

5. ç”Ÿäº§éƒ¨ç½²é—®é¢˜
   - è®­ç»ƒç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒä¸ä¸€è‡´
   - æ¨¡å‹æ€§èƒ½åœ¨ç”Ÿäº§ä¸­ä¸‹é™
   - ç¼ºä¹æ¨¡å‹ç›‘æ§æœºåˆ¶
"""

print("AutoMLæœ€ä½³å®è·µå’Œå¸¸è§é™·é˜±åŠ è½½å®Œæˆï¼")
```

## å‚è€ƒæ–‡çŒ® ğŸ“š

- Hutter et al. (2019): "Automated Machine Learning: Methods, Systems, Challenges"
- Feurer & Hutter (2019): "Hyperparameter Optimization"
- Elsken et al. (2019): "Neural Architecture Search: A Survey"
- Zoph & Le (2017): "Neural Architecture Search with Reinforcement Learning"
- Real et al. (2019): "Regularized Evolution for Image Classifier Architecture Search"

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [è¶…å‚æ•°è°ƒä¼˜](hyperparameter_tuning.md) - æ·±å…¥è¶…å‚æ•°ä¼˜åŒ–
- [æ¨¡å‹éƒ¨ç½²](../deployment/pytorch_deployment.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [MLOpså®è·µ](mlops_practices.md) - æœºå™¨å­¦ä¹ å·¥ç¨‹åŒ–