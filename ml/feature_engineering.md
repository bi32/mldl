# ç‰¹å¾å·¥ç¨‹å®æˆ˜ï¼šæ•°æ®çš„è‰ºæœ¯ ğŸ¨

ç‰¹å¾å·¥ç¨‹æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„ç¯èŠ‚ä¹‹ä¸€ã€‚å¥½çš„ç‰¹å¾èƒœè¿‡å¤æ‚çš„ç®—æ³•ï¼

## 1. ç‰¹å¾å·¥ç¨‹æ¦‚è¿° ğŸŒŸ

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, RobustScaler,
    LabelEncoder, OneHotEncoder, OrdinalEncoder,
    PolynomialFeatures, KBinsDiscretizer
)
from sklearn.feature_selection import (
    SelectKBest, f_classif, mutual_info_classif,
    RFE, RFECV, SelectFromModel
)
from sklearn.decomposition import PCA, TruncatedSVD, NMF
import warnings
warnings.filterwarnings('ignore')

class FeatureEngineeringPipeline:
    """ç‰¹å¾å·¥ç¨‹å®Œæ•´æµç¨‹"""
    
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
        self.feature_names = []
        
    def analyze_dataset(self, df):
        """æ•°æ®é›†åˆ†æ"""
        print("=" * 50)
        print("æ•°æ®é›†åŸºæœ¬ä¿¡æ¯")
        print("=" * 50)
        print(f"æ ·æœ¬æ•°: {len(df)}")
        print(f"ç‰¹å¾æ•°: {len(df.columns)}")
        print(f"å†…å­˜ä½¿ç”¨: {df.memory_usage().sum() / 1024**2:.2f} MB")
        
        print("\næ•°æ®ç±»å‹åˆ†å¸ƒ:")
        print(df.dtypes.value_counts())
        
        print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
        missing = df.isnull().sum()
        missing_pct = 100 * missing / len(df)
        missing_df = pd.DataFrame({
            'Missing_Count': missing,
            'Missing_Percent': missing_pct
        })
        print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False))
        
        print("\næ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
        print(df.describe())
        
        print("\nç±»åˆ«ç‰¹å¾å”¯ä¸€å€¼:")
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            print(f"{col}: {df[col].nunique()} unique values")
    
    def handle_missing_values(self, df, strategy='smart'):
        """å¤„ç†ç¼ºå¤±å€¼"""
        df_processed = df.copy()
        
        if strategy == 'smart':
            # æ•°å€¼ç‰¹å¾
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if df[col].isnull().sum() > 0:
                    # ç¼ºå¤±æ¯”ä¾‹
                    missing_pct = df[col].isnull().sum() / len(df)
                    
                    if missing_pct > 0.5:
                        # ç¼ºå¤±å¤ªå¤šï¼Œè€ƒè™‘åˆ é™¤
                        print(f"Warning: {col} has {missing_pct:.2%} missing values")
                    elif missing_pct > 0.2:
                        # åˆ›å»ºç¼ºå¤±æŒ‡ç¤ºå™¨
                        df_processed[f'{col}_was_missing'] = df[col].isnull().astype(int)
                        # ç”¨ä¸­ä½æ•°å¡«å……
                        df_processed[col].fillna(df[col].median(), inplace=True)
                    else:
                        # ç”¨å‡å€¼å¡«å……
                        df_processed[col].fillna(df[col].mean(), inplace=True)
            
            # ç±»åˆ«ç‰¹å¾
            categorical_cols = df.select_dtypes(include=['object']).columns
            for col in categorical_cols:
                if df[col].isnull().sum() > 0:
                    # ç”¨ä¼—æ•°æˆ–"Unknown"å¡«å……
                    mode = df[col].mode()
                    if len(mode) > 0:
                        df_processed[col].fillna(mode[0], inplace=True)
                    else:
                        df_processed[col].fillna('Unknown', inplace=True)
        
        elif strategy == 'drop':
            df_processed = df.dropna()
        
        elif strategy == 'forward_fill':
            df_processed = df.fillna(method='ffill')
        
        elif strategy == 'backward_fill':
            df_processed = df.fillna(method='bfill')
        
        return df_processed
    
    def detect_outliers(self, df, method='iqr', threshold=1.5):
        """å¼‚å¸¸å€¼æ£€æµ‹"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        outliers = {}
        
        for col in numeric_cols:
            if method == 'iqr':
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR
                
                outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)
                outliers[col] = df[outlier_mask].index.tolist()
                
            elif method == 'zscore':
                from scipy import stats
                z_scores = np.abs(stats.zscore(df[col].dropna()))
                outlier_mask = z_scores > threshold
                outliers[col] = df[col].dropna()[outlier_mask].index.tolist()
            
            elif method == 'isolation_forest':
                from sklearn.ensemble import IsolationForest
                iso_forest = IsolationForest(contamination=0.1, random_state=42)
                outlier_pred = iso_forest.fit_predict(df[[col]].dropna())
                outliers[col] = df[[col]].dropna()[outlier_pred == -1].index.tolist()
        
        return outliers
    
    def remove_outliers(self, df, outliers, strategy='cap'):
        """å¤„ç†å¼‚å¸¸å€¼"""
        df_processed = df.copy()
        
        for col, outlier_indices in outliers.items():
            if strategy == 'remove':
                df_processed = df_processed.drop(outlier_indices)
            
            elif strategy == 'cap':
                # å°é¡¶å¤„ç†
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                df_processed[col] = df_processed[col].clip(lower_bound, upper_bound)
            
            elif strategy == 'transform':
                # å¯¹æ•°å˜æ¢
                if df[col].min() > 0:
                    df_processed[col] = np.log1p(df[col])
        
        return df_processed
```

## 2. æ•°å€¼ç‰¹å¾å·¥ç¨‹ ğŸ”¢

```python
class NumericalFeatureEngineering:
    """æ•°å€¼ç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        self.scalers = {}
        self.transformers = {}
    
    def scale_features(self, df, method='standard'):
        """ç‰¹å¾ç¼©æ”¾"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df_scaled = df.copy()
        
        if method == 'standard':
            scaler = StandardScaler()
        elif method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'robust':
            scaler = RobustScaler()
        elif method == 'maxabs':
            from sklearn.preprocessing import MaxAbsScaler
            scaler = MaxAbsScaler()
        else:
            return df
        
        df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
        self.scalers[method] = scaler
        
        return df_scaled
    
    def create_polynomial_features(self, df, degree=2, include_bias=False):
        """å¤šé¡¹å¼ç‰¹å¾"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        poly = PolynomialFeatures(degree=degree, include_bias=include_bias)
        poly_features = poly.fit_transform(df[numeric_cols])
        
        # è·å–ç‰¹å¾å
        feature_names = poly.get_feature_names_out(numeric_cols)
        
        # åˆ›å»ºDataFrame
        poly_df = pd.DataFrame(poly_features, columns=feature_names, index=df.index)
        
        # åˆå¹¶åŸå§‹ç‰¹å¾å’Œå¤šé¡¹å¼ç‰¹å¾
        result = pd.concat([df.drop(columns=numeric_cols), poly_df], axis=1)
        
        return result
    
    def create_interaction_features(self, df, features_pairs):
        """äº¤äº’ç‰¹å¾"""
        df_inter = df.copy()
        
        for feat1, feat2 in features_pairs:
            # ä¹˜æ³•äº¤äº’
            df_inter[f'{feat1}_times_{feat2}'] = df[feat1] * df[feat2]
            
            # é™¤æ³•äº¤äº’ï¼ˆé¿å…é™¤é›¶ï¼‰
            df_inter[f'{feat1}_div_{feat2}'] = df[feat1] / (df[feat2] + 1e-8)
            
            # åŠ æ³•äº¤äº’
            df_inter[f'{feat1}_plus_{feat2}'] = df[feat1] + df[feat2]
            
            # å‡æ³•äº¤äº’
            df_inter[f'{feat1}_minus_{feat2}'] = df[feat1] - df[feat2]
        
        return df_inter
    
    def create_statistical_features(self, df, numeric_cols, group_col=None):
        """ç»Ÿè®¡ç‰¹å¾"""
        df_stats = df.copy()
        
        if group_col:
            # åˆ†ç»„ç»Ÿè®¡
            for col in numeric_cols:
                df_stats[f'{col}_mean_by_{group_col}'] = df.groupby(group_col)[col].transform('mean')
                df_stats[f'{col}_std_by_{group_col}'] = df.groupby(group_col)[col].transform('std')
                df_stats[f'{col}_max_by_{group_col}'] = df.groupby(group_col)[col].transform('max')
                df_stats[f'{col}_min_by_{group_col}'] = df.groupby(group_col)[col].transform('min')
                
                # ä¸ç»„å†…å‡å€¼çš„å·®å¼‚
                df_stats[f'{col}_diff_from_mean_{group_col}'] = \
                    df[col] - df_stats[f'{col}_mean_by_{group_col}']
        else:
            # è¡Œç»Ÿè®¡
            df_stats['row_mean'] = df[numeric_cols].mean(axis=1)
            df_stats['row_std'] = df[numeric_cols].std(axis=1)
            df_stats['row_max'] = df[numeric_cols].max(axis=1)
            df_stats['row_min'] = df[numeric_cols].min(axis=1)
            df_stats['row_range'] = df_stats['row_max'] - df_stats['row_min']
            df_stats['row_sum'] = df[numeric_cols].sum(axis=1)
        
        return df_stats
    
    def binning_features(self, df, columns, n_bins=5, strategy='quantile'):
        """ç‰¹å¾åˆ†ç®±"""
        df_binned = df.copy()
        
        for col in columns:
            # åˆ†ç®±
            kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)
            df_binned[f'{col}_binned'] = kbd.fit_transform(df[[col]])
            
            # åˆ›å»ºåˆ†ç®±è¾¹ç•Œç‰¹å¾
            bins = kbd.bin_edges_[0]
            for i in range(len(bins)-1):
                df_binned[f'{col}_bin_{i}'] = ((df[col] >= bins[i]) & (df[col] < bins[i+1])).astype(int)
        
        return df_binned
    
    def transform_skewed_features(self, df, skew_threshold=0.5):
        """ååº¦å˜æ¢"""
        from scipy.stats import skew
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df_transformed = df.copy()
        
        for col in numeric_cols:
            col_skew = skew(df[col].dropna())
            
            if abs(col_skew) > skew_threshold:
                if col_skew > 0:  # å³å
                    # å°è¯•logå˜æ¢
                    if df[col].min() > 0:
                        df_transformed[f'{col}_log'] = np.log1p(df[col])
                    # å°è¯•å¹³æ–¹æ ¹å˜æ¢
                    if df[col].min() >= 0:
                        df_transformed[f'{col}_sqrt'] = np.sqrt(df[col])
                    # Box-Coxå˜æ¢
                    from scipy.stats import boxcox
                    if df[col].min() > 0:
                        df_transformed[f'{col}_boxcox'], _ = boxcox(df[col])
                else:  # å·¦å
                    # å¹³æ–¹å˜æ¢
                    df_transformed[f'{col}_square'] = df[col] ** 2
                    # æŒ‡æ•°å˜æ¢
                    df_transformed[f'{col}_exp'] = np.exp(df[col])
        
        return df_transformed

# æ—¶é—´ç‰¹å¾å·¥ç¨‹
class TimeFeatureEngineering:
    """æ—¶é—´ç‰¹å¾å·¥ç¨‹"""
    
    @staticmethod
    def extract_datetime_features(df, datetime_col):
        """æå–æ—¥æœŸæ—¶é—´ç‰¹å¾"""
        df_time = df.copy()
        
        # ç¡®ä¿æ˜¯datetimeç±»å‹
        df_time[datetime_col] = pd.to_datetime(df[datetime_col])
        
        # åŸºç¡€ç‰¹å¾
        df_time[f'{datetime_col}_year'] = df_time[datetime_col].dt.year
        df_time[f'{datetime_col}_month'] = df_time[datetime_col].dt.month
        df_time[f'{datetime_col}_day'] = df_time[datetime_col].dt.day
        df_time[f'{datetime_col}_hour'] = df_time[datetime_col].dt.hour
        df_time[f'{datetime_col}_minute'] = df_time[datetime_col].dt.minute
        df_time[f'{datetime_col}_second'] = df_time[datetime_col].dt.second
        df_time[f'{datetime_col}_dayofweek'] = df_time[datetime_col].dt.dayofweek
        df_time[f'{datetime_col}_dayofyear'] = df_time[datetime_col].dt.dayofyear
        df_time[f'{datetime_col}_weekofyear'] = df_time[datetime_col].dt.isocalendar().week
        df_time[f'{datetime_col}_quarter'] = df_time[datetime_col].dt.quarter
        
        # æ˜¯å¦æ˜¯å‘¨æœ«
        df_time[f'{datetime_col}_is_weekend'] = (df_time[f'{datetime_col}_dayofweek'] >= 5).astype(int)
        
        # æ˜¯å¦æ˜¯æœˆåˆ/æœˆæœ«
        df_time[f'{datetime_col}_is_month_start'] = df_time[datetime_col].dt.is_month_start.astype(int)
        df_time[f'{datetime_col}_is_month_end'] = df_time[datetime_col].dt.is_month_end.astype(int)
        
        # å¾ªç¯ç‰¹å¾ç¼–ç 
        df_time[f'{datetime_col}_hour_sin'] = np.sin(2 * np.pi * df_time[f'{datetime_col}_hour'] / 24)
        df_time[f'{datetime_col}_hour_cos'] = np.cos(2 * np.pi * df_time[f'{datetime_col}_hour'] / 24)
        df_time[f'{datetime_col}_day_sin'] = np.sin(2 * np.pi * df_time[f'{datetime_col}_day'] / 31)
        df_time[f'{datetime_col}_day_cos'] = np.cos(2 * np.pi * df_time[f'{datetime_col}_day'] / 31)
        df_time[f'{datetime_col}_month_sin'] = np.sin(2 * np.pi * df_time[f'{datetime_col}_month'] / 12)
        df_time[f'{datetime_col}_month_cos'] = np.cos(2 * np.pi * df_time[f'{datetime_col}_month'] / 12)
        
        return df_time
    
    @staticmethod
    def create_lag_features(df, target_col, lag_periods=[1, 7, 30]):
        """åˆ›å»ºæ»åç‰¹å¾"""
        df_lag = df.copy()
        
        for lag in lag_periods:
            df_lag[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
        
        # ç§»åŠ¨å¹³å‡
        for window in [3, 7, 30]:
            df_lag[f'{target_col}_rolling_mean_{window}'] = df[target_col].rolling(window=window).mean()
            df_lag[f'{target_col}_rolling_std_{window}'] = df[target_col].rolling(window=window).std()
            df_lag[f'{target_col}_rolling_max_{window}'] = df[target_col].rolling(window=window).max()
            df_lag[f'{target_col}_rolling_min_{window}'] = df[target_col].rolling(window=window).min()
        
        # æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡
        for alpha in [0.1, 0.3, 0.5]:
            df_lag[f'{target_col}_ewm_{alpha}'] = df[target_col].ewm(alpha=alpha).mean()
        
        return df_lag
```

## 3. ç±»åˆ«ç‰¹å¾å·¥ç¨‹ ğŸ·ï¸

```python
class CategoricalFeatureEngineering:
    """ç±»åˆ«ç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        self.encoders = {}
        self.target_encoders = {}
    
    def label_encoding(self, df, columns):
        """æ ‡ç­¾ç¼–ç """
        df_encoded = df.copy()
        
        for col in columns:
            le = LabelEncoder()
            df_encoded[col] = le.fit_transform(df[col].astype(str))
            self.encoders[col] = le
        
        return df_encoded
    
    def onehot_encoding(self, df, columns, max_categories=10):
        """ç‹¬çƒ­ç¼–ç """
        df_encoded = df.copy()
        
        for col in columns:
            # é™åˆ¶ç±»åˆ«æ•°é‡
            top_categories = df[col].value_counts().head(max_categories).index
            df_encoded[col] = df[col].where(df[col].isin(top_categories), 'Other')
            
            # ç‹¬çƒ­ç¼–ç 
            dummies = pd.get_dummies(df_encoded[col], prefix=col)
            df_encoded = pd.concat([df_encoded, dummies], axis=1)
            df_encoded.drop(col, axis=1, inplace=True)
        
        return df_encoded
    
    def target_encoding(self, df, columns, target, smooth=5):
        """ç›®æ ‡ç¼–ç """
        df_encoded = df.copy()
        
        for col in columns:
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡å‡å€¼
            mean_target = df.groupby(col)[target].mean()
            
            # å¹³æ»‘å¤„ç†
            global_mean = df[target].mean()
            counts = df[col].value_counts()
            smooth_mean = (counts * mean_target + smooth * global_mean) / (counts + smooth)
            
            # æ˜ å°„
            df_encoded[f'{col}_target_enc'] = df[col].map(smooth_mean)
            self.target_encoders[col] = smooth_mean
        
        return df_encoded
    
    def frequency_encoding(self, df, columns):
        """é¢‘ç‡ç¼–ç """
        df_encoded = df.copy()
        
        for col in columns:
            freq = df[col].value_counts(normalize=True)
            df_encoded[f'{col}_freq'] = df[col].map(freq)
        
        return df_encoded
    
    def count_encoding(self, df, columns):
        """è®¡æ•°ç¼–ç """
        df_encoded = df.copy()
        
        for col in columns:
            counts = df[col].value_counts()
            df_encoded[f'{col}_count'] = df[col].map(counts)
        
        return df_encoded
    
    def binary_encoding(self, df, columns):
        """äºŒè¿›åˆ¶ç¼–ç """
        import category_encoders as ce
        
        df_encoded = df.copy()
        
        for col in columns:
            encoder = ce.BinaryEncoder(cols=[col])
            encoded = encoder.fit_transform(df[[col]])
            
            # é‡å‘½ååˆ—
            encoded.columns = [f'{col}_bin_{i}' for i in range(len(encoded.columns))]
            
            # åˆå¹¶
            df_encoded = pd.concat([df_encoded, encoded], axis=1)
            df_encoded.drop(col, axis=1, inplace=True)
        
        return df_encoded
    
    def embedding_encoding(self, df, col, embedding_dim=5):
        """åµŒå…¥ç¼–ç ï¼ˆç”¨äºæ·±åº¦å­¦ä¹ ï¼‰"""
        from sklearn.decomposition import TruncatedSVD
        
        # å…ˆè¿›è¡Œç‹¬çƒ­ç¼–ç 
        dummies = pd.get_dummies(df[col])
        
        # SVDé™ç»´
        svd = TruncatedSVD(n_components=embedding_dim)
        embeddings = svd.fit_transform(dummies)
        
        # åˆ›å»ºDataFrame
        embedding_df = pd.DataFrame(
            embeddings, 
            columns=[f'{col}_emb_{i}' for i in range(embedding_dim)],
            index=df.index
        )
        
        return pd.concat([df, embedding_df], axis=1)

# é«˜çº§ç¼–ç æŠ€æœ¯
class AdvancedEncoding:
    """é«˜çº§ç¼–ç æŠ€æœ¯"""
    
    @staticmethod
    def woe_encoding(df, feature, target):
        """WOEç¼–ç ï¼ˆWeight of Evidenceï¼‰"""
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¥½åæ¯”ä¾‹
        crosstab = pd.crosstab(df[feature], df[target])
        crosstab['Total'] = crosstab.sum(axis=1)
        crosstab['Bad_Rate'] = crosstab[1] / crosstab['Total']
        crosstab['Good_Rate'] = crosstab[0] / crosstab['Total']
        
        # è®¡ç®—WOE
        crosstab['WOE'] = np.log(crosstab['Bad_Rate'] / crosstab['Good_Rate'])
        
        # æ˜ å°„
        woe_dict = crosstab['WOE'].to_dict()
        df[f'{feature}_woe'] = df[feature].map(woe_dict)
        
        return df
    
    @staticmethod
    def james_stein_encoding(df, feature, target):
        """James-Steinç¼–ç """
        # è®¡ç®—å…¨å±€å‡å€¼
        global_mean = df[target].mean()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡å€¼å’Œæ–¹å·®
        group_stats = df.groupby(feature)[target].agg(['mean', 'var', 'count'])
        
        # James-Steinæ”¶ç¼©
        k = len(group_stats)
        grand_var = df[target].var()
        
        shrinkage = 1 - (k - 3) * grand_var / group_stats['var'].sum()
        shrinkage = max(0, shrinkage)
        
        group_stats['js_estimate'] = shrinkage * group_stats['mean'] + (1 - shrinkage) * global_mean
        
        # æ˜ å°„
        js_dict = group_stats['js_estimate'].to_dict()
        df[f'{feature}_js'] = df[feature].map(js_dict)
        
        return df
```

## 4. ç‰¹å¾é€‰æ‹© ğŸ¯

```python
class FeatureSelection:
    """ç‰¹å¾é€‰æ‹©æ–¹æ³•"""
    
    def __init__(self):
        self.selected_features = []
        self.feature_scores = {}
    
    def filter_method(self, X, y, method='mutual_info', k=10):
        """è¿‡æ»¤æ³•"""
        if method == 'mutual_info':
            if y.dtype == 'object' or len(np.unique(y)) < 10:
                selector = SelectKBest(mutual_info_classif, k=k)
            else:
                from sklearn.feature_selection import mutual_info_regression
                selector = SelectKBest(mutual_info_regression, k=k)
        
        elif method == 'chi2':
            from sklearn.feature_selection import chi2
            selector = SelectKBest(chi2, k=k)
        
        elif method == 'f_score':
            if y.dtype == 'object' or len(np.unique(y)) < 10:
                selector = SelectKBest(f_classif, k=k)
            else:
                from sklearn.feature_selection import f_regression
                selector = SelectKBest(f_regression, k=k)
        
        elif method == 'variance':
            from sklearn.feature_selection import VarianceThreshold
            selector = VarianceThreshold(threshold=0.01)
        
        X_selected = selector.fit_transform(X, y)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾
        if hasattr(X, 'columns'):
            self.selected_features = X.columns[selector.get_support()].tolist()
        
        return X_selected
    
    def wrapper_method(self, X, y, estimator, n_features=10):
        """åŒ…è£…æ³•"""
        # RFE
        rfe = RFE(estimator, n_features_to_select=n_features)
        X_selected = rfe.fit_transform(X, y)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾
        if hasattr(X, 'columns'):
            self.selected_features = X.columns[rfe.support_].tolist()
        
        # ç‰¹å¾æ’å
        self.feature_scores = dict(zip(X.columns, rfe.ranking_))
        
        return X_selected
    
    def embedded_method(self, X, y, estimator):
        """åµŒå…¥æ³•"""
        # ä½¿ç”¨L1æ­£åˆ™åŒ–
        from sklearn.linear_model import LassoCV
        
        if isinstance(estimator, LassoCV):
            estimator.fit(X, y)
            selector = SelectFromModel(estimator, prefit=True)
        else:
            selector = SelectFromModel(estimator)
            selector.fit(X, y)
        
        X_selected = selector.transform(X)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾
        if hasattr(X, 'columns'):
            self.selected_features = X.columns[selector.get_support()].tolist()
        
        return X_selected
    
    def permutation_importance(self, X, y, estimator, n_repeats=10):
        """æ’åˆ—é‡è¦æ€§"""
        from sklearn.inspection import permutation_importance
        
        # è®­ç»ƒæ¨¡å‹
        estimator.fit(X, y)
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        perm_importance = permutation_importance(
            estimator, X, y, n_repeats=n_repeats, random_state=42
        )
        
        # åˆ›å»ºé‡è¦æ€§DataFrame
        importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance_mean': perm_importance.importances_mean,
            'importance_std': perm_importance.importances_std
        }).sort_values('importance_mean', ascending=False)
        
        self.feature_scores = importance_df.set_index('feature')['importance_mean'].to_dict()
        
        return importance_df
    
    def boruta_selection(self, X, y, estimator, max_iter=100):
        """Borutaç‰¹å¾é€‰æ‹©"""
        from boruta import BorutaPy
        
        # Borutaç‰¹å¾é€‰æ‹©
        boruta = BorutaPy(estimator, n_estimators='auto', max_iter=max_iter)
        boruta.fit(X.values, y.values)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾
        self.selected_features = X.columns[boruta.support_].tolist()
        
        # è·å–ç‰¹å¾æ’å
        self.feature_scores = dict(zip(X.columns, boruta.ranking_))
        
        return X[self.selected_features]
    
    def shap_selection(self, X, y, estimator, threshold=0.01):
        """åŸºäºSHAPå€¼çš„ç‰¹å¾é€‰æ‹©"""
        import shap
        
        # è®­ç»ƒæ¨¡å‹
        estimator.fit(X, y)
        
        # è®¡ç®—SHAPå€¼
        explainer = shap.Explainer(estimator, X)
        shap_values = explainer(X)
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        feature_importance = np.abs(shap_values.values).mean(axis=0)
        
        # é€‰æ‹©é‡è¦ç‰¹å¾
        important_features = X.columns[feature_importance > threshold]
        self.selected_features = important_features.tolist()
        
        # ä¿å­˜ç‰¹å¾åˆ†æ•°
        self.feature_scores = dict(zip(X.columns, feature_importance))
        
        return X[self.selected_features]
```

## 5. ç‰¹å¾å˜æ¢ ğŸ”„

```python
class FeatureTransformation:
    """ç‰¹å¾å˜æ¢"""
    
    def __init__(self):
        self.transformers = {}
    
    def pca_transform(self, X, n_components=0.95, plot=True):
        """PCAé™ç»´"""
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(X)
        
        self.transformers['pca'] = pca
        
        if plot:
            import matplotlib.pyplot as plt
            
            # è§£é‡Šæ–¹å·®æ¯”
            plt.figure(figsize=(10, 5))
            
            plt.subplot(1, 2, 1)
            plt.bar(range(len(pca.explained_variance_ratio_)), 
                   pca.explained_variance_ratio_)
            plt.xlabel('Component')
            plt.ylabel('Explained Variance Ratio')
            plt.title('PCA Explained Variance Ratio')
            
            plt.subplot(1, 2, 2)
            plt.plot(np.cumsum(pca.explained_variance_ratio_))
            plt.xlabel('Number of Components')
            plt.ylabel('Cumulative Explained Variance')
            plt.title('Cumulative Explained Variance')
            plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')
            plt.legend()
            
            plt.tight_layout()
            plt.show()
        
        return X_pca
    
    def lda_transform(self, X, y, n_components=None):
        """LDAé™ç»´"""
        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
        
        lda = LinearDiscriminantAnalysis(n_components=n_components)
        X_lda = lda.fit_transform(X, y)
        
        self.transformers['lda'] = lda
        
        return X_lda
    
    def kernel_pca_transform(self, X, n_components=2, kernel='rbf'):
        """æ ¸PCA"""
        from sklearn.decomposition import KernelPCA
        
        kpca = KernelPCA(n_components=n_components, kernel=kernel)
        X_kpca = kpca.fit_transform(X)
        
        self.transformers['kpca'] = kpca
        
        return X_kpca
    
    def autoencoder_transform(self, X, encoding_dim=10, epochs=50):
        """è‡ªç¼–ç å™¨é™ç»´"""
        from tensorflow.keras.models import Model
        from tensorflow.keras.layers import Input, Dense
        
        input_dim = X.shape[1]
        
        # ç¼–ç å™¨
        input_layer = Input(shape=(input_dim,))
        encoded = Dense(encoding_dim * 2, activation='relu')(input_layer)
        encoded = Dense(encoding_dim, activation='relu')(encoded)
        
        # è§£ç å™¨
        decoded = Dense(encoding_dim * 2, activation='relu')(encoded)
        decoded = Dense(input_dim, activation='sigmoid')(decoded)
        
        # è‡ªç¼–ç å™¨æ¨¡å‹
        autoencoder = Model(input_layer, decoded)
        autoencoder.compile(optimizer='adam', loss='mse')
        
        # è®­ç»ƒ
        autoencoder.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, verbose=0)
        
        # ç¼–ç å™¨æ¨¡å‹
        encoder = Model(input_layer, encoded)
        
        # å˜æ¢
        X_encoded = encoder.predict(X)
        
        self.transformers['autoencoder'] = encoder
        
        return X_encoded
    
    def nmf_transform(self, X, n_components=10):
        """éè´ŸçŸ©é˜µåˆ†è§£"""
        # ç¡®ä¿éè´Ÿ
        X_positive = X - X.min() + 1e-10
        
        nmf = NMF(n_components=n_components, random_state=42)
        X_nmf = nmf.fit_transform(X_positive)
        
        self.transformers['nmf'] = nmf
        
        return X_nmf
    
    def tsne_transform(self, X, n_components=2):
        """t-SNEé™ç»´ï¼ˆä¸»è¦ç”¨äºå¯è§†åŒ–ï¼‰"""
        from sklearn.manifold import TSNE
        
        tsne = TSNE(n_components=n_components, random_state=42)
        X_tsne = tsne.fit_transform(X)
        
        return X_tsne
    
    def umap_transform(self, X, n_components=2):
        """UMAPé™ç»´"""
        import umap
        
        reducer = umap.UMAP(n_components=n_components, random_state=42)
        X_umap = reducer.fit_transform(X)
        
        self.transformers['umap'] = reducer
        
        return X_umap
```

## 6. ç‰¹å¾ç»„åˆä¸ç”Ÿæˆ ğŸ”¨

```python
class FeatureGeneration:
    """ç‰¹å¾ç”Ÿæˆ"""
    
    @staticmethod
    def generate_ratio_features(df, numerator_cols, denominator_cols):
        """ç”Ÿæˆæ¯”ç‡ç‰¹å¾"""
        df_ratio = df.copy()
        
        for num_col in numerator_cols:
            for den_col in denominator_cols:
                if num_col != den_col:
                    df_ratio[f'{num_col}_div_{den_col}'] = df[num_col] / (df[den_col] + 1e-8)
        
        return df_ratio
    
    @staticmethod
    def generate_aggregation_features(df, group_cols, agg_cols, agg_funcs=['mean', 'std', 'max', 'min']):
        """ç”Ÿæˆèšåˆç‰¹å¾"""
        df_agg = df.copy()
        
        for group_col in group_cols:
            for agg_col in agg_cols:
                for func in agg_funcs:
                    agg_name = f'{agg_col}_{func}_by_{group_col}'
                    df_agg[agg_name] = df.groupby(group_col)[agg_col].transform(func)
        
        return df_agg
    
    @staticmethod
    def generate_diff_features(df, columns, periods=[1]):
        """ç”Ÿæˆå·®åˆ†ç‰¹å¾"""
        df_diff = df.copy()
        
        for col in columns:
            for period in periods:
                df_diff[f'{col}_diff_{period}'] = df[col].diff(period)
                df_diff[f'{col}_pct_change_{period}'] = df[col].pct_change(period)
        
        return df_diff
    
    @staticmethod
    def generate_text_features(df, text_col):
        """ç”Ÿæˆæ–‡æœ¬ç‰¹å¾"""
        df_text = df.copy()
        
        # åŸºç¡€ç»Ÿè®¡
        df_text[f'{text_col}_length'] = df[text_col].str.len()
        df_text[f'{text_col}_word_count'] = df[text_col].str.split().str.len()
        df_text[f'{text_col}_unique_word_count'] = df[text_col].apply(lambda x: len(set(str(x).split())))
        
        # ç‰¹æ®Šå­—ç¬¦
        df_text[f'{text_col}_punctuation_count'] = df[text_col].str.count('[^\w\s]')
        df_text[f'{text_col}_digit_count'] = df[text_col].str.count('\d')
        df_text[f'{text_col}_upper_count'] = df[text_col].str.count('[A-Z]')
        
        # å¹³å‡è¯é•¿
        df_text[f'{text_col}_avg_word_length'] = df[text_col].apply(
            lambda x: np.mean([len(word) for word in str(x).split()]) if str(x).split() else 0
        )
        
        return df_text
    
    @staticmethod
    def generate_domain_features(df, domain='retail'):
        """ç”Ÿæˆé¢†åŸŸç‰¹å®šç‰¹å¾"""
        df_domain = df.copy()
        
        if domain == 'retail':
            # é›¶å”®é¢†åŸŸ
            if 'price' in df.columns and 'quantity' in df.columns:
                df_domain['total_amount'] = df['price'] * df['quantity']
            
            if 'customer_id' in df.columns:
                df_domain['customer_frequency'] = df.groupby('customer_id')['customer_id'].transform('count')
            
            if 'date' in df.columns:
                df_domain['is_weekend'] = pd.to_datetime(df['date']).dt.dayofweek.isin([5, 6]).astype(int)
                df_domain['is_holiday'] = 0  # éœ€è¦å‡æœŸæ—¥å†
        
        elif domain == 'finance':
            # é‡‘èé¢†åŸŸ
            if 'income' in df.columns and 'debt' in df.columns:
                df_domain['debt_to_income'] = df['debt'] / (df['income'] + 1e-8)
            
            if 'credit_limit' in df.columns and 'balance' in df.columns:
                df_domain['utilization_rate'] = df['balance'] / (df['credit_limit'] + 1e-8)
        
        return df_domain
```

## 7. è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹ ğŸ¤–

```python
class AutoFeatureEngineering:
    """è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        self.feature_importance = {}
        self.generated_features = []
    
    def auto_generate_features(self, df, target=None, max_features=100):
        """è‡ªåŠ¨ç”Ÿæˆç‰¹å¾"""
        original_features = df.columns.tolist()
        df_auto = df.copy()
        
        # 1. è¯†åˆ«ç‰¹å¾ç±»å‹
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
        
        # 2. æ•°å€¼ç‰¹å¾
        if len(numeric_cols) > 1:
            # äº¤äº’ç‰¹å¾
            for i, col1 in enumerate(numeric_cols[:10]):  # é™åˆ¶æ•°é‡
                for col2 in numeric_cols[i+1:10]:
                    df_auto[f'{col1}_times_{col2}'] = df[col1] * df[col2]
                    df_auto[f'{col1}_plus_{col2}'] = df[col1] + df[col2]
                    
                    if len(df_auto.columns) > max_features:
                        break
        
        # 3. ç±»åˆ«ç‰¹å¾
        for col in categorical_cols[:5]:  # é™åˆ¶æ•°é‡
            # é¢‘ç‡ç¼–ç 
            freq = df[col].value_counts(normalize=True)
            df_auto[f'{col}_freq'] = df[col].map(freq)
            
            # å¦‚æœæœ‰ç›®æ ‡å˜é‡ï¼Œåšç›®æ ‡ç¼–ç 
            if target is not None:
                mean_target = df.groupby(col)[target].mean()
                df_auto[f'{col}_target_mean'] = df[col].map(mean_target)
        
        # 4. æ—¶é—´ç‰¹å¾
        for col in datetime_cols:
            df_auto[f'{col}_year'] = pd.to_datetime(df[col]).dt.year
            df_auto[f'{col}_month'] = pd.to_datetime(df[col]).dt.month
            df_auto[f'{col}_dayofweek'] = pd.to_datetime(df[col]).dt.dayofweek
        
        # 5. ç»Ÿè®¡ç‰¹å¾
        if len(numeric_cols) > 2:
            df_auto['numeric_mean'] = df[numeric_cols].mean(axis=1)
            df_auto['numeric_std'] = df[numeric_cols].std(axis=1)
            df_auto['numeric_max'] = df[numeric_cols].max(axis=1)
            df_auto['numeric_min'] = df[numeric_cols].min(axis=1)
        
        self.generated_features = [col for col in df_auto.columns if col not in original_features]
        
        return df_auto
    
    def feature_tools_engineering(self, df, target_col=None, max_depth=2):
        """ä½¿ç”¨Featuretoolsè‡ªåŠ¨ç‰¹å¾å·¥ç¨‹"""
        import featuretools as ft
        
        # åˆ›å»ºå®ä½“é›†
        es = ft.EntitySet(id="data")
        
        # æ·»åŠ å®ä½“
        es = es.add_dataframe(
            dataframe_name="main",
            dataframe=df,
            index="index"
        )
        
        # æ·±åº¦ç‰¹å¾åˆæˆ
        feature_matrix, feature_defs = ft.dfs(
            entityset=es,
            target_dataframe_name="main",
            max_depth=max_depth,
            verbose=True
        )
        
        return feature_matrix
    
    def genetic_feature_selection(self, X, y, n_features=20, n_generations=50):
        """é—ä¼ ç®—æ³•ç‰¹å¾é€‰æ‹©"""
        from sklearn.model_selection import cross_val_score
        from sklearn.ensemble import RandomForestClassifier
        import random
        
        n_total_features = X.shape[1]
        population_size = 50
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = []
        for _ in range(population_size):
            chromosome = [random.randint(0, 1) for _ in range(n_total_features)]
            # ç¡®ä¿è‡³å°‘æœ‰n_featuresä¸ªç‰¹å¾
            while sum(chromosome) < n_features:
                chromosome[random.randint(0, n_total_features-1)] = 1
            population.append(chromosome)
        
        # è¿›åŒ–
        for generation in range(n_generations):
            # è¯„ä¼°é€‚åº”åº¦
            fitness_scores = []
            for chromosome in population:
                selected_features = [i for i, gene in enumerate(chromosome) if gene == 1]
                if len(selected_features) > 0:
                    X_selected = X.iloc[:, selected_features]
                    score = cross_val_score(
                        RandomForestClassifier(n_estimators=10, random_state=42),
                        X_selected, y, cv=3
                    ).mean()
                    fitness_scores.append(score)
                else:
                    fitness_scores.append(0)
            
            # é€‰æ‹©
            sorted_population = [x for _, x in sorted(
                zip(fitness_scores, population), 
                key=lambda pair: pair[0], 
                reverse=True
            )]
            
            # äº¤å‰å’Œå˜å¼‚
            new_population = sorted_population[:10]  # ç²¾è‹±ä¿ç•™
            
            while len(new_population) < population_size:
                # é€‰æ‹©çˆ¶æ¯
                parent1 = random.choice(sorted_population[:20])
                parent2 = random.choice(sorted_population[:20])
                
                # äº¤å‰
                crossover_point = random.randint(1, n_total_features-1)
                child = parent1[:crossover_point] + parent2[crossover_point:]
                
                # å˜å¼‚
                if random.random() < 0.1:
                    mutation_point = random.randint(0, n_total_features-1)
                    child[mutation_point] = 1 - child[mutation_point]
                
                new_population.append(child)
            
            population = new_population
        
        # è¿”å›æœ€ä½³ç‰¹å¾é›†
        best_chromosome = sorted_population[0]
        selected_features = [i for i, gene in enumerate(best_chromosome) if gene == 1]
        
        return X.iloc[:, selected_features]
```

## 8. ç‰¹å¾å·¥ç¨‹å®æˆ˜ç¤ºä¾‹ ğŸ¯

```python
def feature_engineering_pipeline_example():
    """å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµç¨‹ç¤ºä¾‹"""
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    df = pd.DataFrame({
        'numeric1': np.random.randn(n_samples),
        'numeric2': np.random.exponential(2, n_samples),
        'numeric3': np.random.uniform(0, 100, n_samples),
        'category1': np.random.choice(['A', 'B', 'C', 'D'], n_samples),
        'category2': np.random.choice(['X', 'Y', 'Z'], n_samples),
        'date': pd.date_range('2023-01-01', periods=n_samples, freq='H'),
        'text': ['text_' + str(i) for i in range(n_samples)],
        'target': np.random.randint(0, 2, n_samples)
    })
    
    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
    df.loc[np.random.choice(df.index, 50), 'numeric2'] = np.nan
    df.loc[np.random.choice(df.index, 30), 'category1'] = np.nan
    
    print("åŸå§‹æ•°æ®é›†å½¢çŠ¶:", df.shape)
    
    # 1. åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹pipeline
    fe_pipeline = FeatureEngineeringPipeline()
    
    # 2. æ•°æ®åˆ†æ
    fe_pipeline.analyze_dataset(df)
    
    # 3. å¤„ç†ç¼ºå¤±å€¼
    df = fe_pipeline.handle_missing_values(df)
    
    # 4. å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
    outliers = fe_pipeline.detect_outliers(df[['numeric1', 'numeric2', 'numeric3']])
    df = fe_pipeline.remove_outliers(df, outliers, strategy='cap')
    
    # 5. æ•°å€¼ç‰¹å¾å·¥ç¨‹
    num_fe = NumericalFeatureEngineering()
    
    # åˆ›å»ºäº¤äº’ç‰¹å¾
    df = num_fe.create_interaction_features(df, [('numeric1', 'numeric2'), ('numeric2', 'numeric3')])
    
    # åˆ›å»ºç»Ÿè®¡ç‰¹å¾
    df = num_fe.create_statistical_features(df, ['numeric1', 'numeric2', 'numeric3'])
    
    # 6. ç±»åˆ«ç‰¹å¾å·¥ç¨‹
    cat_fe = CategoricalFeatureEngineering()
    
    # ç›®æ ‡ç¼–ç 
    df = cat_fe.target_encoding(df, ['category1', 'category2'], 'target')
    
    # é¢‘ç‡ç¼–ç 
    df = cat_fe.frequency_encoding(df, ['category1', 'category2'])
    
    # 7. æ—¶é—´ç‰¹å¾å·¥ç¨‹
    time_fe = TimeFeatureEngineering()
    df = time_fe.extract_datetime_features(df, 'date')
    
    # 8. ç‰¹å¾é€‰æ‹©
    X = df.drop(['target', 'date', 'text', 'category1', 'category2'], axis=1)
    y = df['target']
    
    selector = FeatureSelection()
    
    # ä½¿ç”¨éšæœºæ£®æ—è¿›è¡Œç‰¹å¾é‡è¦æ€§è¯„ä¼°
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    importance_df = selector.permutation_importance(X, y, rf)
    
    print("\nç‰¹å¾é‡è¦æ€§ Top 10:")
    print(importance_df.head(10))
    
    # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
    X_selected = selector.filter_method(X, y, method='mutual_info', k=15)
    
    print(f"\næœ€ç»ˆç‰¹å¾æ•°é‡: {X_selected.shape[1]}")
    
    # 9. ç‰¹å¾ç¼©æ”¾
    X_scaled = num_fe.scale_features(pd.DataFrame(X_selected), method='standard')
    
    # 10. æ¨¡å‹è¯„ä¼°
    from sklearn.model_selection import cross_val_score
    
    scores = cross_val_score(rf, X_scaled, y, cv=5, scoring='roc_auc')
    print(f"\näº¤å‰éªŒè¯AUCåˆ†æ•°: {scores.mean():.4f} (+/- {scores.std():.4f})")
    
    return df

# è¿è¡Œç¤ºä¾‹
result_df = feature_engineering_pipeline_example()
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ“‹

```python
def feature_engineering_best_practices():
    """ç‰¹å¾å·¥ç¨‹æœ€ä½³å®è·µ"""
    
    practices = {
        "æ•°æ®ç†è§£": [
            "æ·±å…¥ç†è§£ä¸šåŠ¡èƒŒæ™¯å’Œæ•°æ®å«ä¹‰",
            "ä¸é¢†åŸŸä¸“å®¶åˆä½œ",
            "è¿›è¡ŒEDAï¼ˆæ¢ç´¢æ€§æ•°æ®åˆ†æï¼‰",
            "è¯†åˆ«æ•°æ®è´¨é‡é—®é¢˜"
        ],
        
        "ç‰¹å¾åˆ›å»º": [
            "åŸºäºä¸šåŠ¡é€»è¾‘åˆ›å»ºç‰¹å¾",
            "è€ƒè™‘ç‰¹å¾ä¹‹é—´çš„äº¤äº’",
            "åˆ›å»ºèšåˆç‰¹å¾å’Œç»Ÿè®¡ç‰¹å¾",
            "åˆ©ç”¨æ—¶é—´åºåˆ—ç‰¹å¾",
            "é€‚å½“ä½¿ç”¨å¤šé¡¹å¼ç‰¹å¾"
        ],
        
        "ç‰¹å¾ç¼–ç ": [
            "ç±»åˆ«ç‰¹å¾ï¼šæ ¹æ®åŸºæ•°é€‰æ‹©ç¼–ç æ–¹å¼",
            "é«˜åŸºæ•°ï¼šç›®æ ‡ç¼–ç ã€åµŒå…¥",
            "ä½åŸºæ•°ï¼šç‹¬çƒ­ç¼–ç ",
            "æœ‰åºç±»åˆ«ï¼šåºæ•°ç¼–ç "
        ],
        
        "ç‰¹å¾é€‰æ‹©": [
            "ç§»é™¤å¸¸æ•°ç‰¹å¾",
            "ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾",
            "ä½¿ç”¨å¤šç§æ–¹æ³•éªŒè¯ç‰¹å¾é‡è¦æ€§",
            "è€ƒè™‘ç‰¹å¾çš„å¯è§£é‡Šæ€§"
        ],
        
        "æ•°æ®æ³„éœ²é¢„é˜²": [
            "ç¡®ä¿ç‰¹å¾ä¸åŒ…å«æœªæ¥ä¿¡æ¯",
            "æ­£ç¡®å¤„ç†æ—¶é—´åºåˆ—æ•°æ®",
            "éªŒè¯é›†å’Œæµ‹è¯•é›†åˆ†å¼€å¤„ç†",
            "ç›®æ ‡ç¼–ç ä½¿ç”¨äº¤å‰éªŒè¯"
        ],
        
        "æ€§èƒ½ä¼˜åŒ–": [
            "å‡å°‘ç‰¹å¾æ•°é‡ä»¥åŠ å¿«è®­ç»ƒ",
            "ä½¿ç”¨ç¨€ç–çŸ©é˜µå­˜å‚¨ç‹¬çƒ­ç¼–ç ",
            "å¹¶è¡ŒåŒ–ç‰¹å¾ç”Ÿæˆè¿‡ç¨‹",
            "ç¼“å­˜è®¡ç®—å¯†é›†çš„ç‰¹å¾"
        ]
    }
    
    return practices

# å¸¸è§é™·é˜±
common_pitfalls = """
1. æ•°æ®æ³„éœ²ï¼šä½¿ç”¨äº†åŒ…å«ç›®æ ‡ä¿¡æ¯çš„ç‰¹å¾
2. è¿‡åº¦å·¥ç¨‹ï¼šåˆ›å»ºå¤ªå¤šæ— ç”¨ç‰¹å¾
3. å¿½è§†ç¼ºå¤±å€¼ï¼šä¸å½“å¤„ç†ç¼ºå¤±å€¼
4. æ ‡å‡†åŒ–æ—¶æœºï¼šåœ¨åˆ†å‰²æ•°æ®å‰è¿›è¡Œæ ‡å‡†åŒ–
5. ç»´åº¦ç¾éš¾ï¼šç‰¹å¾è¿‡å¤šå¯¼è‡´è¿‡æ‹Ÿåˆ
6. å¿½è§†ç‰¹å¾åˆ†å¸ƒï¼šä¸å¤„ç†åæ€åˆ†å¸ƒ
7. ç±»åˆ«ä¸ä¸€è‡´ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†ç±»åˆ«ä¸åŒ¹é…
8. æ—¶é—´ç‰¹å¾å¤„ç†ä¸å½“ï¼šä¸è€ƒè™‘å‘¨æœŸæ€§
"""

print("ç‰¹å¾å·¥ç¨‹æœ€ä½³å®è·µåŠ è½½å®Œæˆï¼")
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [æ¨¡å‹è¯„ä¼°](evaluation.md) - æ­£ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½
- [è¶…å‚æ•°è°ƒä¼˜](hyperparameter_tuning.md) - ä¼˜åŒ–æ¨¡å‹å‚æ•°
- [é›†æˆå­¦ä¹ ](ensemble.md) - ç»„åˆå¤šä¸ªæ¨¡å‹