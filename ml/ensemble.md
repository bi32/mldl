# é›†æˆå­¦ä¹ æ–¹æ³•è¯¦è§£ ğŸŒ²

é›†æˆå­¦ä¹ å°±åƒç»„å»ºä¸€ä¸ªä¸“å®¶å›¢é˜Ÿï¼Œæ¯ä¸ªä¸“å®¶æœ‰è‡ªå·±çš„ä¸“é•¿ï¼Œé€šè¿‡æŠ•ç¥¨æˆ–åŠ æƒçš„æ–¹å¼åšå‡ºæœ€ç»ˆå†³ç­–ã€‚

## 1. XGBoost - ç«èµ›ä¹‹ç‹ ğŸ‘‘

### æ ¸å¿ƒæ€æƒ³
XGBoostæ˜¯"æé™æ¢¯åº¦æå‡"ï¼Œé€šè¿‡ä¸æ–­æ·»åŠ æ–°æ ‘æ¥çº æ­£ä¹‹å‰æ ‘çš„é”™è¯¯ï¼Œå°±åƒä¸æ–­è¯·æ•™æ–°è€å¸ˆæ¥è¡¥å……çŸ¥è¯†ç›²ç‚¹ã€‚

### å®Œæ•´ä»£ç å®ç°

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, accuracy_score, r2_score
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# å®‰è£…å‘½ä»¤ï¼špip install xgboost

# åˆ›å»ºå›å½’æ•°æ®é›†
from sklearn.datasets import make_regression
X_reg, y_reg = make_regression(n_samples=1000, n_features=20, 
                               n_informative=15, noise=10, random_state=42)

# åˆ›å»ºåˆ†ç±»æ•°æ®é›†
from sklearn.datasets import make_classification
X_clf, y_clf = make_classification(n_samples=1000, n_features=20,
                                   n_informative=15, n_redundant=5,
                                   n_classes=3, random_state=42)

# XGBoostå›å½’ç¤ºä¾‹
print("=== XGBoost å›å½’ ===")

X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

# åŸºç¡€æ¨¡å‹
xgb_reg = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    objective='reg:squarederror',
    random_state=42
)

# è®­ç»ƒæ¨¡å‹
xgb_reg.fit(
    X_train_reg, y_train_reg,
    eval_set=[(X_test_reg, y_test_reg)],
    eval_metric='rmse',
    early_stopping_rounds=10,
    verbose=False
)

# é¢„æµ‹
y_pred_reg = xgb_reg.predict(X_test_reg)

# è¯„ä¼°
mse = mean_squared_error(y_test_reg, y_pred_reg)
r2 = r2_score(y_test_reg, y_pred_reg)
print(f"MSE: {mse:.4f}")
print(f"RÂ²: {r2:.4f}")
print(f"æœ€ä½³è¿­ä»£è½®æ•°: {xgb_reg.best_iteration}")

# XGBooståˆ†ç±»ç¤ºä¾‹
print("\n=== XGBoost åˆ†ç±» ===")

X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X_clf, y_clf, test_size=0.2, stratify=y_clf, random_state=42
)

xgb_clf = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    objective='multi:softprob',
    random_state=42
)

xgb_clf.fit(
    X_train_clf, y_train_clf,
    eval_set=[(X_test_clf, y_test_clf)],
    eval_metric='mlogloss',
    early_stopping_rounds=10,
    verbose=False
)

y_pred_clf = xgb_clf.predict(X_test_clf)
accuracy = accuracy_score(y_test_clf, y_pred_clf)
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

# ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# å›å½’ç‰¹å¾é‡è¦æ€§
feature_importance_reg = pd.DataFrame({
    'feature': [f'F{i}' for i in range(X_reg.shape[1])],
    'importance': xgb_reg.feature_importances_
}).sort_values('importance', ascending=False).head(10)

axes[0].barh(range(10), feature_importance_reg['importance'].values)
axes[0].set_yticks(range(10))
axes[0].set_yticklabels(feature_importance_reg['feature'].values)
axes[0].set_xlabel('é‡è¦æ€§')
axes[0].set_title('XGBoostå›å½’ - Top 10ç‰¹å¾')

# åˆ†ç±»ç‰¹å¾é‡è¦æ€§
xgb.plot_importance(xgb_clf, max_num_features=10, ax=axes[1])
axes[1].set_title('XGBooståˆ†ç±» - Top 10ç‰¹å¾')

plt.tight_layout()
plt.show()

# é«˜çº§åŠŸèƒ½ï¼šè‡ªå®šä¹‰è¯„ä¼°å‡½æ•°å’Œç›®æ ‡å‡½æ•°
def custom_eval_metric(y_pred, dtrain):
    """è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡"""
    y_true = dtrain.get_label()
    error = np.mean(np.abs(y_true - y_pred))
    return 'custom_mae', error

# ä½¿ç”¨DMatrixï¼ˆXGBoostçš„æ•°æ®ç»“æ„ï¼‰
dtrain = xgb.DMatrix(X_train_reg, label=y_train_reg)
dtest = xgb.DMatrix(X_test_reg, label=y_test_reg)

# è®¾ç½®å‚æ•°
params = {
    'objective': 'reg:squarederror',
    'max_depth': 5,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# è®­ç»ƒæ¨¡å‹
watchlist = [(dtrain, 'train'), (dtest, 'test')]
model = xgb.train(
    params, dtrain, 
    num_boost_round=100,
    evals=watchlist,
    feval=custom_eval_metric,
    early_stopping_rounds=10,
    verbose_eval=False
)

print(f"\nè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡æœ€ä½³è½®æ•°: {model.best_iteration}")

# å­¦ä¹ æ›²çº¿
results = model.evals_result()
plt.figure(figsize=(10, 6))
plt.plot(results['train']['rmse'], label='è®­ç»ƒRMSE')
plt.plot(results['test']['rmse'], label='æµ‹è¯•RMSE')
plt.xlabel('è¿­ä»£è½®æ•°')
plt.ylabel('RMSE')
plt.title('XGBoostå­¦ä¹ æ›²çº¿')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## 2. LightGBM - é€Ÿåº¦ä¹‹å…‰ âš¡

### æ ¸å¿ƒæ€æƒ³
LightGBMä½¿ç”¨åŸºäºç›´æ–¹å›¾çš„ç®—æ³•å’Œå¶å­ç”Ÿé•¿ç­–ç•¥ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶å¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦ã€‚

```python
import lightgbm as lgb
# å®‰è£…å‘½ä»¤ï¼špip install lightgbm

print("=== LightGBM ç¤ºä¾‹ ===")

# LightGBMå›å½’
lgb_reg = lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    num_leaves=31,
    random_state=42,
    force_col_wise=True  # é¿å…è­¦å‘Š
)

# è®­ç»ƒ
lgb_reg.fit(
    X_train_reg, y_train_reg,
    eval_set=[(X_test_reg, y_test_reg)],
    eval_metric='rmse',
    callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
)

# é¢„æµ‹å’Œè¯„ä¼°
y_pred_lgb = lgb_reg.predict(X_test_reg)
mse_lgb = mean_squared_error(y_test_reg, y_pred_lgb)
r2_lgb = r2_score(y_test_reg, y_pred_lgb)

print(f"LightGBM MSE: {mse_lgb:.4f}")
print(f"LightGBM RÂ²: {r2_lgb:.4f}")

# LightGBMåˆ†ç±»
lgb_clf = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    num_leaves=31,
    random_state=42,
    force_col_wise=True
)

lgb_clf.fit(
    X_train_clf, y_train_clf,
    eval_set=[(X_test_clf, y_test_clf)],
    eval_metric='multi_logloss',
    callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
)

y_pred_lgb_clf = lgb_clf.predict(X_test_clf)
accuracy_lgb = accuracy_score(y_test_clf, y_pred_lgb_clf)
print(f"LightGBM å‡†ç¡®ç‡: {accuracy_lgb:.4f}")

# LightGBMçš„ç‹¬ç‰¹åŠŸèƒ½ï¼šç±»åˆ«ç‰¹å¾å¤„ç†
# åˆ›å»ºåŒ…å«ç±»åˆ«ç‰¹å¾çš„æ•°æ®
np.random.seed(42)
n_samples = 1000

# æ•°å€¼ç‰¹å¾
num_features = np.random.randn(n_samples, 3)

# ç±»åˆ«ç‰¹å¾
cat_features = np.column_stack([
    np.random.choice(['A', 'B', 'C'], n_samples),
    np.random.choice(['X', 'Y', 'Z'], n_samples),
    np.random.choice(range(10), n_samples)  # æ•°å€¼å‹ç±»åˆ«
])

# åˆå¹¶ç‰¹å¾
X_mixed = np.column_stack([num_features, cat_features])

# åˆ›å»ºDataFrame
df_mixed = pd.DataFrame(X_mixed, columns=['num1', 'num2', 'num3', 
                                          'cat1', 'cat2', 'cat3'])

# è½¬æ¢ç±»åˆ«ç‰¹å¾ä¸ºcategoryç±»å‹
for col in ['cat1', 'cat2', 'cat3']:
    df_mixed[col] = df_mixed[col].astype('category')

# ç”Ÿæˆç›®æ ‡å˜é‡
y_mixed = (df_mixed['num1'] > 0).astype(int) & \
          (df_mixed['cat1'] == 'A').astype(int)

# åˆ’åˆ†æ•°æ®
X_train_mixed, X_test_mixed, y_train_mixed, y_test_mixed = train_test_split(
    df_mixed, y_mixed, test_size=0.2, random_state=42
)

# è®­ç»ƒLightGBMï¼ˆè‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾ï¼‰
lgb_cat = lgb.LGBMClassifier(
    n_estimators=100,
    random_state=42,
    force_col_wise=True
)

lgb_cat.fit(
    X_train_mixed, y_train_mixed,
    eval_set=[(X_test_mixed, y_test_mixed)],
    categorical_feature=['cat1', 'cat2', 'cat3'],
    callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
)

print(f"\nåŒ…å«ç±»åˆ«ç‰¹å¾çš„LightGBMå‡†ç¡®ç‡: "
      f"{lgb_cat.score(X_test_mixed, y_test_mixed):.4f}")

# é€Ÿåº¦å¯¹æ¯”
import time

# å¤§æ•°æ®é›†
X_large, y_large = make_regression(n_samples=10000, n_features=100, 
                                   n_informative=50, random_state=42)
X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(
    X_large, y_large, test_size=0.2, random_state=42
)

# XGBoostè®¡æ—¶
start = time.time()
xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0).fit(
    X_train_l, y_train_l
)
xgb_time = time.time() - start

# LightGBMè®¡æ—¶
start = time.time()
lgb.LGBMRegressor(n_estimators=100, random_state=42, verbosity=-1,
                  force_col_wise=True).fit(X_train_l, y_train_l)
lgb_time = time.time() - start

print(f"\nè®­ç»ƒæ—¶é—´å¯¹æ¯”ï¼ˆ10000æ ·æœ¬ï¼Œ100ç‰¹å¾ï¼‰:")
print(f"XGBoost: {xgb_time:.2f}ç§’")
print(f"LightGBM: {lgb_time:.2f}ç§’")
print(f"LightGBMåŠ é€Ÿæ¯”: {xgb_time/lgb_time:.1f}x")
```

## 3. CatBoost - ç±»åˆ«ç‰¹å¾å¤§å¸ˆ ğŸ±

### æ ¸å¿ƒæ€æƒ³
CatBoostä¸“é—¨ä¼˜åŒ–äº†ç±»åˆ«ç‰¹å¾çš„å¤„ç†ï¼Œå¹¶ä½¿ç”¨å¯¹ç§°æ ‘ç»“æ„å‡å°‘è¿‡æ‹Ÿåˆã€‚

```python
import catboost as cb
# å®‰è£…å‘½ä»¤ï¼špip install catboost

print("\n=== CatBoost ç¤ºä¾‹ ===")

# CatBoostå›å½’
cat_reg = cb.CatBoostRegressor(
    iterations=100,
    depth=5,
    learning_rate=0.1,
    random_seed=42,
    verbose=False
)

# è®­ç»ƒ
cat_reg.fit(
    X_train_reg, y_train_reg,
    eval_set=(X_test_reg, y_test_reg),
    early_stopping_rounds=10,
    verbose=False
)

# é¢„æµ‹
y_pred_cat = cat_reg.predict(X_test_reg)
mse_cat = mean_squared_error(y_test_reg, y_pred_cat)
r2_cat = r2_score(y_test_reg, y_pred_cat)

print(f"CatBoost MSE: {mse_cat:.4f}")
print(f"CatBoost RÂ²: {r2_cat:.4f}")

# CatBoostå¤„ç†ç±»åˆ«ç‰¹å¾çš„ä¼˜åŠ¿
# ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„æ··åˆæ•°æ®
print("\n=== CatBoostç±»åˆ«ç‰¹å¾å¤„ç† ===")

# éœ€è¦æŒ‡å®šç±»åˆ«ç‰¹å¾çš„ç´¢å¼•
cat_features_indices = [3, 4, 5]  # cat1, cat2, cat3çš„ç´¢å¼•

# å‡†å¤‡æ•°æ®
X_train_cat = X_train_mixed.values
X_test_cat = X_test_mixed.values

# åˆ›å»ºPoolï¼ˆCatBoostçš„æ•°æ®ç»“æ„ï¼‰
train_pool = cb.Pool(
    X_train_cat, y_train_mixed,
    cat_features=cat_features_indices
)
test_pool = cb.Pool(
    X_test_cat, y_test_mixed,
    cat_features=cat_features_indices
)

# è®­ç»ƒCatBoost
cat_clf = cb.CatBoostClassifier(
    iterations=100,
    depth=5,
    learning_rate=0.1,
    random_seed=42,
    verbose=False
)

cat_clf.fit(train_pool, eval_set=test_pool, 
           early_stopping_rounds=10, verbose=False)

# é¢„æµ‹
y_pred_catboost = cat_clf.predict(test_pool)
accuracy_cat = accuracy_score(y_test_mixed, y_pred_catboost)
print(f"CatBoostå‡†ç¡®ç‡ï¼ˆç±»åˆ«ç‰¹å¾ï¼‰: {accuracy_cat:.4f}")

# SHAPå€¼è§£é‡Šï¼ˆCatBoostå†…ç½®ï¼‰
shap_values = cat_clf.get_feature_importance(
    test_pool, 
    type='ShapValues'
)

# å¯è§†åŒ–SHAPå€¼
plt.figure(figsize=(10, 6))
shap_mean = np.abs(shap_values[:, :-1]).mean(axis=0)
feature_names = [f'num{i+1}' for i in range(3)] + \
                [f'cat{i+1}' for i in range(3)]
indices = np.argsort(shap_mean)[::-1]

plt.bar(range(len(indices)), shap_mean[indices])
plt.xticks(range(len(indices)), 
          [feature_names[i] for i in indices])
plt.xlabel('ç‰¹å¾')
plt.ylabel('å¹³å‡|SHAP|å€¼')
plt.title('CatBoostç‰¹å¾é‡è¦æ€§ï¼ˆSHAPï¼‰')
plt.show()
```

## 4. ä¸‰å¤§æ¡†æ¶å¯¹æ¯”å®æˆ˜

```python
# ç»¼åˆå¯¹æ¯”é¡¹ç›®ï¼šé¢„æµ‹æˆ¿ä»·
def create_house_data(n_samples=2000):
    """åˆ›å»ºæˆ¿ä»·æ•°æ®ï¼ˆåŒ…å«æ•°å€¼å’Œç±»åˆ«ç‰¹å¾ï¼‰"""
    np.random.seed(42)
    
    # æ•°å€¼ç‰¹å¾
    area = np.random.uniform(30, 300, n_samples)
    rooms = np.random.randint(1, 6, n_samples)
    age = np.random.uniform(0, 50, n_samples)
    floor = np.random.randint(1, 30, n_samples)
    
    # ç±»åˆ«ç‰¹å¾
    district = np.random.choice(['æœé˜³', 'æµ·æ·€', 'ä¸œåŸ', 'è¥¿åŸ', 'ä¸°å°'], 
                               n_samples)
    orientation = np.random.choice(['ä¸œ', 'å—', 'è¥¿', 'åŒ—', 'ä¸œå—', 'è¥¿å—'], 
                                  n_samples)
    decoration = np.random.choice(['æ¯›å¯', 'ç®€è£…', 'ç²¾è£…', 'è±ªè£…'], 
                                 n_samples)
    
    # ä»·æ ¼è®¡ç®—ï¼ˆæœ‰é€»è¾‘å…³ç³»ï¼‰
    price = (
        area * 15000 +
        rooms * 30000 +
        age * (-3000) +
        floor * 1000 +
        (district == 'æµ·æ·€').astype(int) * 100000 +
        (district == 'æœé˜³').astype(int) * 80000 +
        (orientation == 'å—').astype(int) * 20000 +
        (decoration == 'è±ªè£…').astype(int) * 50000 +
        np.random.randn(n_samples) * 30000
    )
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'é¢ç§¯': area,
        'æˆ¿é—´æ•°': rooms,
        'æˆ¿é¾„': age,
        'æ¥¼å±‚': floor,
        'åŒºåŸŸ': district,
        'æœå‘': orientation,
        'è£…ä¿®': decoration,
        'ä»·æ ¼': price
    })
    
    return df

# åˆ›å»ºæ•°æ®
house_df = create_house_data()
print("æˆ¿ä»·æ•°æ®é›†:")
print(house_df.head())
print(f"\næ•°æ®å½¢çŠ¶: {house_df.shape}")

# å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
X = house_df.drop('ä»·æ ¼', axis=1)
y = house_df['ä»·æ ¼']

# å¤„ç†ç±»åˆ«ç‰¹å¾
from sklearn.preprocessing import LabelEncoder

# ä¸ºXGBoostå’Œé€šç”¨æ¨¡å‹ç¼–ç 
X_encoded = X.copy()
label_encoders = {}
cat_columns = ['åŒºåŸŸ', 'æœå‘', 'è£…ä¿®']

for col in cat_columns:
    le = LabelEncoder()
    X_encoded[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42
)

# ä¸ºCatBoostå‡†å¤‡ï¼ˆä¿ç•™åŸå§‹ç±»åˆ«ï¼‰
X_train_cat = X.loc[X_train.index]
X_test_cat = X.loc[X_test.index]

# ä¸‰å¤§æ¡†æ¶å¯¹æ¯”
models = {}
results = {}

print("\n=== è®­ç»ƒä¸‰å¤§æ¢¯åº¦æå‡æ¡†æ¶ ===")

# 1. XGBoost
print("è®­ç»ƒXGBoost...")
models['XGBoost'] = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
models['XGBoost'].fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=20,
    verbose=False
)

# 2. LightGBM
print("è®­ç»ƒLightGBM...")
models['LightGBM'] = lgb.LGBMRegressor(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    force_col_wise=True
)
models['LightGBM'].fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]
)

# 3. CatBoost
print("è®­ç»ƒCatBoost...")
cat_features_idx = [X_train_cat.columns.get_loc(col) 
                   for col in cat_columns]

models['CatBoost'] = cb.CatBoostRegressor(
    iterations=200,
    depth=6,
    learning_rate=0.1,
    cat_features=cat_features_idx,
    random_seed=42,
    verbose=False
)
models['CatBoost'].fit(
    X_train_cat, y_train,
    eval_set=(X_test_cat, y_test),
    early_stopping_rounds=20,
    verbose=False
)

# è¯„ä¼°æ‰€æœ‰æ¨¡å‹
print("\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===")
for name, model in models.items():
    if name == 'CatBoost':
        y_pred = model.predict(X_test_cat)
    else:
        y_pred = model.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    mae = np.mean(np.abs(y_test - y_pred))
    
    results[name] = {
        'RMSE': rmse,
        'MAE': mae,
        'RÂ²': r2,
        'MAPE': np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    }

# åˆ›å»ºå¯¹æ¯”è¡¨
results_df = pd.DataFrame(results).T
print(results_df.round(2))

# å¯è§†åŒ–å¯¹æ¯”
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 1. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
metrics = ['RMSE', 'MAE', 'RÂ²']
for idx, metric in enumerate(metrics):
    ax = axes[0, idx]
    values = [results[model][metric] for model in models.keys()]
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    bars = ax.bar(models.keys(), values, color=colors)
    ax.set_title(f'{metric}å¯¹æ¯”')
    ax.set_ylabel(metric)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{val:.2f}', ha='center', va='bottom')

# 2. é¢„æµ‹vsçœŸå®ï¼ˆå‰100ä¸ªæ ·æœ¬ï¼‰
for idx, (name, model) in enumerate(models.items()):
    ax = axes[1, idx]
    if name == 'CatBoost':
        y_pred_sample = model.predict(X_test_cat.iloc[:100])
    else:
        y_pred_sample = model.predict(X_test.iloc[:100])
    
    y_test_sample = y_test.iloc[:100].values
    
    ax.scatter(y_test_sample, y_pred_sample, alpha=0.5, s=10)
    ax.plot([y_test_sample.min(), y_test_sample.max()],
            [y_test_sample.min(), y_test_sample.max()],
            'r--', lw=2)
    ax.set_xlabel('çœŸå®ä»·æ ¼')
    ax.set_ylabel('é¢„æµ‹ä»·æ ¼')
    ax.set_title(f'{name}')

plt.tight_layout()
plt.show()

# ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
print("\n=== ç‰¹å¾é‡è¦æ€§å¯¹æ¯” ===")

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, (name, model) in enumerate(models.items()):
    ax = axes[idx]
    
    if name == 'XGBoost':
        importance = model.feature_importances_
        feature_names = X_train.columns
    elif name == 'LightGBM':
        importance = model.feature_importances_
        feature_names = X_train.columns
    else:  # CatBoost
        importance = model.feature_importances_
        feature_names = X_train_cat.columns
    
    # æ’åºå¹¶å–å‰10
    indices = np.argsort(importance)[::-1][:7]
    
    ax.barh(range(7), importance[indices])
    ax.set_yticks(range(7))
    ax.set_yticklabels([feature_names[i] for i in indices])
    ax.set_xlabel('é‡è¦æ€§')
    ax.set_title(f'{name}ç‰¹å¾é‡è¦æ€§')

plt.tight_layout()
plt.show()
```

## 5. è¶…å‚æ•°è°ƒä¼˜æœ€ä½³å®è·µ

```python
from sklearn.model_selection import RandomizedSearchCV
import scipy.stats as stats

# XGBoostè¶…å‚æ•°ç©ºé—´
xgb_param_dist = {
    'n_estimators': stats.randint(100, 500),
    'max_depth': stats.randint(3, 10),
    'learning_rate': stats.uniform(0.01, 0.29),
    'subsample': stats.uniform(0.6, 0.4),
    'colsample_bytree': stats.uniform(0.6, 0.4),
    'min_child_weight': stats.randint(1, 10),
    'gamma': stats.uniform(0, 0.5),
    'reg_alpha': stats.uniform(0, 1),
    'reg_lambda': stats.uniform(0, 2)
}

# LightGBMè¶…å‚æ•°ç©ºé—´
lgb_param_dist = {
    'n_estimators': stats.randint(100, 500),
    'max_depth': stats.randint(3, 10),
    'num_leaves': stats.randint(20, 100),
    'learning_rate': stats.uniform(0.01, 0.29),
    'feature_fraction': stats.uniform(0.6, 0.4),
    'bagging_fraction': stats.uniform(0.6, 0.4),
    'bagging_freq': stats.randint(1, 10),
    'min_child_samples': stats.randint(5, 30),
    'reg_alpha': stats.uniform(0, 1),
    'reg_lambda': stats.uniform(0, 2)
}

# CatBoostè¶…å‚æ•°ç©ºé—´
cat_param_dist = {
    'iterations': stats.randint(100, 500),
    'depth': stats.randint(4, 10),
    'learning_rate': stats.uniform(0.01, 0.29),
    'l2_leaf_reg': stats.uniform(1, 10),
    'bagging_temperature': stats.uniform(0, 1),
    'random_strength': stats.uniform(0, 1),
    'border_count': stats.randint(32, 255)
}

print("=== è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹ï¼ˆä½¿ç”¨å°æ•°æ®é›†æ¼”ç¤ºï¼‰===")

# ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œæ¼”ç¤º
X_small, y_small = make_regression(n_samples=500, n_features=10, 
                                   n_informative=8, random_state=42)
X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(
    X_small, y_small, test_size=0.2, random_state=42
)

# XGBoostè°ƒä¼˜
print("\nè°ƒä¼˜XGBoost...")
xgb_random = RandomizedSearchCV(
    xgb.XGBRegressor(random_state=42),
    xgb_param_dist,
    n_iter=20,  # å®é™…ä½¿ç”¨æ—¶å¯ä»¥å¢åŠ 
    cv=3,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    random_state=42
)

xgb_random.fit(X_train_s, y_train_s)
print(f"æœ€ä½³å‚æ•°: {xgb_random.best_params_}")
print(f"æœ€ä½³CVåˆ†æ•°: {-xgb_random.best_score_:.4f}")

# è´å¶æ–¯ä¼˜åŒ–ï¼ˆæ›´é«˜æ•ˆçš„è°ƒå‚æ–¹æ³•ï¼‰
# pip install scikit-optimize
try:
    from skopt import BayesSearchCV
    from skopt.space import Real, Integer
    
    # å®šä¹‰è´å¶æ–¯ä¼˜åŒ–ç©ºé—´
    bayes_space = {
        'n_estimators': Integer(100, 500),
        'max_depth': Integer(3, 10),
        'learning_rate': Real(0.01, 0.3, 'log-uniform'),
        'subsample': Real(0.6, 1.0),
        'colsample_bytree': Real(0.6, 1.0)
    }
    
    bayes_search = BayesSearchCV(
        xgb.XGBRegressor(random_state=42),
        bayes_space,
        n_iter=20,
        cv=3,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42
    )
    
    bayes_search.fit(X_train_s, y_train_s)
    print(f"\nè´å¶æ–¯ä¼˜åŒ–æœ€ä½³å‚æ•°: {bayes_search.best_params_}")
    print(f"è´å¶æ–¯ä¼˜åŒ–æœ€ä½³åˆ†æ•°: {-bayes_search.best_score_:.4f}")
    
except ImportError:
    print("\nè´å¶æ–¯ä¼˜åŒ–éœ€è¦å®‰è£…: pip install scikit-optimize")
```

## 6. å®æˆ˜æŠ€å·§æ€»ç»“

### é€‰æ‹©æŒ‡å—
```python
def choose_gbm_framework(data_characteristics):
    """æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©æœ€åˆé€‚çš„æ¢¯åº¦æå‡æ¡†æ¶"""
    
    recommendations = []
    
    if data_characteristics.get('has_categorical', False):
        recommendations.append("CatBoost - è‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾")
    
    if data_characteristics.get('large_dataset', False):
        recommendations.append("LightGBM - è®­ç»ƒé€Ÿåº¦æœ€å¿«")
    
    if data_characteristics.get('need_interpretability', False):
        recommendations.append("XGBoost - ç‰¹å¾é‡è¦æ€§åˆ†ææˆç†Ÿ")
    
    if data_characteristics.get('competition', False):
        recommendations.append("XGBoost - ç«èµ›ç»éªŒä¸°å¯Œï¼Œè°ƒå‚èµ„æ–™å¤š")
    
    if data_characteristics.get('gpu_available', False):
        recommendations.append("XGBoost/LightGBM - GPUæ”¯æŒå¥½")
    
    return recommendations

# ç¤ºä¾‹
data_chars = {
    'has_categorical': True,
    'large_dataset': True,
    'need_interpretability': False,
    'competition': False,
    'gpu_available': False
}

print("æ¨èçš„æ¡†æ¶:")
for rec in choose_gbm_framework(data_chars):
    print(f"  - {rec}")
```

### é˜²æ­¢è¿‡æ‹ŸåˆæŠ€å·§
```python
# 1. æ—©åœ
# æ‰€æœ‰æ¡†æ¶éƒ½æ”¯æŒearly_stopping_rounds

# 2. æ­£åˆ™åŒ–å‚æ•°
overfitting_params = {
    'XGBoost': {
        'max_depth': 3,  # é™åˆ¶æ ‘æ·±åº¦
        'min_child_weight': 5,  # å¢åŠ æœ€å°å¶å­æƒé‡
        'gamma': 0.1,  # å¢åŠ åˆ†è£‚æ‰€éœ€æœ€å°æŸå¤±å‡å°‘
        'reg_alpha': 0.1,  # L1æ­£åˆ™åŒ–
        'reg_lambda': 1.0,  # L2æ­£åˆ™åŒ–
        'subsample': 0.8,  # æ ·æœ¬é‡‡æ ·
        'colsample_bytree': 0.8  # ç‰¹å¾é‡‡æ ·
    },
    'LightGBM': {
        'max_depth': 3,
        'num_leaves': 20,  # é™åˆ¶å¶å­æ•°
        'min_child_samples': 20,  # å¢åŠ æœ€å°å¶å­æ ·æœ¬æ•°
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0
    },
    'CatBoost': {
        'depth': 4,
        'l2_leaf_reg': 3,  # L2æ­£åˆ™åŒ–
        'bagging_temperature': 0.1,  # è´å¶æ–¯bootstrap
        'random_strength': 1  # éšæœºæ€§å¼ºåº¦
    }
}
```

### åŠ é€Ÿè®­ç»ƒ
```python
# 1. ä½¿ç”¨æ›´å°‘çš„ç‰¹å¾
# 2. å‡å°‘æ•°æ®ç²¾åº¦ï¼ˆfloat32 vs float64ï¼‰
# 3. ä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰

# GPUç¤ºä¾‹ï¼ˆéœ€è¦GPUç‰ˆæœ¬çš„åº“ï¼‰
gpu_params = {
    'XGBoost': {'tree_method': 'gpu_hist', 'gpu_id': 0},
    'LightGBM': {'device': 'gpu', 'gpu_platform_id': 0},
    'CatBoost': {'task_type': 'GPU', 'devices': '0'}
}
```

## æœ€ä½³å®è·µå»ºè®®

1. **å…ˆç”¨é»˜è®¤å‚æ•°**ï¼šä¸‰å¤§æ¡†æ¶çš„é»˜è®¤å‚æ•°éƒ½å¾ˆå¥½
2. **å…³æ³¨è¿‡æ‹Ÿåˆ**ï¼šä½¿ç”¨æ—©åœå’Œäº¤å‰éªŒè¯
3. **ç‰¹å¾å·¥ç¨‹ä¼˜å…ˆ**ï¼šå¥½ç‰¹å¾æ¯”è°ƒå‚é‡è¦
4. **é›†æˆå¤šä¸ªæ¨¡å‹**ï¼šç»„åˆä¸åŒæ¡†æ¶çš„é¢„æµ‹

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- [è¶…å‚æ•°è°ƒä¼˜](hyperparameter_tuning.md) - ç³»ç»ŸåŒ–è°ƒå‚æ–¹æ³•
- [ç‰¹å¾å·¥ç¨‹](feature_engineering.md) - æå‡æ¨¡å‹è¾“å…¥è´¨é‡
- [æ¨¡å‹è§£é‡Š](model_interpretation.md) - ç†è§£æ¨¡å‹å†³ç­–